{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import History \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPLOAD DATA\n",
    "# (each user should put datafiles in this directory on their computer)\n",
    "datapath = \"blast_tab_1hit.out\"\n",
    "file = open(datapath, 'r')\n",
    "\n",
    "same_entries = []\n",
    "diff_entries = []\n",
    "max_length_in = 0\n",
    "max_length_out = 0\n",
    "\n",
    "for ln in file:\n",
    "    toks = ln.split('\\t')\n",
    "    max_length_in = max(max_length_in,len(toks[2]))\n",
    "    max_length_out = max(max_length_out,len(toks[3]))\n",
    "    if toks[2] == toks[3]:\n",
    "        same_entries.append([toks[2], toks[3]])\n",
    "    else:\n",
    "        diff_entries.append([toks[2], toks[3]])\n",
    "\n",
    "file.close()\n",
    "num_entries = len(same_entries) + len(diff_entries)\n",
    "\n",
    "\n",
    "\n",
    "diff_entries_input = [entry[0] for entry in diff_entries]\n",
    "diff_entries_output = [(\"\\t\" + entry[1] + \"\\n\") for entry in diff_entries] #use '\\t' as start character and '\\n' as end character\n",
    "#Visualize\n",
    "diff_entries_output[1]\n",
    "one_hot_input = {'A': 0, 'T': 1, 'C': 2, 'G': 3, '-': 4, '\\t': 5, '\\n': 6}\n",
    "one_hot_output = {'A': 0, 'T': 1, 'C': 2, 'G': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58863\n"
     ]
    }
   ],
   "source": [
    "print(len(diff_entries_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Choose dataset size\n",
    "numExamples = 50000\n",
    "input_seqs = diff_entries_input[0:numExamples]\n",
    "output_seqs = diff_entries_output[0:numExamples]\n",
    "\n",
    "#ENCODE\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_seqs), max_length_in, len(one_hot_input)),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_seqs), max_length_out, len(one_hot_input)),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_seqs), max_length_out, len(one_hot_input)),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_seqs, output_seqs) in enumerate(zip(input_seqs, output_seqs)):\n",
    "    for t, char in enumerate(input_seqs):\n",
    "        encoder_input_data[i, t, one_hot_input[char]] = 1.\n",
    "    for t, char in enumerate(output_seqs):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, one_hot_input[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, one_hot_input[char]] = 1\n",
    "            \n",
    "print(encoder_input_data[1,1:100, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE MODEL\n",
    "\n",
    "# define number of tokens in the lexicon\n",
    "num_encoder_tokens = len(one_hot_input)\n",
    "num_decoder_tokens = len(one_hot_input)\n",
    "latent_dim = 256\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 1873s 47ms/step - loss: 1.0054 - acc: 0.2667 - val_loss: 0.9578 - val_acc: 0.2746\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 2116s 53ms/step - loss: 0.9351 - acc: 0.2947 - val_loss: 0.9067 - val_acc: 0.3163\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 2216s 55ms/step - loss: 0.8561 - acc: 0.3662 - val_loss: 0.7747 - val_acc: 0.4301\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 2298s 57ms/step - loss: 0.7714 - acc: 0.4578 - val_loss: 0.7462 - val_acc: 0.4776\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 2355s 59ms/step - loss: 0.6301 - acc: 0.5525 - val_loss: 0.5328 - val_acc: 0.6168\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 2394s 60ms/step - loss: 0.8019 - acc: 0.4156 - val_loss: 0.9256 - val_acc: 0.3004\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 2434s 61ms/step - loss: 0.9048 - acc: 0.3160 - val_loss: 0.8788 - val_acc: 0.3322\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 2457s 61ms/step - loss: 0.8450 - acc: 0.3694 - val_loss: 0.8018 - val_acc: 0.4141\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 2479s 62ms/step - loss: 0.7621 - acc: 0.4647 - val_loss: 0.8342 - val_acc: 0.4265\n",
      "Epoch 10/100\n",
      "30500/40000 [=====================>........] - ETA: 9:22 - loss: 0.7205 - acc: 0.4980"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "epochs = 100\n",
    "\n",
    "learning_rates = [.001, .005, .01, .05]\n",
    "for lr in learning_rates:\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    # Run training\n",
    "    history = History()\n",
    "\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2, verbose = 1,\n",
    "              callbacks = [history])\n",
    "    model.save('s2s_lr' + str(lr) + '.h5')\n",
    "    with open('history_lr' + str(lr), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.load(open( \"history_lr0.001\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in learning_rates:\n",
    "    history = pickle.load(open()'history_lr' + str(lr) + \".001\", \"rb\")\n",
    "    plt.plot(range(0, epochs), history[\"loss\"])\n",
    "    plt.plot(range(0, epochs), history[\"val_loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
