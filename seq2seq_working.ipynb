{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import History \n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Concatenate\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPLOAD DATA\n",
    "# (each user should put datafiles in this directory on their computer)\n",
    "datapath = \"blast_tab_1hit.out\"\n",
    "file = open(datapath, 'r')\n",
    "\n",
    "same_entries = []\n",
    "diff_entries = []\n",
    "max_length_in = 0\n",
    "max_length_out = 0\n",
    "\n",
    "for ln in file:\n",
    "    toks = ln.split('\\t')\n",
    "    max_length_in = max(max_length_in,len(toks[2]))\n",
    "    max_length_out = max(max_length_out,len(toks[3]))\n",
    "    if toks[2] == toks[3]:\n",
    "        same_entries.append([toks[2], toks[3]])\n",
    "    else:\n",
    "        diff_entries.append([toks[2], toks[3]])\n",
    "\n",
    "file.close()\n",
    "num_entries = len(same_entries) + len(diff_entries)\n",
    "\n",
    "\n",
    "\n",
    "diff_entries_input = [entry[0] for entry in diff_entries]\n",
    "diff_entries_output = [entry[1] for entry in diff_entries]\n",
    "same_entries_input = [entry[0] for entry in same_entries]\n",
    "same_entries_output = [entry[1] for entry in same_entries]\n",
    "#diff_entries_output = [(\"\\t\" + entry[1] + \"\\n\") for entry in diff_entries] #use '\\t' as start character and '\\n' as end character\n",
    "#Visualize\n",
    "diff_entries_output[1]\n",
    "one_hot_input = {'A': 0, 'T': 1, 'C': 2, 'G': 3, '-': 4, '\\t': 5, '\\n': 6}\n",
    "one_hot_output = {'A': 0, 'T': 1, 'C': 2, 'G': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58863\n"
     ]
    }
   ],
   "source": [
    "print(len(diff_entries_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108863\n",
      "108863\n"
     ]
    }
   ],
   "source": [
    "numSameEntries = 50000\n",
    "input_seqs = diff_entries_input + same_entries_input[0:numSameEntries]\n",
    "output_seqs = diff_entries_output + same_entries_output[0:numSameEntries]\n",
    "labels = [[1]] * len(diff_entries_input) + [[-1]] * numSameEntries\n",
    "print(len(labels))\n",
    "print(len(input_seqs))\n",
    "\n",
    "c = list(zip(input_seqs, output_seqs, labels))\n",
    "random.shuffle(c)\n",
    "input_seqs, output_seqs, labels = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108863, 351, 7)\n",
      "(108863, 351, 7)\n"
     ]
    }
   ],
   "source": [
    "#one hot encode\n",
    "\n",
    "#ENCODE\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_seqs), max_length_in, len(one_hot_input)),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_seqs), max_length_out, len(one_hot_input)),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_seqs), max_length_out, len(one_hot_input)),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_seqs, output_seqs) in enumerate(zip(input_seqs, output_seqs)):\n",
    "    for t, char in enumerate(input_seqs):\n",
    "        encoder_input_data[i, t, one_hot_input[char]] = 1.\n",
    "    for t, char in enumerate(output_seqs):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, one_hot_input[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, one_hot_input[char]] = 1\n",
    "            \n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_46/Tanh:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(one_hot_input)\n",
    "num_decoder_tokens = len(one_hot_input)\n",
    "latent_dim = 100\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "e_lstm_1 = Bidirectional(LSTM(latent_dim, return_sequences = True))(encoder_inputs)\n",
    "e_dropout = Dropout(0.5)(e_lstm_1)\n",
    "e_lstm_2 = Bidirectional(LSTM(latent_dim, return_sequences = True))(e_dropout)\n",
    "#e_lstm_3 = Bidirectional(LSTM(latent_dim, return_sequences = True))\n",
    "\n",
    "output = TimeDistributed(Dense(num_decoder_tokens, activation = \"softmax\"))(e_lstm_2)\n",
    "noise_class = Dense(1, activation = \"tanh\")(Concatenate()([h2, c2]))\n",
    "#noise_class = Activation(\"tanh\")(noise_class)\n",
    "print(noise_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108863, 351, 7)\n",
      "(108863, 351, 7)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1],\n",
       "       [-1],\n",
       "       [-1],\n",
       "       ...,\n",
       "       [-1],\n",
       "       [ 1],\n",
       "       [ 1]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, None, 7)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_36 (Bidirectio (None, None, 200)         86400     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               [(None, None, 100), (None 120400    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 206,901\n",
      "Trainable params: 206,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[0.05149814]\n",
      " [0.05149815]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1],\n",
       "       [-1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "print(model.predict(encoder_input_data[0:2, :, :]))\n",
    "np.array(labels)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 28s 35ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00000, saving model to seqWeights/LSTM-test-01-0.00.hdf5\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 19s 23ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 18s 23ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 20s 25ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 18s 23ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00000\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 18s 23ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.00000\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 18s 23ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.00000\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.00000\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 20s 25ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.00000\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 20s 25ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.00000\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.00000\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 19s 23ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.00000\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.00000\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.00000\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.00000\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.00000\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.00000\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.00000\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.00000\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 18s 23ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.00000\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.00000\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 20s 25ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.00000\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.00000\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.00000\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.00000\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.00000\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.00000\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.00000\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.2089 - acc: 0.0000e+00 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.00000\n",
      "Epoch 30/50\n",
      "500/800 [=================>............] - ETA: 11s - loss: 1.7408 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-804b258c135a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m          callbacks = [history, checkpoint])\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train by classification of noisy/non-noisy\n",
    "\n",
    "model = Model(encoder_inputs, noise_class)\n",
    "\n",
    "#model.load_weights(\"seqWeights/LSTM-comboseqs-dropout0.5-manytomany-01-0.82.hdf5\")\n",
    "adam = keras.optimizers.Adam(lr = .001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer= adam, loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = History()\n",
    "filepath=\"seqWeights/LSTM-test-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "numExamples = 1000\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "#output_seqs = decoder_input_data[0:numExamples, :, :]\n",
    "#y = output_seqs.reshape(numExamples, output_seqs.shape[1], 1)\n",
    "model.fit(encoder_input_data[0:numExamples, :, :],\n",
    "          np.array(labels)[0:numExamples],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2, verbose = 1,\n",
    "         callbacks = [history, checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56000 samples, validate on 14000 samples\n",
      "Epoch 1/50\n",
      "56000/56000 [==============================] - 84515s 2s/step - loss: 0.0186 - acc: 0.8059 - val_loss: 0.0173 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86578, saving model to seqWeights/LSTM-comboseqs-dropout0.5-manytomany-01-0.87.hdf5\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-aac11f75a867>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m          callbacks = [history, checkpoint])\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train by sequence correctness\n",
    "model = Model(encoder_inputs, output)\n",
    "\n",
    "model.load_weights(\"seqWeights/LSTM-comboseqs-dropout0.5-manytomany-01-0.82.hdf5\")\n",
    "adam = keras.optimizers.Adam(lr = .01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = History()\n",
    "filepath=\"seqWeights/LSTM-comboseqs-dropout0.5-manytomany-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "numExamples = 70000\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "#output_seqs = decoder_input_data[0:numExamples, :, :]\n",
    "#y = output_seqs.reshape(numExamples, output_seqs.shape[1], 1)\n",
    "model.fit(encoder_input_data[0:numExamples, :, :],\n",
    "          decoder_input_data[0:numExamples, :, :],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2, verbose = 1,\n",
    "         callbacks = [history, checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"seqWeights/LSTM-comboseqs-dropout0.5-manytomany-01-0.87.hdf5\")\n",
    "test_seqs = encoder_input_data[0:10, :, :]\n",
    "\n",
    "probs = np.array(model.predict(test_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argmax(probs, axis = -1)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ACAGGGGTGGCAAGCGTTGTCCGGATTTACTGGGTGTAAAGGGTGCGCAGGCGGACTTATAAGTCGGGGGTTAAATCCATGTGCTTAACACATGCATGGCTTCCGATACTGTAAGTCTAGAGTCTCGGAGAGGAAGATGGAATTTCCGGTGTAACGGTGGAATGTGTAGATATCGGAAAGAACACCAGTGGCGAAGGCAGTCTTCTGGCCGAGTACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAGCCAAGGGCACTCCCACACACACGCCAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACACCACAGACAGCAACCAG\n",
      "Input: ACGGAGGGTGCGAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGCTTGGTAAGTCAGGGGTGAAAGCCCGCGGCTCAACCGCGGAATTGCCTTTGATACTGC-CGAGCTAGAGTCCGGGAGAGGGTAGTGGAATTCCAGGTGTAGGAGTGAAATCCGTAGAGATCTGGAGGAACATCAGTGGCGAAGGCGACTACCTGGACCGGTACTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "Output: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "\n",
      "predicted: ACGGAGGGGGTTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGCGCGTAGGCGGACCAGAAAGTTGGGGGTGAAATCCCGGGGCTCAACCCCGGAACTGCCTCCAAAACTCCTGGTCTTGAGTTCGAGAGAGGTGAGTGGAATTCCGAGTGTAGAGGTGAAATTCGTAGATATTCGGAGGAACACCAGTGGCGAAGGCGGCTCACTGGCTCGATACTGACGCTGAGGTGCGAAAGTGTGGGGAGCAAACAGCCAACGGCACTCCCACACACACGCCAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACACCACAGACAGCAACCAG\n",
      "Input: ACAGAGGGGGCAAGCGTTGTCCGGAGTTACTGGGCGTAAAGGGCGCGCAGGCGGTGGGCTGCGTCGGCGCTGAAAGCGCCCCGCTTAACGGGGCGAGGCGCGCCGATACGAGTCCACTCGAGGCAAGCAGAGGGTGGCGGAATTCCGGGTGGAGTGGTGAAATGCGTAGAGATCCGGAGGAACGCCGGTGGGGAAGCCGGCCACCTGGGCTTGACCTGACGCTGCGGCGCGACAGCGTGGGGAGCAAACCG\n",
      "Output: ACAGAGGGGGCAAGCGTTGTCCGGAGTTACTGGGCGTAAAGGGCGCGCAGGCGGTGGGCTGCGTCGGCGCTGAAAGCGCCCCGCTTAACGGGGCGAGGCGCGCCGATACGAGTCCACTCGAGGCAAGCAGAGGGTGGCGGAATTCCGGGTGGAGCGGTGAAATGCGTAGAGATCCGGAGGAACGCCGGTGGGGAAGCCGGCCACCTGGGCTTGACCTGACGCTGCGGCGCGACAGCGTGGGGAGCAAACCG\n",
      "\n",
      "\n",
      "predicted: ACGTAGGGGGCGAGCGTTGTCCGGAATTACTGGGCGTAAAGGGCGCGTAGGCGGCCGTTCAAGTCAGGTGTAAAAAACCCGGGCTCAACCCGGGGGATGCACATGAAACTGGGTGGCTAGAGGGCAGGAGAGGGGAGTGGAATTCCCGGTGTAGCGGTGAAATGCGTAGATATCGGGAGGAATACCAGTGGCGAAGGCGACTCTCTGGACTGACCCTGACGCTGAGGCGCGAAAGCGTGGGGAGCAAACAGCCAAGGGCACTCCCACACACACGCCAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACACCACAGACAGCAACCAG\n",
      "Input: ACGTAGGTCCCGAACGTTGCGCGAATTTACTGGGCGTAAAGGGTCCGTAGGCGGTTTAGCAAGTGGTTGGTGAAATTTCACGGCTCAACCGTGAAACTGCCTTCCAAACTGCTAAACTTGAGGCAGGGAGAGGTCGGCGGAATTCCCGGTGTAGCGGTGAAATGCGTAGATATCGGGAGGAACACCAGTGGCGAAGGCGGCCGACTGGAACTGTTCTGACGCTGAGGGACGAAAGCTAGGGGAGCAAACCG\n",
      "Output: ACGTAGGTCCCGAACGTTGCGCGAATTTACTGGGCGTAAAGGGTCCGTAGGCGGTTTAGCAAGTGGTTGGTGAAATTTCACGGCTCAACCGTGAAACTGCCTTCCAAACTGCTAAACTTGAGGCAGGGAGAGGTCGGCGGAATTCCCGGTGTAGCGGTGAAATGCGTAGATATCGGGAGGAACACCAGTGGCGAAGGCGGCCGACTGGAACTGTCCTGACGCTGAGGGACGAAAGCTAGGGGAGCAAACCG\n",
      "\n",
      "\n",
      "predicted: ACGTAGGGCGCAAGCGTTGTCCGGATTTATTGGGCGTAAAGAGCTCGTAGGCGGCTTGTCGCGTCGACTGTGAAAACCCGTGGCTCAACTGCGGGCTTGCAGTCGATACGGACAGGCTAGAGTTCGGTAGGGGAGACTGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGGTGGCGAAGGCGGGTCTCTGGGCCGATACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGCCAACGGCACTCCCACACACACGACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACACCACAGACAGCAACCAG\n",
      "Input: ACCGACGGCCCGAGTGGTGGCCACTTTTATTGGGCCTAAAGCGTCCGTAGCCGGTCCAGTAAGTCCTTGTTTAAATCCTGCGGCTTAACCGCAGGACTGGCAGGGATACTGCTGGACTTGGGACCGGGAGAGGACAAGGGTACTTCAGGGGTAGCGGTGAAATGTGTTGATCCTTGAAGGACCACCTATGGCGAAGGCACTTGTCTGGAACGGGCCCGACGGTGAGGGACGAAAGCCAGGGGCGCGAACCG\n",
      "Output: ACCGACGGCCCGAGTGGTGGCCACTTTTATTGGGCCTAAAGCGTCCGTAGCCGGTCCAGTAAGTCCTTGTTTAAATCCTGCGGCTTAACCGCAGGACTGGCAGGGATACTGCTGGACTTGGGACCGGGAGAGGACAAGGGTACTTCAGGGGTAGCGGTGAAATGTGTTGATCCTTGAAGGACCACCTATGGCGAAGGCACTTGTCTGGAACGGGTCCGACGGTGAGGGACGAAAGCCAGGGGCGCGAACCG\n",
      "\n",
      "\n",
      "predicted: ACAGGGGTGGCAAGCGTTGTCCGGATTTACTGGGTGTAAAGGGTGCGCAGGCGGACTTATAAGTCGGGGGTTAAATCCATGTGCTTAACACATGCATGGCTTCCGATACTGTAAGTCTAGAGTCTCGGAGAGGAAGATGGAATTTCCGGTGTAACGGTGGAATGTGTAGATATCGGAAAGAACACCAGTGGCGAAGGCAGTCTTCTGGCCGAGTACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAGCCAAGGGCACTCCCACACACACGCCAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACACCACAGACAGCAACCAG\n",
      "Input: ACGTATGTCACAAGCGTTATCCGGATTTATTGGGCGTAAAGCGCGTCTAGGTGGTTATGCAAGTCTGATGTGAAAATGCAGGGCTCAACTCTGTATTGCGTTGGAAACTGTGTAACTAGAGTACTGGAGAGGTAAGCGGAACTACAAGTGTAGAGGTGAAATTCGTAGATATTTGTAGGAATGCCGATGGGGAAGCCAGCTTACTAGACAGATACTGACGCTGAAGCGCGAAAGCGTGGGTAGCAAACAG\n",
      "Output: ACGTATGTCACAAGCGTTATCCGGATTTATTGGGCGTAAAGCGCGTCTAGGTGGTTATGTAAGTCTGATGTGAAAATGCAGGGCTCAACTCTGTATTGCGTTGGAAACTGTGTAACTAGAGTACTGGAGAGGTAAGCGGAACTACAAGTGTAGAGGTGAAATTCGTAGATATTTGTAGGAATGCCGATGGGGAAGCCAGCTTACTAGACAGATACTGACGCTGAAGCGCGAAAGCGTGGGTAGCAAACAG\n",
      "\n",
      "\n",
      "predicted: ACGGAGGGGGTTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGCGCGTAGGCGGACCAGAAAGTTGGGGGTGAAATCCCGGGGCTCAACCCCGGAACTGCCTCCAAAACTCCTGGTCTTGAGTTCGAGAGAGGTGAGTGGAATTCCGAGTGTAGAGGTGAAATTCGTAGATATTCGGAGGAACACCAGTGGCGAAGGCGGCTCACTGGCTCGATACTGACGCTGAGGTGCGAAAGTGTGGGGAGCAAACAGCCAACGGCACTCCCACACACACGCCAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACACCACAGACAGCAACCAG\n",
      "Input: ACGGAGGGTCCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTTGTTAAGCGAGATGTGAAAGCCCCGGGCTCAACCTGGGAATTGCATTTCGAACTGGCGAACTAGAGTCTCGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAAAGACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG\n",
      "Output: ACGGAGGGTCCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTTGTTAAGCGAGATGTGAAAGCCCCGGGCTCAACCTGGGAATTGCATTTCGAACTGGCGAACTAGAGTCTTGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAAAGACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "\n",
      "predicted: ACGTAGGTGGCGAGCGTTGTCCGGAATTACTGGGTGTAAAGGGCGCGTAGGCGGGGATGCAAGTCAGATGTGAAATTCTGGGGCTTAACCCCGGCGCTGCATCTGAAACTGTATCTCTTGAGTGCTGGAGAGGAAAGCGGAATTCCTAGTGTAGCGGTGAAATGCGTAGATATTAGGAGGAACACCAGTGGCGAAGGCGGCTTTCTGGACAGTAACTGACGCTGAGGCGCGAAAGCGTGGGGAGCAAACAGCCAAGGGCACTCCCACACACACGCCAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACACCACAGACAGCAACCAG\n",
      "Input: ACGGAGGGGGTTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGCGCGTAGGCGGACCAGAAAGTTGGGGGTGAAATCCCGGGGCTCAACCCCGGAACTGCATCCAAAACTCCTGGTCTTGAGTTCGAGAGAGGTGAGTGGAATTCCGAGTGTAGAGGTGAAATTCGTAGATATTCGGAGGAACACCAGTGGCGAAGGCGGCTCACTGGCTCGATACTGACGCTGAGGTGCGAAAGTGTGGGGAGCAAACAG\n",
      "Output: ACGGAGGGGGTTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGCGCGTAGGCGGACCAGAAAGTTGGGGGTGAAATCCCGGGGCTCAACCCCGGAACTGCCTCCAAAACTCCTGGTCTTGAGTTCGAGAGAGGTGAGTGGAATTCCGAGTGTAGAGGTGAAATTCGTAGATATTCGGAGGAACACCAGTGGCGAAGGCGGCTCACTGGCTCGATACTGACGCTGAGGTGCGAAAGTGTGGGGAGCAAACAG\n",
      "\n",
      "\n",
      "predicted: ACAGGGGTGGCAAGCGTTGTCCGGATTTACTGGGTGTAAAGGGTGCGCAGGCGGATCGATAAGTCGGGGGTTAAATCCATGTGCTTAACACATGCACGGCTTCCGATACTGTTGATCTAGAGTCTCGAAGAGGAAGGTGGAATTTCCGGTGTAACGGTGGAATGTGTAGATATCGGAAAGAACACCAGTGGCGAAGGCAGCCTTCTGGTCGAGTACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAGCCAAGGGCACTCCCACACACACGCCAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACACCACAGACAGCAACCAG\n",
      "Input: ACCGACGGCCCGAGTGGTGGCCACTTTTATTGGGCCTAAAGCGTCCGTAGCCGGTCCAGTAAGTCCTTGTTTAAATCCTGCGGCTTAACCGCAGGACTGGCAGGGATACTGCTGGACTTGGGACCGGGAGAGGACAAGGGTACTTCAGGGGTAGCGGTGAAATGTGTTGATCCTTGAAGGACCACCTATGGCGAAGGCACTTGTCTGGAACGGGTCCGACGGTGAGGGACGAAGGCCAGGGGAGCAAACCG\n",
      "Output: ACCGACGGCCCGAGTGGTGGCCACTTTTATTGGGCCTAAAGCGTCCGTAGCCGGTCCAGTAAGTCCTTGTTTAAATCCTGCGGCTTAACCGCAGGACTGGCAGGGATACTGCTGGACTTGGGACCGGGAGAGGACAAGGGTACTTCAGGGGTAGCGGTGAAATGTGTTGATCCTTGAAGGACCACCTATGGCGAAGGCACTTGTCTGGAACGGGTCCGACGGTGAGGGACGAAAGCCAGGGGCGCGAACCG\n",
      "\n",
      "\n",
      "predicted: ACGTAGGTGGCGAGCGTTGTCCGGAATTACTGGGTGTAAAGGGCGCGTAGGCGGGGATGCAAGTCAGATGTGAAATTCCGGGGCTTAACCCCGGCGCTGCATCTGAAACTGTATCTCTTGAGTGCTGGAGAGGAAAGCGGAATTCCTAGTGTAGCGGTGAAATGCGTAGATATTAGGAGGAACACCAGTGGCGAAGGCGGCTTTCTGGACAGTAACTGACGCTGAGGCGCGAAAGCGTGGGGAGCAAACAGCCAAGGGCACTCCCACACACACGCCAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACACCACAGACAGCAACCAG\n",
      "Input: ACGGAGGGTCCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTTGTTAAGCGAGATGTGAAAGCCCCGGGCTCAACCTGGGAATTGCATTTCGAACTGGCGAACTAGAGTCTTGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGG-CCCCTGGACAAAGACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG\n",
      "Output: ACGGAGGGTCCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTTGTTAAGCGAGATGTGAAAGCCCCGGGCTCAACCTGGGAATTGCATTTCGAACTGGCGAACTAGAGTCTTGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAAAGACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "\n",
      "predicted: ACGGAGGGCGCGAGCGTTACCCGGATTCACTGGGCGTAAAGGGCGTGTAAGCGGCCTGGGGCGTCCCATGTGAAAGACCACGGCTCAACCGTGGGGGAGCGTGGGATACGCTCAGGCTAGACGGTGGGAGAGGGTGGTGGAATTCCCGGAGTAGCGGTGAAATGCGCAGATACCGGGAGGAACGCCGATGGCGAAGGCAGCGTAGCCCCCACCAACGACCACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACAACGCCGGCGGCGGCGGCGGCGCCGGCGGCAGGACCCGG\n",
      "Input: ACGGAGGATCCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGGTGGACAGTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGCTGTCTTGAGTACAGTAGAGGTGGGCGGAATTCGTGGTGTAGCGGTGAAATGCTTAGATATCACGAAGAACTCCGATTGCGAAGGCAGCCTGCTAAGCTGCAACTGACATTGAGGCTCGAAAGTGTGGGTATCAAACAG\n",
      "Output: ACGGAGGATCCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGGTGGACAGTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGCTGTCTTGAGTACAGTAGAGGTGGGCGGAATTCGTGGTGTAGCGGTGAAATGCTTAGATATCACGAAGAACTCCGATTGCGAAGGCAGCTCACTGGACTGCAACTGACACTGATGCTCGAAAGTGTGGGTATCAAACAG\n",
      "\n",
      "\n",
      "Baseline error (average noisy nucleotide per sequence): 3.6\n",
      "Output error (average noisy nucleotide per sequence): 87.7\n"
     ]
    }
   ],
   "source": [
    "#translate and cal errors\n",
    "indices = np.argmax(probs, axis = -1)\n",
    "output_dict = np.array(['A', 'T', 'C', 'G', '-', '\\t', '\\n'])\n",
    "\n",
    "error_baseline_sum = 0\n",
    "error_output_sum = 0\n",
    "#input_seqs = diff_entries_input + same_entries_input[1:50000]\n",
    "#output_seqs = diff_entries_output + same_entries_output[1:50000]\n",
    "input_seqs = encoder_input_data\n",
    "for i in range(indices.shape[0]):\n",
    "    pred = ''.join(output_dict[indices[i,]])\n",
    "    inp = input_seqs[i]\n",
    "    out = output_seqs[i]\n",
    "    print(\"predicted: \" + pred)\n",
    "    print(\"Input: \" + inp)\n",
    "    print(\"Output: \" + out)\n",
    "    print('\\n')\n",
    "    \n",
    "    error_baseline = np.sum([1  for j in range(len(inp)) if inp[j] != out[j] ])\n",
    "    error_output = np.sum([1  for j in range(len(inp)) if pred[j] != out[j] ])\n",
    "    error_baseline_sum += error_baseline\n",
    "    error_output_sum += error_output\n",
    "                          \n",
    "    #print(\"Pred - Input: \" +  str(np.sum([1  for i in range(len(inp)) if pred[i] != inp[i] ] )))\n",
    "    #print(\"Pred - Output (ie We care): \" +  str(error_output ))\n",
    "    #print(\"Input - Output (ie Baseline): \" +  str(error_baseline ))\n",
    "    \n",
    "print(\"Baseline error (average noisy nucleotide per sequence): \" + str(error_baseline_sum / indices.shape[0]))\n",
    "print(\"Output error (average noisy nucleotide per sequence): \" + str(error_output_sum / indices.shape[0]))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/100\n",
      "5600/5600 [==============================] - 742s 133ms/step - loss: 0.0276 - acc: 0.8640 - val_loss: 0.0230 - val_acc: 0.8619\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.87103\n",
      "Epoch 2/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0254 - acc: 0.8169 - val_loss: 0.0216 - val_acc: 0.7565\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.87103\n",
      "Epoch 3/100\n",
      "5600/5600 [==============================] - 718s 128ms/step - loss: 0.0242 - acc: 0.7669 - val_loss: 0.0209 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.87103\n",
      "Epoch 4/100\n",
      "5600/5600 [==============================] - 714s 127ms/step - loss: 0.0232 - acc: 0.7606 - val_loss: 0.0200 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.87103\n",
      "Epoch 5/100\n",
      "5600/5600 [==============================] - 718s 128ms/step - loss: 0.0223 - acc: 0.7445 - val_loss: 0.0198 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.87103\n",
      "Epoch 6/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0216 - acc: 0.7182 - val_loss: 0.0188 - val_acc: 0.7213\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.87103\n",
      "Epoch 7/100\n",
      "5600/5600 [==============================] - 715s 128ms/step - loss: 0.0233 - acc: 0.7213 - val_loss: 0.0218 - val_acc: 0.7198\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.87103\n",
      "Epoch 8/100\n",
      "5600/5600 [==============================] - 720s 129ms/step - loss: 0.0375 - acc: 0.8040 - val_loss: 0.0421 - val_acc: 0.8448\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.87103\n",
      "Epoch 9/100\n",
      "5600/5600 [==============================] - 712s 127ms/step - loss: 0.0396 - acc: 0.8396 - val_loss: 0.0297 - val_acc: 0.8340\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87103\n",
      "Epoch 10/100\n",
      "5600/5600 [==============================] - 717s 128ms/step - loss: 0.0326 - acc: 0.8266 - val_loss: 0.0266 - val_acc: 0.8258\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.87103\n",
      "Epoch 11/100\n",
      "5600/5600 [==============================] - 719s 128ms/step - loss: 0.0299 - acc: 0.8185 - val_loss: 0.0253 - val_acc: 0.8062\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87103\n",
      "Epoch 12/100\n",
      "5600/5600 [==============================] - 717s 128ms/step - loss: 0.0282 - acc: 0.8118 - val_loss: 0.0240 - val_acc: 0.8207\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.87103\n",
      "Epoch 13/100\n",
      "5600/5600 [==============================] - 721s 129ms/step - loss: 0.0270 - acc: 0.8058 - val_loss: 0.0228 - val_acc: 0.7959\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87103\n",
      "Epoch 14/100\n",
      "5600/5600 [==============================] - 716s 128ms/step - loss: 0.0257 - acc: 0.7943 - val_loss: 0.0218 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.87103\n",
      "Epoch 15/100\n",
      "5600/5600 [==============================] - 718s 128ms/step - loss: 0.0250 - acc: 0.7797 - val_loss: 0.0225 - val_acc: 0.7838\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87103\n",
      "Epoch 16/100\n",
      "5600/5600 [==============================] - 714s 128ms/step - loss: 0.0243 - acc: 0.7921 - val_loss: 0.0208 - val_acc: 0.7911\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87103\n",
      "Epoch 17/100\n",
      "5600/5600 [==============================] - 720s 129ms/step - loss: 0.0233 - acc: 0.7812 - val_loss: 0.0203 - val_acc: 0.7539\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87103\n",
      "Epoch 18/100\n",
      "5600/5600 [==============================] - 726s 130ms/step - loss: 0.0225 - acc: 0.7756 - val_loss: 0.0195 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87103\n",
      "Epoch 19/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0217 - acc: 0.7756 - val_loss: 0.0193 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.87103\n",
      "Epoch 20/100\n",
      "5600/5600 [==============================] - 717s 128ms/step - loss: 0.0210 - acc: 0.7738 - val_loss: 0.0187 - val_acc: 0.7747\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87103\n",
      "Epoch 21/100\n",
      "5600/5600 [==============================] - 716s 128ms/step - loss: 0.0207 - acc: 0.7701 - val_loss: 0.0183 - val_acc: 0.7665\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87103\n",
      "Epoch 22/100\n",
      "5600/5600 [==============================] - 721s 129ms/step - loss: 0.0203 - acc: 0.7650 - val_loss: 0.0180 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87103\n",
      "Epoch 23/100\n",
      "5600/5600 [==============================] - 714s 128ms/step - loss: 0.0193 - acc: 0.7671 - val_loss: 0.0176 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87103\n",
      "Epoch 24/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0207 - acc: 0.7576 - val_loss: 0.0194 - val_acc: 0.7557\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87103\n",
      "Epoch 25/100\n",
      "5600/5600 [==============================] - 716s 128ms/step - loss: 0.0198 - acc: 0.7575 - val_loss: 0.0176 - val_acc: 0.7507\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87103\n",
      "Epoch 26/100\n",
      "5600/5600 [==============================] - 722s 129ms/step - loss: 0.0185 - acc: 0.7539 - val_loss: 0.0173 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87103\n",
      "Epoch 27/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0184 - acc: 0.7565 - val_loss: 0.0173 - val_acc: 0.7582\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87103\n",
      "Epoch 28/100\n",
      "5600/5600 [==============================] - 717s 128ms/step - loss: 0.0179 - acc: 0.7527 - val_loss: 0.0168 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87103\n",
      "Epoch 29/100\n",
      "5600/5600 [==============================] - 715s 128ms/step - loss: 0.0173 - acc: 0.7543 - val_loss: 0.0169 - val_acc: 0.7494\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87103\n",
      "Epoch 30/100\n",
      "5600/5600 [==============================] - 712s 127ms/step - loss: 0.0171 - acc: 0.7556 - val_loss: 0.0165 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87103\n",
      "Epoch 31/100\n",
      "5600/5600 [==============================] - 718s 128ms/step - loss: 0.0185 - acc: 0.7500 - val_loss: 0.0163 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87103\n",
      "Epoch 32/100\n",
      "5600/5600 [==============================] - 723s 129ms/step - loss: 0.0166 - acc: 0.7528 - val_loss: 0.0159 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87103\n",
      "Epoch 33/100\n",
      "5600/5600 [==============================] - 719s 128ms/step - loss: 0.0166 - acc: 0.7572 - val_loss: 0.0159 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87103\n",
      "Epoch 34/100\n",
      "5600/5600 [==============================] - 714s 127ms/step - loss: 0.0156 - acc: 0.7619 - val_loss: 0.0161 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87103\n",
      "Epoch 35/100\n",
      "5600/5600 [==============================] - 717s 128ms/step - loss: 0.0191 - acc: 0.7633 - val_loss: 0.0293 - val_acc: 0.7570\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87103\n",
      "Epoch 36/100\n",
      "5600/5600 [==============================] - 718s 128ms/step - loss: 0.0270 - acc: 0.7621 - val_loss: 0.0218 - val_acc: 0.7537\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87103\n",
      "Epoch 37/100\n",
      "5600/5600 [==============================] - 715s 128ms/step - loss: 0.0230 - acc: 0.7604 - val_loss: 0.0211 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87103\n",
      "Epoch 38/100\n",
      "5600/5600 [==============================] - 721s 129ms/step - loss: 0.0234 - acc: 0.7646 - val_loss: 0.0215 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87103\n",
      "Epoch 39/100\n",
      "5600/5600 [==============================] - 720s 129ms/step - loss: 0.0223 - acc: 0.7550 - val_loss: 0.0192 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87103\n",
      "Epoch 40/100\n",
      "5600/5600 [==============================] - 712s 127ms/step - loss: 0.0203 - acc: 0.7533 - val_loss: 0.0183 - val_acc: 0.7588\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87103\n",
      "Epoch 41/100\n",
      "5600/5600 [==============================] - 722s 129ms/step - loss: 0.0193 - acc: 0.7552 - val_loss: 0.0179 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87103\n",
      "Epoch 42/100\n",
      "5600/5600 [==============================] - 723s 129ms/step - loss: 0.0187 - acc: 0.7577 - val_loss: 0.0174 - val_acc: 0.7482\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.87103\n",
      "Epoch 43/100\n",
      "5600/5600 [==============================] - 721s 129ms/step - loss: 0.0183 - acc: 0.7526 - val_loss: 0.0171 - val_acc: 0.7485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: val_acc did not improve from 0.87103\n",
      "Epoch 44/100\n",
      "5600/5600 [==============================] - 714s 127ms/step - loss: 0.0176 - acc: 0.7592 - val_loss: 0.0168 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.87103\n",
      "Epoch 45/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0172 - acc: 0.7589 - val_loss: 0.0166 - val_acc: 0.7532\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87103\n",
      "Epoch 46/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0171 - acc: 0.7594 - val_loss: 0.0165 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.87103\n",
      "Epoch 47/100\n",
      "5600/5600 [==============================] - 720s 129ms/step - loss: 0.0165 - acc: 0.7593 - val_loss: 0.0165 - val_acc: 0.7594\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87103\n",
      "Epoch 48/100\n",
      "5600/5600 [==============================] - 714s 127ms/step - loss: 0.0161 - acc: 0.7600 - val_loss: 0.0159 - val_acc: 0.7652\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87103\n",
      "Epoch 49/100\n",
      "5600/5600 [==============================] - 718s 128ms/step - loss: 0.0156 - acc: 0.7647 - val_loss: 0.0160 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87103\n",
      "Epoch 50/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0153 - acc: 0.7674 - val_loss: 0.0157 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87103\n",
      "Epoch 51/100\n",
      "5600/5600 [==============================] - 720s 129ms/step - loss: 0.0150 - acc: 0.7680 - val_loss: 0.0156 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.87103\n",
      "Epoch 52/100\n",
      "5600/5600 [==============================] - 720s 128ms/step - loss: 0.0147 - acc: 0.7721 - val_loss: 0.0156 - val_acc: 0.7774\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87103\n",
      "Epoch 53/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0143 - acc: 0.7723 - val_loss: 0.0153 - val_acc: 0.7750\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87103\n",
      "Epoch 54/100\n",
      "5600/5600 [==============================] - 719s 128ms/step - loss: 0.0140 - acc: 0.7698 - val_loss: 0.0154 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87103\n",
      "Epoch 55/100\n",
      "5600/5600 [==============================] - 714s 127ms/step - loss: 0.0137 - acc: 0.7688 - val_loss: 0.0151 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87103\n",
      "Epoch 56/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0135 - acc: 0.7726 - val_loss: 0.0149 - val_acc: 0.7742\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87103\n",
      "Epoch 57/100\n",
      "5600/5600 [==============================] - 721s 129ms/step - loss: 0.0134 - acc: 0.7750 - val_loss: 0.0152 - val_acc: 0.7704\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.87103\n",
      "Epoch 58/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0131 - acc: 0.7682 - val_loss: 0.0148 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87103\n",
      "Epoch 59/100\n",
      "5600/5600 [==============================] - 723s 129ms/step - loss: 0.0129 - acc: 0.7724 - val_loss: 0.0147 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87103\n",
      "Epoch 60/100\n",
      "5600/5600 [==============================] - 712s 127ms/step - loss: 0.0126 - acc: 0.7667 - val_loss: 0.0147 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.87103\n",
      "Epoch 61/100\n",
      "5600/5600 [==============================] - 713s 127ms/step - loss: 0.0122 - acc: 0.7677 - val_loss: 0.0146 - val_acc: 0.7695\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.87103\n",
      "Epoch 62/100\n",
      "5600/5600 [==============================] - 720s 129ms/step - loss: 0.0121 - acc: 0.7635 - val_loss: 0.0144 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.87103\n",
      "Epoch 63/100\n",
      "5600/5600 [==============================] - 726s 130ms/step - loss: 0.0121 - acc: 0.7671 - val_loss: 0.0148 - val_acc: 0.7476\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.87103\n",
      "Epoch 64/100\n",
      "5600/5600 [==============================] - 721s 129ms/step - loss: 0.0121 - acc: 0.7686 - val_loss: 0.0144 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.87103\n",
      "Epoch 65/100\n",
      "5600/5600 [==============================] - 716s 128ms/step - loss: 0.0119 - acc: 0.7662 - val_loss: 0.0145 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.87103\n",
      "Epoch 66/100\n",
      "5600/5600 [==============================] - 716s 128ms/step - loss: 0.0136 - acc: 0.7613 - val_loss: 0.0167 - val_acc: 0.7684\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.87103\n",
      "Epoch 67/100\n",
      "5600/5600 [==============================] - 721s 129ms/step - loss: 0.0137 - acc: 0.7644 - val_loss: 0.0144 - val_acc: 0.7471\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.87103\n",
      "Epoch 68/100\n",
      "5600/5600 [==============================] - 712s 127ms/step - loss: 0.0117 - acc: 0.7622 - val_loss: 0.0142 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.87103\n",
      "Epoch 69/100\n",
      "5600/5600 [==============================] - 720s 129ms/step - loss: 0.0115 - acc: 0.7629 - val_loss: 0.0142 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.87103\n",
      "Epoch 70/100\n",
      "5600/5600 [==============================] - 716s 128ms/step - loss: 0.0110 - acc: 0.7590 - val_loss: 0.0190 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.87103\n",
      "Epoch 71/100\n",
      "5600/5600 [==============================] - 715s 128ms/step - loss: 0.0118 - acc: 0.7578 - val_loss: 0.0140 - val_acc: 0.7561\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.87103\n",
      "Epoch 72/100\n",
      "5600/5600 [==============================] - 721s 129ms/step - loss: 0.0105 - acc: 0.7572 - val_loss: 0.0139 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.87103\n",
      "Epoch 73/100\n",
      "5600/5600 [==============================] - 711s 127ms/step - loss: 0.0106 - acc: 0.7699 - val_loss: 0.0136 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.87103\n",
      "Epoch 74/100\n",
      "5600/5600 [==============================] - 730s 130ms/step - loss: 0.0099 - acc: 0.7696 - val_loss: 0.0137 - val_acc: 0.7830\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.87103\n",
      "Epoch 75/100\n",
      "5600/5600 [==============================] - 722s 129ms/step - loss: 0.0098 - acc: 0.7732 - val_loss: 0.0137 - val_acc: 0.7714\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.87103\n",
      "Epoch 76/100\n",
      "5600/5600 [==============================] - 715s 128ms/step - loss: 0.0096 - acc: 0.7724 - val_loss: 0.0138 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.87103\n",
      "Epoch 77/100\n",
      "5600/5600 [==============================] - 720s 129ms/step - loss: 0.0095 - acc: 0.7740 - val_loss: 0.0137 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.87103\n",
      "Epoch 78/100\n",
      "5600/5600 [==============================] - 717s 128ms/step - loss: 0.0093 - acc: 0.7660 - val_loss: 0.0133 - val_acc: 0.7816\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.87103\n",
      "Epoch 79/100\n",
      "5600/5600 [==============================] - 720s 128ms/step - loss: 0.0091 - acc: 0.7706 - val_loss: 0.0139 - val_acc: 0.7651\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.87103\n",
      "Epoch 80/100\n",
      "5600/5600 [==============================] - 712s 127ms/step - loss: 0.0089 - acc: 0.7760 - val_loss: 0.0134 - val_acc: 0.7821\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.87103\n",
      "Epoch 81/100\n",
      "5600/5600 [==============================] - 717s 128ms/step - loss: 0.0087 - acc: 0.7761 - val_loss: 0.0134 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.87103\n",
      "Epoch 82/100\n",
      "5600/5600 [==============================] - 721s 129ms/step - loss: 0.0086 - acc: 0.7749 - val_loss: 0.0135 - val_acc: 0.7885\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.87103\n",
      "Epoch 83/100\n",
      "5600/5600 [==============================] - 716s 128ms/step - loss: 0.0312 - acc: 0.7858 - val_loss: 0.0259 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.87103\n",
      "Epoch 84/100\n",
      "5600/5600 [==============================] - 719s 128ms/step - loss: 0.0262 - acc: 0.7723 - val_loss: 0.0210 - val_acc: 0.7539\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.87103\n",
      "Epoch 85/100\n",
      "5600/5600 [==============================] - 725s 129ms/step - loss: 0.0227 - acc: 0.7743 - val_loss: 0.0195 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.87103\n",
      "Epoch 86/100\n",
      "5600/5600 [==============================] - 718s 128ms/step - loss: 0.0207 - acc: 0.7665 - val_loss: 0.0184 - val_acc: 0.7744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: val_acc did not improve from 0.87103\n",
      "Epoch 87/100\n",
      "5600/5600 [==============================] - 720s 129ms/step - loss: 0.0197 - acc: 0.7721 - val_loss: 0.0177 - val_acc: 0.7561\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.87103\n",
      "Epoch 88/100\n",
      "5600/5600 [==============================] - 710s 127ms/step - loss: 0.0277 - acc: 0.7632 - val_loss: 0.0259 - val_acc: 0.7795\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.87103\n",
      "Epoch 89/100\n",
      "5600/5600 [==============================] - 715s 128ms/step - loss: 0.0279 - acc: 0.7514 - val_loss: 0.0248 - val_acc: 0.7384\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.87103\n",
      "Epoch 90/100\n",
      "5600/5600 [==============================] - 709s 127ms/step - loss: 0.0259 - acc: 0.7420 - val_loss: 0.0224 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.87103\n",
      "Epoch 91/100\n",
      "5600/5600 [==============================] - 719s 128ms/step - loss: 0.0228 - acc: 0.7227 - val_loss: 0.0196 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.87103\n",
      "Epoch 92/100\n",
      "5600/5600 [==============================] - 710s 127ms/step - loss: 0.0223 - acc: 0.7221 - val_loss: 0.0196 - val_acc: 0.7149\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.87103\n",
      "Epoch 93/100\n",
      "5600/5600 [==============================] - 724s 129ms/step - loss: 0.0202 - acc: 0.7174 - val_loss: 0.0178 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.87103\n",
      "Epoch 94/100\n",
      "5600/5600 [==============================] - 715s 128ms/step - loss: 0.0187 - acc: 0.7282 - val_loss: 0.0174 - val_acc: 0.7419\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.87103\n",
      "Epoch 95/100\n",
      "5600/5600 [==============================] - 714s 127ms/step - loss: 0.0179 - acc: 0.7315 - val_loss: 0.0172 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.87103\n",
      "Epoch 96/100\n",
      "5600/5600 [==============================] - 717s 128ms/step - loss: 0.0192 - acc: 0.7279 - val_loss: 0.0177 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.87103\n",
      "Epoch 97/100\n",
      "5600/5600 [==============================] - 714s 127ms/step - loss: 0.0173 - acc: 0.7730 - val_loss: 0.0165 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.87103\n",
      "Epoch 98/100\n",
      "5600/5600 [==============================] - 715s 128ms/step - loss: 0.0162 - acc: 0.7857 - val_loss: 0.0161 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.87103\n",
      "Epoch 99/100\n",
      "5600/5600 [==============================] - 724s 129ms/step - loss: 0.0158 - acc: 0.7743 - val_loss: 0.0158 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.87103\n",
      "Epoch 100/100\n",
      "5600/5600 [==============================] - 714s 127ms/step - loss: 0.0152 - acc: 0.7720 - val_loss: 0.0156 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.87103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x165a968ca20>"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save history and continue training\n",
    "with open(\"seqWeights/LSTM-manytomany.txt\", 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "model.load_weights(\"seqWeights/LSTM-manytomany-17-0.87.hdf5\")\n",
    "adam = keras.optimizers.Adam(lr = .001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(encoder_input_data[0:numExamples, :, :],\n",
    "          decoder_input_data[0:numExamples, :, :],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2, verbose = 1,\n",
    "         callbacks = [history, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 9441s 1s/step - loss: 0.9891 - acc: 0.2595 - val_loss: 0.9704 - val_acc: 0.2642\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.26419, saving model to LSTM7layer-01-0.26.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_84 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_77/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_77/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n",
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_85 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_78/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_78/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n",
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_86 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_79/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_79/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n",
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_87 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_80/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_80/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n",
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_88 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_81/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_81/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n",
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_89 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_82/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_82/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n",
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_90 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_83/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_83/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "5100/8000 [==================>...........] - ETA: 27:29 - loss: 0.9641 - acc: 0.2678"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-9370f5d4f51f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m           callbacks = [history, checkpoint])\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 50\n",
    "numExamples = 10000\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#model.load_weights(\"seqWeights/LSTM7layer-05-0.82.hdf5\")\n",
    "adam = keras.optimizers.Adam(lr = .01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Run training\n",
    "history = History()\n",
    "filepath=\"LSTM7layer-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data[0:numExamples, :, :], decoder_input_data[0:numExamples, :, :]], decoder_target_data[0:numExamples, :, :],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2, verbose = 1,\n",
    "          callbacks = [history, checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"seqWeights/LSTM-manytomany-17-0.87.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_inf = Model(encoder_inputs, [e_h1, e_c1, e_h2, e_c2, e_h3, e_c3, e_h4, e_c4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Inference on new sequence\n",
    "\n",
    "d_input_states = [d_input_h1, d_input_c1, d_input_h2, d_input_c2, d_input_h3, d_input_c3, d_input_h4, d_input_c4]\n",
    "d_input_h1 = Input(shape = (latent_dim, ))\n",
    "d_input_c1 = Input(shape = (latent_dim, ))\n",
    "d_input_h2 = Input(shape = (latent_dim, ))\n",
    "d_input_c2 = Input(shape = (latent_dim, ))\n",
    "d_input_h3 = Input(shape = (latent_dim, ))\n",
    "d_input_c3 = Input(shape = (latent_dim, ))\n",
    "d_input_h4 = Input(shape = (latent_dim, ))\n",
    "d_input_c4 = Input(shape = (latent_dim, ))\n",
    "\n",
    "\n",
    "d_out_1, d_h1, d_c1 = d_lstm_1(decoder_inputs, initial_state = [d_input_h1, d_input_c1])\n",
    "d_out_2, d_h2, d_c2 = d_lstm_2(d_out_1, initial_state = [d_input_h2, d_input_c2])\n",
    "d_out_3, d_h3, d_c3 = d_lstm_3(d_out_2, initial_state = [d_input_h3, d_input_c3])\n",
    "d_out_4, d_h4, d_c4 = d_lstm_4(d_out_3, initial_state = [d_input_h4, d_input_c4])\n",
    "\n",
    "d_output_states = [d_h1, d_c1, d_h2, d_c2, d_h3, d_c3, d_h4, d_c4 ]\n",
    "\n",
    "decoder_out = d_dense(d_out_4)\n",
    "decoder_model_inf = Model(inputs = [decoder_inputs] + [d_input_h1, d_input_c1, d_input_h2, d_input_c2, d_input_h3, d_input_c3, d_input_h4, d_input_c4],\n",
    "                         outputs = [decoder_out] + [d_h1, d_c1, d_h2, d_c2, d_h3, d_c3, d_h4, d_c4 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_nuc = ['A', 'T', 'C', 'G', '-', '\\t', '\\n']\n",
    "def decode_seq(inp_seq):\n",
    "    print(inp_seq.shape)\n",
    "    states = encoder_model_inf.predict(inp_seq)\n",
    "    print(len(states))\n",
    "    target_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "    target_seq[0,0, one_hot_input['\\t']] = 1\n",
    "    \n",
    "    translated_seq = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        decoder_out, d_h1, d_c1, d_h2, d_c2, d_h3, d_c3, d_h4, d_c4  = decoder_model_inf.predict(x = [target_seq] + states)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0, -1, :])\n",
    "        sampled_nucleotide = index_to_nuc[max_val_index]\n",
    "        translated_seq += sampled_nucleotide\n",
    "        \n",
    "        if (sampled_nucleotide == '\\n'):\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "        target_seq[0,0, max_val_index] = 1\n",
    "        \n",
    "        states = [d_h1, d_c1, d_h2, d_c2, d_h3, d_c3, d_h4, d_c4]\n",
    "    return(translated_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACAGGGGTGGCAAGCGTTGTCCGGATTTACTGGGTGTAAAGGGTGCGCAGGCGGATTCATAAGTCGGGGGTTAAATCCATGTGCTTAACACATGCAAGGCTTCCGATACTGTGAGTCTATAGTCTCGAAGAGGAAGATGGAATTTCCGGTGTAACGGTGGAATGTGTAGATATCGGAAAGAACACCAGTGGCGAAGGCAGTCTTCTGGTCGAGAACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACGGAGGGTGCGAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGCTTGGTAAGTCAGGGGTGAAAGCCCGCGGCTCAACCGCGGAATTGCCTTTGATACTGC-CGAGCTAGAGTCCGGGAGAGGGTAGTGGAATTCCAGGTGTAGGAGTGAAATCCGTAGAGATCTGGAGGAACATCAGTGGCGAAGGCGACTACCTGGACCGGTACTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACAGAGGGGGCAAGCGTTGTCCGGAGTTACTGGGCGTAAAGGGCGCGCAGGCGGTGGGCTGCGTCGGCGCTGAAAGCGCCCCGCTTAACGGGGCGAGGCGCGCCGATACGAGTCCACTCGAGGCAAGCAGAGGGTGGCGGAATTCCGGGTGGAGTGGTGAAATGCGTAGAGATCCGGAGGAACGCCGGTGGGGAAGCCGGCCACCTGGGCTTGACCTGACGCTGCGGCGCGACAGCGTGGGGAGCAAACCG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACAGAGGGGGCAAGCGTTGTCCGGAGTTACTGGGCGTAAAGGGCGCGCAGGCGGTGGGCTGCGTCGGCGCTGAAAGCGCCCCGCTTAACGGGGCGAGGCGCGCCGATACGAGTCCACTCGAGGCAAGCAGAGGGTGGCGGAATTCCGGGTGGAGCGGTGAAATGCGTAGAGATCCGGAGGAACGCCGGTGGGGAAGCCGGCCACCTGGGCTTGACCTGACGCTGCGGCGCGACAGCGTGGGGAGCAAACCG\n",
      "\n",
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACGTAGGTCCCGAACGTTGCGCGAATTTACTGGGCGTAAAGGGTCCGTAGGCGGTTTAGCAAGTGGTTGGTGAAATTTCACGGCTCAACCGTGAAACTGCCTTCCAAACTGCTAAACTTGAGGCAGGGAGAGGTCGGCGGAATTCCCGGTGTAGCGGTGAAATGCGTAGATATCGGGAGGAACACCAGTGGCGAAGGCGGCCGACTGGAACTGTTCTGACGCTGAGGGACGAAAGCTAGGGGAGCAAACCG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACGTAGGTCCCGAACGTTGCGCGAATTTACTGGGCGTAAAGGGTCCGTAGGCGGTTTAGCAAGTGGTTGGTGAAATTTCACGGCTCAACCGTGAAACTGCCTTCCAAACTGCTAAACTTGAGGCAGGGAGAGGTCGGCGGAATTCCCGGTGTAGCGGTGAAATGCGTAGATATCGGGAGGAACACCAGTGGCGAAGGCGGCCGACTGGAACTGTCCTGACGCTGAGGGACGAAAGCTAGGGGAGCAAACCG\n",
      "\n",
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACCGACGGCCCGAGTGGTGGCCACTTTTATTGGGCCTAAAGCGTCCGTAGCCGGTCCAGTAAGTCCTTGTTTAAATCCTGCGGCTTAACCGCAGGACTGGCAGGGATACTGCTGGACTTGGGACCGGGAGAGGACAAGGGTACTTCAGGGGTAGCGGTGAAATGTGTTGATCCTTGAAGGACCACCTATGGCGAAGGCACTTGTCTGGAACGGGCCCGACGGTGAGGGACGAAAGCCAGGGGCGCGAACCG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACCGACGGCCCGAGTGGTGGCCACTTTTATTGGGCCTAAAGCGTCCGTAGCCGGTCCAGTAAGTCCTTGTTTAAATCCTGCGGCTTAACCGCAGGACTGGCAGGGATACTGCTGGACTTGGGACCGGGAGAGGACAAGGGTACTTCAGGGGTAGCGGTGAAATGTGTTGATCCTTGAAGGACCACCTATGGCGAAGGCACTTGTCTGGAACGGGTCCGACGGTGAGGGACGAAAGCCAGGGGCGCGAACCG\n",
      "\n",
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACGTATGTCACAAGCGTTATCCGGATTTATTGGGCGTAAAGCGCGTCTAGGTGGTTATGCAAGTCTGATGTGAAAATGCAGGGCTCAACTCTGTATTGCGTTGGAAACTGTGTAACTAGAGTACTGGAGAGGTAAGCGGAACTACAAGTGTAGAGGTGAAATTCGTAGATATTTGTAGGAATGCCGATGGGGAAGCCAGCTTACTAGACAGATACTGACGCTGAAGCGCGAAAGCGTGGGTAGCAAACAG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACGTATGTCACAAGCGTTATCCGGATTTATTGGGCGTAAAGCGCGTCTAGGTGGTTATGTAAGTCTGATGTGAAAATGCAGGGCTCAACTCTGTATTGCGTTGGAAACTGTGTAACTAGAGTACTGGAGAGGTAAGCGGAACTACAAGTGTAGAGGTGAAATTCGTAGATATTTGTAGGAATGCCGATGGGGAAGCCAGCTTACTAGACAGATACTGACGCTGAAGCGCGAAAGCGTGGGTAGCAAACAG\n",
      "\n",
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACGGAGGGTCCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTTGTTAAGCGAGATGTGAAAGCCCCGGGCTCAACCTGGGAATTGCATTTCGAACTGGCGAACTAGAGTCTCGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAAAGACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACGGAGGGTCCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTTGTTAAGCGAGATGTGAAAGCCCCGGGCTCAACCTGGGAATTGCATTTCGAACTGGCGAACTAGAGTCTTGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAAAGACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACGGAGGGGGTTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGCGCGTAGGCGGACCAGAAAGTTGGGGGTGAAATCCCGGGGCTCAACCCCGGAACTGCATCCAAAACTCCTGGTCTTGAGTTCGAGAGAGGTGAGTGGAATTCCGAGTGTAGAGGTGAAATTCGTAGATATTCGGAGGAACACCAGTGGCGAAGGCGGCTCACTGGCTCGATACTGACGCTGAGGTGCGAAAGTGTGGGGAGCAAACAG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACGGAGGGGGTTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGCGCGTAGGCGGACCAGAAAGTTGGGGGTGAAATCCCGGGGCTCAACCCCGGAACTGCCTCCAAAACTCCTGGTCTTGAGTTCGAGAGAGGTGAGTGGAATTCCGAGTGTAGAGGTGAAATTCGTAGATATTCGGAGGAACACCAGTGGCGAAGGCGGCTCACTGGCTCGATACTGACGCTGAGGTGCGAAAGTGTGGGGAGCAAACAG\n",
      "\n",
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACCGACGGCCCGAGTGGTGGCCACTTTTATTGGGCCTAAAGCGTCCGTAGCCGGTCCAGTAAGTCCTTGTTTAAATCCTGCGGCTTAACCGCAGGACTGGCAGGGATACTGCTGGACTTGGGACCGGGAGAGGACAAGGGTACTTCAGGGGTAGCGGTGAAATGTGTTGATCCTTGAAGGACCACCTATGGCGAAGGCACTTGTCTGGAACGGGTCCGACGGTGAGGGACGAAGGCCAGGGGAGCAAACCG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACCGACGGCCCGAGTGGTGGCCACTTTTATTGGGCCTAAAGCGTCCGTAGCCGGTCCAGTAAGTCCTTGTTTAAATCCTGCGGCTTAACCGCAGGACTGGCAGGGATACTGCTGGACTTGGGACCGGGAGAGGACAAGGGTACTTCAGGGGTAGCGGTGAAATGTGTTGATCCTTGAAGGACCACCTATGGCGAAGGCACTTGTCTGGAACGGGTCCGACGGTGAGGGACGAAAGCCAGGGGCGCGAACCG\n",
      "\n",
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACGGAGGGTCCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTTGTTAAGCGAGATGTGAAAGCCCCGGGCTCAACCTGGGAATTGCATTTCGAACTGGCGAACTAGAGTCTTGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGG-CCCCTGGACAAAGACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACGGAGGGTCCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTTGTTAAGCGAGATGTGAAAGCCCCGGGCTCAACCTGGGAATTGCATTTCGAACTGGCGAACTAGAGTCTTGTAGAGGGGGGTAGAATTCCAGGTGTAGCGGTGAAATGCGTAGAGATCTGGAGGAATACCGGTGGCGAAGGCGGCCCCCTGGACAAAGACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "(1, 351, 7)\n",
      "8\n",
      "Original Sequence: ACGGAGGATCCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGGTGGACAGTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGCTGTCTTGAGTACAGTAGAGGTGGGCGGAATTCGTGGTGTAGCGGTGAAATGCTTAGATATCACGAAGAACTCCGATTGCGAAGGCAGCCTGCTAAGCTGCAACTGACATTGAGGCTCGAAAGTGTGGGTATCAAACAG\n",
      "Predicted Sequence: ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG\n",
      "\n",
      "Target Sequence: \tACGGAGGATCCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGGTGGACAGTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGCTGTCTTGAGTACAGTAGAGGTGGGCGGAATTCGTGGTGTAGCGGTGAAATGCTTAGATATCACGAAGAACTCCGATTGCGAAGGCAGCTCACTGGACTGCAACTGACACTGATGCTCGAAAGTGTGGGTATCAAACAG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_seqs = diff_entries_input\n",
    "output_seqs = diff_entries_output\n",
    "\n",
    "for i in range(10):\n",
    "    input_seq = np.array(np.expand_dims(encoder_input_data[i, :, :], axis = 0))\n",
    "    trans_seq = decode_seq(input_seq)\n",
    "    print(\"Original Sequence: \" + input_seqs[i])\n",
    "    print(\"Predicted Sequence: \" + trans_seq)\n",
    "    print(\"Target Sequence: \" + output_seqs[i])\n",
    "    \n",
    "#Metric can be standard confusion matrix: of the bases that should have been substituted, how many were substituted correctly (true positive)\n",
    "#of the bases that should not have been substituted, how many were not substituted, true negative\n",
    "# of the bases that should not have been substituted, how many were: false positive\n",
    "# of the bases that should have been substituted, how many were not: false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.load(open( \"history_lr0.001\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "history = pickle.load(open('history_lr' + str(lr), \"rb\"))\n",
    "def plot_loss(history):\n",
    "    epochs = len(history[\"loss\"])\n",
    "    plt.plot(range(0, epochs), history[\"loss\"])\n",
    "    plt.plot(range(0, epochs), history[\"val_loss\"])\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_new = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_train_model(model, history, model_save, history_save, lr = .01, epochs = 10, batch_size = 50):\n",
    "    \n",
    "    history_new = History()\n",
    "    \n",
    "    adam = keras.optimizers.Adam(lr = lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "    model.fit([encoder_input_data[0:numExamples, :, :], \n",
    "               decoder_input_data[0:numExamples, :, :]],\n",
    "               decoder_target_data[0:numExamples, :, :],\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_split=0.2, \n",
    "                verbose = 1,\n",
    "                callbacks = [history_new])\n",
    "\n",
    "    #update history\n",
    "    for k in history.keys():\n",
    "        history[k] = history[k] + history_new.history[k]\n",
    "    with open(history_save, 'wb') as file_pi:\n",
    "        pickle.dump(history, file_pi)\n",
    "        \n",
    "    #Save model\n",
    "    model.save(model_save)\n",
    "    return(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/10\n",
      "5600/5600 [==============================] - 224s 40ms/step - loss: 0.0798 - acc: 0.7747 - val_loss: 0.0685 - val_acc: 0.7723\n",
      "Epoch 2/10\n",
      "5600/5600 [==============================] - 229s 41ms/step - loss: 0.0707 - acc: 0.7751 - val_loss: 0.0598 - val_acc: 0.7758\n",
      "Epoch 3/10\n",
      "5600/5600 [==============================] - 232s 41ms/step - loss: 0.1394 - acc: 0.7452 - val_loss: 0.1471 - val_acc: 0.7368\n",
      "Epoch 4/10\n",
      "5600/5600 [==============================] - 234s 42ms/step - loss: 0.1315 - acc: 0.7457 - val_loss: 0.1064 - val_acc: 0.7546\n",
      "Epoch 5/10\n",
      "5600/5600 [==============================] - 234s 42ms/step - loss: 0.1164 - acc: 0.7536 - val_loss: 0.1231 - val_acc: 0.7497\n",
      "Epoch 6/10\n",
      "5600/5600 [==============================] - 186s 33ms/step - loss: 0.1119 - acc: 0.7528 - val_loss: 0.0914 - val_acc: 0.7563\n",
      "Epoch 7/10\n",
      "5600/5600 [==============================] - 183s 33ms/step - loss: 0.0977 - acc: 0.7573 - val_loss: 0.0834 - val_acc: 0.7637\n",
      "Epoch 8/10\n",
      "5600/5600 [==============================] - 211s 38ms/step - loss: 0.0943 - acc: 0.7623 - val_loss: 0.0916 - val_acc: 0.7700\n",
      "Epoch 9/10\n",
      "5600/5600 [==============================] - 236s 42ms/step - loss: 0.1139 - acc: 0.7610 - val_loss: 0.0937 - val_acc: 0.7662\n",
      "Epoch 10/10\n",
      "5600/5600 [==============================] - 229s 41ms/step - loss: 0.1000 - acc: 0.7667 - val_loss: 0.0846 - val_acc: 0.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_12 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_11_15/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_11_15/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load and continue training as desired\n",
    "lr = 0.001\n",
    "model = load_model('s2s_lr.01_lr.001.h5')\n",
    "history = pickle.load(open('history_lr.01_ly.001.h5', \"rb\"))\n",
    "\n",
    "model, history = continue_train_model(model = model, history = history, model_save = ('s2s_lr.01_lr.001.h5'), history_save = ('history_lr.01_ly.001.h5'), \n",
    "                                      lr = .001, epochs = 10, batch_size = 50)\n",
    " \n",
    "    \n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_multilayer(num_encoder_tokens, num_decoder_tokens):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = num_encoder_tokens, output_dim=162))\n",
    "    model.add(LSTM(units = 100, return_sequences=True, input_shape = (None, 162))) #None refers to unknown sequence length\n",
    "    model.add(LSTM(100, return_sequences = True))\n",
    "    model.add(LSTM(50, return_sequences = True))\n",
    "    model.add(Dense(162, activation = \"softmax\"))\n",
    "    adam = keras.optimizers.Adam(lr = .01, beta_1=0.9, beta_2 = 0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return(model)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "[0. 0. 1. 0. 0. 0. 0.]\n",
      "(5000, 351, 7)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_16 to have 3 dimensions, but got array with shape (5000, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-6fd2afbec8fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m           \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m           callbacks = [history])\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_16 to have 3 dimensions, but got array with shape (5000, 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "#With more latent dimensions\n",
    "numExamples = 5000\n",
    "batch_size = 50\n",
    "epochs = 2\n",
    "\n",
    "model = define_model_multilayer(num_encoder_tokens = 7, num_decoder_tokens = 7)\n",
    "history = History()\n",
    "\n",
    "print(len(diff_entries_input[0]))\n",
    "print(encoder_input_data[1, 1, :])\n",
    "print(encoder_input_data[0:numExamples, :, :].shape)\n",
    "#model.fit(encoder_input_data[0:numExamples, :, :],\n",
    "#          decoder_target_data[0:numExamples, :, :],\n",
    "#          batch_size=batch_size,\n",
    "#          epochs=epochs,\n",
    "#          validation_split=0.1, \n",
    "#          verbose = 1,\n",
    "#          callbacks = [history])\n",
    "\n",
    "model.fit(np.array(diff_entries_input)[0:numExamples, ],\n",
    "          np.array(diff_entries_output)[0:numExamples, ],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1, \n",
    "          verbose = 1,\n",
    "          callbacks = [history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(5, 351, 7)\n",
      "[0 2 3 3 0 3 3 3 1 3 2 3 0 3 2 3 1 1 0 0 1 2 3 3 0 0 1 2 0 2 1 3 3 3 2 3 1\n",
      " 0 0 0 3 2 3 2 0 2 3 1 0 3 3 2 1 3 2 1 1 3 3 1 0 0 3 1 2 0 3 3 3 3 1 3 0 0\n",
      " 0 3 2 2 2 3 2 3 3 2 1 2 0 0 2 2 3 2 3 3 0 0 1 1 3 2 2 1 1 1 3 0 1 0 2 1 3\n",
      " 2 0 2 3 0 3 2 1 0 3 0 3 1 2 2 3 3 3 0 3 0 3 3 3 1 0 3 1 3 3 0 0 1 1 2 2 0\n",
      " 3 3 1 3 1 0 3 3 0 3 1 3 0 0 0 1 2 2 3 1 0 3 0 3 0 1 2 1 3 3 0 3 3 0 0 2 0\n",
      " 1 2 0 3 1 3 3 2 3 0 0 3 3 2 3 0 2 1 0 2 2 1 3 3 0 2 2 3 3 1 0 2 1 3 0 2 3\n",
      " 2 1 3 0 3 3 1 3 2 3 0 0 0 3 2 3 1 3 3 3 3 0 3 2 0 0 0 2 0 3 6 2 2 2 2 2 2\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "['A' 'C' 'G' 'G' 'A' 'G' 'G' 'G' 'T' 'G' 'C' 'G' 'A' 'G' 'C' 'G' 'T' 'T'\n",
      " 'A' 'A' 'T' 'C' 'G' 'G' 'A' 'A' 'T' 'C' 'A' 'C' 'T' 'G' 'G' 'G' 'C' 'G'\n",
      " 'T' 'A' 'A' 'A' 'G' 'C' 'G' 'C' 'A' 'C' 'G' 'T' 'A' 'G' 'G' 'C' 'T' 'G'\n",
      " 'C' 'T' 'T' 'G' 'G' 'T' 'A' 'A' 'G' 'T' 'C' 'A' 'G' 'G' 'G' 'G' 'T' 'G'\n",
      " 'A' 'A' 'A' 'G' 'C' 'C' 'C' 'G' 'C' 'G' 'G' 'C' 'T' 'C' 'A' 'A' 'C' 'C'\n",
      " 'G' 'C' 'G' 'G' 'A' 'A' 'T' 'T' 'G' 'C' 'C' 'T' 'T' 'T' 'G' 'A' 'T' 'A'\n",
      " 'C' 'T' 'G' 'C' 'A' 'C' 'G' 'A' 'G' 'C' 'T' 'A' 'G' 'A' 'G' 'T' 'C' 'C'\n",
      " 'G' 'G' 'G' 'A' 'G' 'A' 'G' 'G' 'G' 'T' 'A' 'G' 'T' 'G' 'G' 'A' 'A' 'T'\n",
      " 'T' 'C' 'C' 'A' 'G' 'G' 'T' 'G' 'T' 'A' 'G' 'G' 'A' 'G' 'T' 'G' 'A' 'A'\n",
      " 'A' 'T' 'C' 'C' 'G' 'T' 'A' 'G' 'A' 'G' 'A' 'T' 'C' 'T' 'G' 'G' 'A' 'G'\n",
      " 'G' 'A' 'A' 'C' 'A' 'T' 'C' 'A' 'G' 'T' 'G' 'G' 'C' 'G' 'A' 'A' 'G' 'G'\n",
      " 'C' 'G' 'A' 'C' 'T' 'A' 'C' 'C' 'T' 'G' 'G' 'A' 'C' 'C' 'G' 'G' 'T' 'A'\n",
      " 'C' 'T' 'G' 'A' 'C' 'G' 'C' 'T' 'G' 'A' 'G' 'G' 'T' 'G' 'C' 'G' 'A' 'A'\n",
      " 'A' 'G' 'C' 'G' 'T' 'G' 'G' 'G' 'G' 'A' 'G' 'C' 'A' 'A' 'A' 'C' 'A' 'G'\n",
      " '\\n' 'C' 'C' 'C' 'C' 'C' 'C' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      "[0 2 0 3 0 3 3 3 3 3 2 0 0 3 2 3 1 1 3 1 2 2 3 3 0 3 1 1 0 2 1 3 3 3 2 3 1\n",
      " 0 0 0 3 3 3 2 3 2 3 2 0 3 3 2 3 3 1 3 3 3 2 1 3 2 3 1 2 3 3 2 3 2 1 3 0 0\n",
      " 0 3 2 3 2 2 2 2 3 2 1 1 0 0 2 3 3 3 3 2 3 0 3 3 2 3 2 3 2 2 3 0 1 0 2 3 0\n",
      " 3 1 2 2 0 2 1 2 3 0 3 3 2 0 0 3 2 0 3 0 3 3 3 1 3 3 2 3 3 0 0 1 1 2 2 3 3\n",
      " 3 1 3 3 0 3 1 3 3 1 3 0 0 0 1 3 2 3 1 0 3 0 3 0 1 2 2 3 3 0 3 3 0 0 2 3 2\n",
      " 2 3 3 1 3 3 3 3 0 0 3 2 2 3 3 2 2 0 2 2 1 3 3 3 2 1 1 3 0 2 2 1 3 0 2 3 2\n",
      " 1 3 2 3 3 2 3 2 3 0 2 0 3 2 3 1 3 3 3 3 0 3 2 0 0 0 2 2 3 3 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "['A' 'C' 'A' 'G' 'A' 'G' 'G' 'G' 'G' 'G' 'C' 'A' 'A' 'G' 'C' 'G' 'T' 'T'\n",
      " 'G' 'T' 'C' 'C' 'G' 'G' 'A' 'G' 'T' 'T' 'A' 'C' 'T' 'G' 'G' 'G' 'C' 'G'\n",
      " 'T' 'A' 'A' 'A' 'G' 'G' 'G' 'C' 'G' 'C' 'G' 'C' 'A' 'G' 'G' 'C' 'G' 'G'\n",
      " 'T' 'G' 'G' 'G' 'C' 'T' 'G' 'C' 'G' 'T' 'C' 'G' 'G' 'C' 'G' 'C' 'T' 'G'\n",
      " 'A' 'A' 'A' 'G' 'C' 'G' 'C' 'C' 'C' 'C' 'G' 'C' 'T' 'T' 'A' 'A' 'C' 'G'\n",
      " 'G' 'G' 'G' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'C' 'G' 'C' 'C' 'G' 'A' 'T' 'A'\n",
      " 'C' 'G' 'A' 'G' 'T' 'C' 'C' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'A' 'A'\n",
      " 'G' 'C' 'A' 'G' 'A' 'G' 'G' 'G' 'T' 'G' 'G' 'C' 'G' 'G' 'A' 'A' 'T' 'T'\n",
      " 'C' 'C' 'G' 'G' 'G' 'T' 'G' 'G' 'A' 'G' 'T' 'G' 'G' 'T' 'G' 'A' 'A' 'A'\n",
      " 'T' 'G' 'C' 'G' 'T' 'A' 'G' 'A' 'G' 'A' 'T' 'C' 'C' 'G' 'G' 'A' 'G' 'G'\n",
      " 'A' 'A' 'C' 'G' 'C' 'C' 'G' 'G' 'T' 'G' 'G' 'G' 'G' 'A' 'A' 'G' 'C' 'C'\n",
      " 'G' 'G' 'C' 'C' 'A' 'C' 'C' 'T' 'G' 'G' 'G' 'C' 'T' 'T' 'G' 'A' 'C' 'C'\n",
      " 'T' 'G' 'A' 'C' 'G' 'C' 'T' 'G' 'C' 'G' 'G' 'C' 'G' 'C' 'G' 'A' 'C' 'A'\n",
      " 'G' 'C' 'G' 'T' 'G' 'G' 'G' 'G' 'A' 'G' 'C' 'A' 'A' 'A' 'C' 'C' 'G' 'G'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      "[0 2 3 1 0 3 3 1 2 2 2 3 0 0 2 3 1 1 3 2 3 2 3 0 0 1 1 1 0 2 1 3 3 3 2 3 1\n",
      " 0 0 0 3 3 3 1 2 2 3 1 0 3 3 2 3 3 1 1 1 0 3 2 0 0 3 1 3 3 1 1 3 3 1 3 0 0\n",
      " 0 1 1 1 2 0 2 3 3 2 1 2 0 0 2 2 3 1 3 0 0 0 2 1 3 2 2 1 1 2 2 0 0 0 2 1 3\n",
      " 2 1 0 0 0 2 1 1 3 0 3 3 2 0 3 3 3 0 3 0 3 3 1 2 3 3 2 3 3 0 0 1 1 2 2 2 3\n",
      " 3 1 3 1 0 3 2 3 3 1 3 0 0 0 1 3 2 3 1 0 3 0 1 0 1 2 3 3 3 0 3 3 0 0 2 0 2\n",
      " 2 0 3 1 3 3 2 3 0 0 3 3 2 3 3 2 2 3 0 2 1 3 3 0 0 2 1 3 1 1 2 1 3 0 2 3 2\n",
      " 1 3 0 3 3 3 0 2 3 0 0 0 3 2 1 0 3 3 3 3 0 3 2 0 0 0 2 2 3 3 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "['A' 'C' 'G' 'T' 'A' 'G' 'G' 'T' 'C' 'C' 'C' 'G' 'A' 'A' 'C' 'G' 'T' 'T'\n",
      " 'G' 'C' 'G' 'C' 'G' 'A' 'A' 'T' 'T' 'T' 'A' 'C' 'T' 'G' 'G' 'G' 'C' 'G'\n",
      " 'T' 'A' 'A' 'A' 'G' 'G' 'G' 'T' 'C' 'C' 'G' 'T' 'A' 'G' 'G' 'C' 'G' 'G'\n",
      " 'T' 'T' 'T' 'A' 'G' 'C' 'A' 'A' 'G' 'T' 'G' 'G' 'T' 'T' 'G' 'G' 'T' 'G'\n",
      " 'A' 'A' 'A' 'T' 'T' 'T' 'C' 'A' 'C' 'G' 'G' 'C' 'T' 'C' 'A' 'A' 'C' 'C'\n",
      " 'G' 'T' 'G' 'A' 'A' 'A' 'C' 'T' 'G' 'C' 'C' 'T' 'T' 'C' 'C' 'A' 'A' 'A'\n",
      " 'C' 'T' 'G' 'C' 'T' 'A' 'A' 'A' 'C' 'T' 'T' 'G' 'A' 'G' 'G' 'C' 'A' 'G'\n",
      " 'G' 'G' 'A' 'G' 'A' 'G' 'G' 'T' 'C' 'G' 'G' 'C' 'G' 'G' 'A' 'A' 'T' 'T'\n",
      " 'C' 'C' 'C' 'G' 'G' 'T' 'G' 'T' 'A' 'G' 'C' 'G' 'G' 'T' 'G' 'A' 'A' 'A'\n",
      " 'T' 'G' 'C' 'G' 'T' 'A' 'G' 'A' 'T' 'A' 'T' 'C' 'G' 'G' 'G' 'A' 'G' 'G'\n",
      " 'A' 'A' 'C' 'A' 'C' 'C' 'A' 'G' 'T' 'G' 'G' 'C' 'G' 'A' 'A' 'G' 'G' 'C'\n",
      " 'G' 'G' 'C' 'C' 'G' 'A' 'C' 'T' 'G' 'G' 'A' 'A' 'C' 'T' 'G' 'T' 'T' 'C'\n",
      " 'T' 'G' 'A' 'C' 'G' 'C' 'T' 'G' 'A' 'G' 'G' 'G' 'A' 'C' 'G' 'A' 'A' 'A'\n",
      " 'G' 'C' 'T' 'A' 'G' 'G' 'G' 'G' 'A' 'G' 'C' 'A' 'A' 'A' 'C' 'C' 'G' 'G'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      "[0 2 2 3 0 2 3 3 2 2 2 3 0 3 1 3 3 1 3 3 2 2 0 2 1 1 1 1 0 1 1 3 3 3 2 2 1\n",
      " 0 0 0 3 2 3 1 2 2 3 1 0 3 2 2 3 3 1 2 2 0 3 1 0 0 3 1 2 2 1 1 3 1 1 1 0 0\n",
      " 0 1 2 2 1 3 2 3 3 2 1 1 0 0 2 2 3 2 0 3 3 0 2 1 3 3 2 0 3 3 3 0 1 0 2 1 3\n",
      " 2 1 3 3 0 2 1 1 3 3 3 0 2 2 3 3 3 0 3 0 3 3 0 2 0 0 3 3 3 1 0 2 1 1 2 0 3\n",
      " 3 3 3 1 0 3 2 3 3 1 3 0 0 0 1 3 1 3 1 1 3 0 1 2 2 1 1 3 0 0 3 3 0 2 2 0 2\n",
      " 2 1 0 1 3 3 2 3 0 0 3 3 2 0 2 1 1 3 1 2 1 3 3 0 0 2 3 3 3 2 2 2 3 0 2 3 3\n",
      " 1 3 0 3 3 3 0 2 3 0 0 0 3 2 2 0 3 3 3 3 2 3 2 3 0 0 2 2 3 3 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "['A' 'C' 'C' 'G' 'A' 'C' 'G' 'G' 'C' 'C' 'C' 'G' 'A' 'G' 'T' 'G' 'G' 'T'\n",
      " 'G' 'G' 'C' 'C' 'A' 'C' 'T' 'T' 'T' 'T' 'A' 'T' 'T' 'G' 'G' 'G' 'C' 'C'\n",
      " 'T' 'A' 'A' 'A' 'G' 'C' 'G' 'T' 'C' 'C' 'G' 'T' 'A' 'G' 'C' 'C' 'G' 'G'\n",
      " 'T' 'C' 'C' 'A' 'G' 'T' 'A' 'A' 'G' 'T' 'C' 'C' 'T' 'T' 'G' 'T' 'T' 'T'\n",
      " 'A' 'A' 'A' 'T' 'C' 'C' 'T' 'G' 'C' 'G' 'G' 'C' 'T' 'T' 'A' 'A' 'C' 'C'\n",
      " 'G' 'C' 'A' 'G' 'G' 'A' 'C' 'T' 'G' 'G' 'C' 'A' 'G' 'G' 'G' 'A' 'T' 'A'\n",
      " 'C' 'T' 'G' 'C' 'T' 'G' 'G' 'A' 'C' 'T' 'T' 'G' 'G' 'G' 'A' 'C' 'C' 'G'\n",
      " 'G' 'G' 'A' 'G' 'A' 'G' 'G' 'A' 'C' 'A' 'A' 'G' 'G' 'G' 'T' 'A' 'C' 'T'\n",
      " 'T' 'C' 'A' 'G' 'G' 'G' 'G' 'T' 'A' 'G' 'C' 'G' 'G' 'T' 'G' 'A' 'A' 'A'\n",
      " 'T' 'G' 'T' 'G' 'T' 'T' 'G' 'A' 'T' 'C' 'C' 'T' 'T' 'G' 'A' 'A' 'G' 'G'\n",
      " 'A' 'C' 'C' 'A' 'C' 'C' 'T' 'A' 'T' 'G' 'G' 'C' 'G' 'A' 'A' 'G' 'G' 'C'\n",
      " 'A' 'C' 'T' 'T' 'G' 'T' 'C' 'T' 'G' 'G' 'A' 'A' 'C' 'G' 'G' 'G' 'C' 'C'\n",
      " 'C' 'G' 'A' 'C' 'G' 'G' 'T' 'G' 'A' 'G' 'G' 'G' 'A' 'C' 'G' 'A' 'A' 'A'\n",
      " 'G' 'C' 'C' 'A' 'G' 'G' 'G' 'G' 'C' 'G' 'C' 'G' 'A' 'A' 'C' 'C' 'G' 'G'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      "[0 2 3 1 0 1 3 1 2 0 2 0 0 3 2 3 1 1 0 1 2 2 3 3 0 1 1 1 0 1 1 3 3 3 2 3 1\n",
      " 0 0 0 3 2 3 2 3 1 2 1 0 3 3 1 3 3 1 1 0 1 3 2 0 0 3 1 2 1 3 0 1 3 1 3 0 0\n",
      " 0 0 1 3 2 0 3 3 3 2 1 2 0 0 2 1 2 1 3 1 0 1 1 3 2 3 1 1 3 3 0 0 0 2 1 3 1\n",
      " 3 1 0 0 2 1 0 3 0 3 1 0 2 1 3 3 0 3 0 3 3 1 0 0 3 2 3 3 0 0 2 1 0 2 0 0 3\n",
      " 1 3 1 0 3 0 3 3 1 3 0 0 0 1 1 2 3 1 0 3 0 1 0 1 1 1 3 1 0 3 3 0 0 1 3 2 2\n",
      " 3 0 1 3 3 3 3 0 0 3 2 2 0 3 2 1 1 0 2 1 0 3 0 2 0 3 0 1 0 2 1 3 0 2 3 2 1\n",
      " 3 0 0 3 2 3 2 3 0 0 0 3 2 3 1 3 3 3 1 0 3 2 0 0 0 2 0 3 6 2 2 2 2 2 2 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "['A' 'C' 'G' 'T' 'A' 'T' 'G' 'T' 'C' 'A' 'C' 'A' 'A' 'G' 'C' 'G' 'T' 'T'\n",
      " 'A' 'T' 'C' 'C' 'G' 'G' 'A' 'T' 'T' 'T' 'A' 'T' 'T' 'G' 'G' 'G' 'C' 'G'\n",
      " 'T' 'A' 'A' 'A' 'G' 'C' 'G' 'C' 'G' 'T' 'C' 'T' 'A' 'G' 'G' 'T' 'G' 'G'\n",
      " 'T' 'T' 'A' 'T' 'G' 'C' 'A' 'A' 'G' 'T' 'C' 'T' 'G' 'A' 'T' 'G' 'T' 'G'\n",
      " 'A' 'A' 'A' 'A' 'T' 'G' 'C' 'A' 'G' 'G' 'G' 'C' 'T' 'C' 'A' 'A' 'C' 'T'\n",
      " 'C' 'T' 'G' 'T' 'A' 'T' 'T' 'G' 'C' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'C'\n",
      " 'T' 'G' 'T' 'G' 'T' 'A' 'A' 'C' 'T' 'A' 'G' 'A' 'G' 'T' 'A' 'C' 'T' 'G'\n",
      " 'G' 'A' 'G' 'A' 'G' 'G' 'T' 'A' 'A' 'G' 'C' 'G' 'G' 'A' 'A' 'C' 'T' 'A'\n",
      " 'C' 'A' 'A' 'G' 'T' 'G' 'T' 'A' 'G' 'A' 'G' 'G' 'T' 'G' 'A' 'A' 'A' 'T'\n",
      " 'T' 'C' 'G' 'T' 'A' 'G' 'A' 'T' 'A' 'T' 'T' 'T' 'G' 'T' 'A' 'G' 'G' 'A'\n",
      " 'A' 'T' 'G' 'C' 'C' 'G' 'A' 'T' 'G' 'G' 'G' 'G' 'A' 'A' 'G' 'C' 'C' 'A'\n",
      " 'G' 'C' 'T' 'T' 'A' 'C' 'T' 'A' 'G' 'A' 'C' 'A' 'G' 'A' 'T' 'A' 'C' 'T'\n",
      " 'G' 'A' 'C' 'G' 'C' 'T' 'G' 'A' 'A' 'G' 'C' 'G' 'C' 'G' 'A' 'A' 'A' 'G'\n",
      " 'C' 'G' 'T' 'G' 'G' 'G' 'T' 'A' 'G' 'C' 'A' 'A' 'A' 'C' 'A' 'G' '\\n' 'C'\n",
      " 'C' 'C' 'C' 'C' 'C' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T'\n",
      " 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict(encoder_input_data[0:5, :, :])\n",
    "print(len(probs))\n",
    "probs = np.array(probs)\n",
    "print(probs.shape)\n",
    "indices = np.argmax(probs, axis = -1)\n",
    "\n",
    "output_dict = np.array(['A', 'T', 'C', 'G', '-', '\\t', '\\n'])\n",
    "for i in range(indices.shape[0]):\n",
    "    print(output_dict[indices[i,]])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
