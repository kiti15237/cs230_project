{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import History \n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Concatenate\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "from termcolor import colored\n",
    "from IPython.display import clear_output\n",
    "from load_5hot import load\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "101886 sequences were uploaded\n",
      "\n",
      "Maximum sequence length in is 308\n",
      "Maximum sequence length out is 308\n",
      "\n",
      "Converting to one-hot...\n",
      "Done\n",
      "Converting to one-hot...\n",
      "Done\n",
      "Converting to one-hot...\n",
      "Done\n",
      "There are 77510 training examples\n",
      "There are 12107 testing examples\n",
      "There are 5 classes: A, C, G, T, -\n",
      "The longest sequence is 308 nucleotides long\n",
      "X_train shape is:\n",
      "(77510, 5, 308, 1)\n",
      "Permuting...\n",
      "finished X\n",
      "finished Y\n",
      "Training sample 0:\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "Validation sample 0:\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "Testing sample 0:\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "[X_train,Y_train,X_test,Y_test,X_val,Y_val] = load(\"blast_tab_1hit.out\")\n",
    "\n",
    "m = X_train.shape[0]\n",
    "print(\"There are \" + str(m) + \" training examples\")\n",
    "print(\"There are \" + str(X_test.shape[0]) + \" testing examples\")\n",
    "print(\"There are \" + str(X_train.shape[1]) + \" classes: A, C, G, T, -\")\n",
    "max_length = max(X_train.shape[2],X_test.shape[2])\n",
    "print(\"The longest sequence is \" + str(max_length) + \" nucleotides long\")\n",
    "print(\"X_train shape is:\")\n",
    "print(X_train.shape)\n",
    "\n",
    "print('Permuting...')\n",
    "np.random.seed(0)\n",
    "rand_perm = np.random.rand(m).argsort()\n",
    "np.take(X_train,rand_perm,axis=0,out=X_train)\n",
    "print(\"finished X\")\n",
    "np.take(Y_train,rand_perm,axis=0,out=Y_train)\n",
    "print(\"finished Y\")\n",
    "\n",
    "#Change dimensions so that we have examples x seqlen x onehot\n",
    "X_train = np.squeeze(np.swapaxes(X_train, 1, 2))\n",
    "Y_train = np.squeeze(np.swapaxes(Y_train, 1, 2))\n",
    "X_val = np.squeeze(np.swapaxes(X_val, 1, 2))\n",
    "Y_val = np.squeeze(np.swapaxes(Y_val, 1, 2))\n",
    "X_test = np.squeeze(np.swapaxes(X_test, 1, 2))\n",
    "Y_test = np.squeeze(np.swapaxes(Y_test, 1, 2))\n",
    "\n",
    "# Visualize data sets to ensure they appear as anticipated\n",
    "sample = 0\n",
    "print('Training sample 0:')\n",
    "print(X_train[sample,0:22, :])\n",
    "print(Y_train[sample,0:22, :])\n",
    "print('Validation sample 0:')\n",
    "print(X_val[sample,0:22, :])\n",
    "print(Y_val[sample,0:22, :])\n",
    "print('Testing sample 0:')\n",
    "print(X_test[sample,0:22, :])\n",
    "print(Y_test[sample,0:22, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_input = {'A': 0, 'C': 1, 'G': 2, 'T': 3, '-': 4}\n",
    "num_encoder_tokens = len(one_hot_input)\n",
    "num_decoder_tokens = len(one_hot_input)\n",
    "latent_dim = 100\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "e_lstm_1 = Bidirectional(LSTM(latent_dim, return_sequences = True))(encoder_inputs)\n",
    "e_dropout = Dropout(0.5)(e_lstm_1)\n",
    "e_lstm_2 = Bidirectional(LSTM(latent_dim, return_sequences = True))(e_dropout)\n",
    "#e_lstm_3 = Bidirectional(LSTM(latent_dim, return_sequences = True))\n",
    "\n",
    "output = TimeDistributed(Dense(num_decoder_tokens, activation = \"softmax\"))(e_lstm_2)\n",
    "#noise_class = Dense(1, activation = \"tanh\")(Concatenate()([h2, c2]))\n",
    "#noise_class = Activation(\"tanh\")(noise_class)\n",
    "#print(noise_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train by classification of noisy/non-noisy\n",
    "\n",
    "#model = Model(encoder_inputs, noise_class)\n",
    "\n",
    "#model.load_weights(\"seqWeights/LSTM-comboseqs-dropout0.5-manytomany-01-0.82.hdf5\")\n",
    "#adam = keras.optimizers.Adam(lr = .001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#model.compile(optimizer= adam, loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#history = History()\n",
    "#filepath=\"seqWeights/LSTM-test-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#numExamples = 1000\n",
    "#batch_size = 100\n",
    "#epochs = 50\n",
    "#output_seqs = decoder_input_data[0:numExamples, :, :]\n",
    "#y = output_seqs.reshape(numExamples, output_seqs.shape[1], 1)\n",
    "#model.fit(encoder_input_data[0:numExamples, :, :],\n",
    "#          np.array(labels)[0:numExamples],\n",
    "#          batch_size=batch_size,\n",
    "#          epochs=epochs,\n",
    "#          validation_split=0.2, verbose = 1,\n",
    "#         callbacks = [history, checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_acc = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "        \n",
    "        #ts = time.time()\n",
    "        #stamp = datetime.datetime.fromtimestamp(ts).strftime('%H:%M:%S')\n",
    "        \n",
    "        #self.losses_file = open('log_loss.txt', 'w')\n",
    "        #self.losses_file.truncate()\n",
    "        #self.losses_file.close()\n",
    "        #self.val_loss_file = open('log_val_loss.txt', 'w')\n",
    "        #self.val_loss_file.truncate()\n",
    "        #self.val_loss_file.close()\n",
    "        #self.val_acc_file = open('log_val_acc.txt', 'w')\n",
    "        #self.val_acc_file.truncate()\n",
    "        #self.val_acc_file.close()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        #plt.plot(self.x, self.val_acc, label = \"val_acc\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "\n",
    "        \n",
    "        self.losses_file = open('log_loss' + datetime.date.time() + '.txt', 'a')\n",
    "        self.losses_file.write(str(logs.get('loss')) + \",\")\n",
    "        self.losses_file.close()\n",
    "        \n",
    "        self.val_loss_file = open('log_val_loss' + datetime.date.time() + '.txt', 'a')\n",
    "        self.val_loss_file.write(str(logs.get('val_loss')) + \",\")\n",
    "        self.val_loss_file.close()\n",
    "        \n",
    "        self.val_acc_file = open('log_val_acc' + datetime.date.time() + '.txt', 'a')\n",
    "        self.val_acc_file.write(str(logs.get('val_acc')) + \",\")\n",
    "        self.val_acc_file.close()\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "73600/77510 [===========================>..] - ETA: 4:03 - loss: 0.0022 - acc: 0.8876"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6ed712993972>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m           \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m           callbacks = [history, checkpoint, plot_losses])\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train by sequence correctness\n",
    "model = Model(encoder_inputs, output)\n",
    "\n",
    "#model.load_weights(\"seqWeights/LSTM-test-comboseqs-dropout0.5-manytomany-11-0.83.hdf5\")\n",
    "adam = keras.optimizers.Adam(lr = .001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.95, amsgrad=False)\n",
    "model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = History()\n",
    "filepath=\"seqWeights/LSTM-2-comboseqs-dropout0.5-manytomany-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only= False, mode='max')\n",
    "\n",
    "#numExamples = 70000\n",
    "batch_size = 100\n",
    "epochs = 30\n",
    "#output_seqs = decoder_input_data[0:numExamples, :, :]\n",
    "#y = output_seqs.reshape(numExamples, output_seqs.shape[1], 1)\n",
    "model.fit(x = X_train, y = Y_train,\n",
    "          validation_data = (X_val, Y_val),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose = 1,\n",
    "          callbacks = [history, checkpoint, plot_losses])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSTM-2-comboseqs-dropout0.5-manytomany-01-0.84.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-02-0.84.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-03-0.83.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-04-0.83.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-05-0.90.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-06-0.89.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-07-0.94.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-08-0.94.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-09-0.93.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-10-0.90.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-11-0.89.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-12-0.89.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-13-0.89.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-14-0.90.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-15-0.88.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-16-0.89.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-17-0.88.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-18-0.86.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-19-0.89.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-20-0.88.hdf5',\n",
       " 'LSTM-2-comboseqs-dropout0.5-manytomany-21-0.89.hdf5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"seqWeights/final/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getPredicted(weightFile, X_data):\n",
    "        #calculate probabilities from model for test data\n",
    "        model.load_weights(\"seqWeights/final/\" + weightFile)\n",
    "        probs = np.array(model.predict(X_data))\n",
    "        #print(probs.shape)\n",
    "        \n",
    "        #convert from soft max to hard max\n",
    "        max_vals = np.max(probs, axis = -1, keepdims = True)\n",
    "        hard_max = np.zeros(probs.shape)\n",
    "        hard_max[probs == max_vals] = 1\n",
    "        print(hard_max.shape)\n",
    "        \n",
    "        return(hard_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM-2-comboseqs-dropout0.5-manytomany-01-0.84.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-02-0.84.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-03-0.83.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-04-0.83.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-05-0.90.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-06-0.89.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-07-0.94.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-08-0.94.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-09-0.93.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-10-0.90.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-11-0.89.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-12-0.89.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-13-0.89.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-14-0.90.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-15-0.88.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-16-0.89.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-17-0.88.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-18-0.86.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-19-0.89.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-20-0.88.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n",
      "LSTM-2-comboseqs-dropout0.5-manytomany-21-0.89.hdf5\n",
      "(12107, 308, 5)\n",
      "(12107, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(12269, 308, 5)\n",
      "(77510, 308, 5)\n",
      "(77510, 308, 5)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from accuracy_calculations import calcError\n",
    "model = Model(encoder_inputs, output)\n",
    "pred_errs_base_train = []\n",
    "pred_errs_base_val = []\n",
    "pred_errs_base_test = []\n",
    "\n",
    "pred_errs_seq_train = []\n",
    "pred_errs_seq_val = []\n",
    "pred_errs_seq_test = []\n",
    "for file in os.listdir(\"seqWeights/final/\"):\n",
    "    print(file)\n",
    "    if file.endswith(\".hdf5\"):\n",
    "      \n",
    "        err_test = calcError(y_predicted = getPredicted(file, X_test), \n",
    "                                        y_true = Y_test, X_data = X_test, verbose = False)\n",
    "        err_val = calcError(y_predicted = getPredicted(file, X_val), \n",
    "                                        y_true = Y_val, X_data = X_val, verbose = False)\n",
    "        err_train = calcError(y_predicted = getPredicted(file, X_train), \n",
    "                                        y_true = Y_train, X_data = X_train, verbose = False)\n",
    "        pred_errs_base_test.append(err_test['perBase'])\n",
    "        pred_errs_seq_test.append(err_test['perSeq'])\n",
    "        \n",
    "        pred_errs_base_val.append(err_val['perBase'])\n",
    "        pred_errs_seq_val.append(err_val['perSeq'])\n",
    "        \n",
    "        pred_errs_base_train.append(err_train['perBase'])\n",
    "        pred_errs_seq_train.append(err_train['perSeq'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_errs_train_file = open('pred_errs_base_train.txt', 'w')\n",
    "pred_errs_train_file.write(str(pred_errs_base_train))\n",
    "pred_errs_train_file.close()\n",
    "\n",
    "pred_errs_val_file = open('pred_errs_base_val.txt', 'w')\n",
    "pred_errs_val_file.write(str(pred_errs_base_val))\n",
    "pred_errs_val_file.close()\n",
    "\n",
    "\n",
    "pred_errs_test_file = open('pred_errs_base_test.txt', 'w')\n",
    "pred_errs_test_file.write(str(pred_errs_base_test))\n",
    "pred_errs_test_file.close()\n",
    "\n",
    "pred_errs_train_file = open('pred_errs_seq_train.txt', 'w')\n",
    "pred_errs_train_file.write(str(pred_errs_seq_train))\n",
    "pred_errs_train_file.close()\n",
    "\n",
    "pred_errs_val_file = open('pred_errs_seq_val.txt', 'w')\n",
    "pred_errs_val_file.write(str(pred_errs_seq_val))\n",
    "pred_errs_val_file.close()\n",
    "\n",
    "\n",
    "pred_errs_test_file = open('pred_errs_seq_test.txt', 'w')\n",
    "pred_errs_test_file.write(str(pred_errs_seq_test))\n",
    "pred_errs_test_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_errs_train_file = open('pred_errs_base_train.txt', 'r')\n",
    "\n",
    "pred_errs_val_file = open('pred_errs_base_val.txt', 'r')\n",
    "\n",
    "pred_errs_test_file = open('pred_errs_base_test.txt', 'r')\n",
    "\n",
    "#pred_errs_train_file = open('pred_errs_seq_train.txt', 'r')\n",
    "#pred_errs_val_file = open('pred_errs_seq_val.txt', 'r')\n",
    "#pred_errs_test_file = open('pred_errs_seq_test.txt', 'r')\n",
    "\n",
    "def format_str(s):\n",
    "    s = s[1:(len(s)-1)]\n",
    "    s = s.split(',')\n",
    "    s = np.squeeze(s)\n",
    "    #s = np.flip(s, axis = 0)\n",
    "    s = np.delete(s, 0).astype(np.float)\n",
    "    return(s)\n",
    "    \n",
    "train_err =  format_str(pred_errs_train_file.read())\n",
    "#np.delete(np.flip(np.squeeze(pred_errs_train_file.read().split(',')), axis = 0), 0).astype(np.float)\n",
    "val_err =  format_str(pred_errs_val_file.read())\n",
    "test_err =  format_str(pred_errs_test_file.read())\n",
    "pred_errs_train_file.close()\n",
    "pred_errs_val_file.close()\n",
    "pred_errs_test_file.close()\n",
    "\n",
    "plt.plot(range(len(train_err)), train_err, label=\"train err per base\")\n",
    "plt.plot(range(len(val_err)), val_err, label=\"val err per base\")\n",
    "#plt.plot(range(len(test_err)), val_err, label=\"val err per base\")\n",
    "\n",
    "#plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "#plt.plot(self.x, self.val_acc, label = \"val_acc\")\n",
    "plt.legend()\n",
    "plt.show();\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 308, 5)\n",
      "(100, 308, 5)\n",
      "[[ 0.  4.  9.  2.  0.]\n",
      " [ 1.  0.  1. 15.  0.]\n",
      " [10.  1.  0.  9.  3.]\n",
      " [ 7. 20.  1.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"seqWeights/LSTM-2-comboseqs-dropout0.5-manytomany-08-0.94.hdf5\")\n",
    "errors = calcError(y_predicted = getPredicted(file, X_test[0:100, :, :]), \n",
    "                                        y_true = Y_test[0:100, :, :], X_data = X_test[0:100, :, :], verbose = False)\n",
    "\n",
    "print(errors['conf_mat']['good'])\n",
    "print(errors['conf_mat']['bad'])\n",
    "print(errors['conf_mat']['fail'])\n",
    "print(errors['perSeqErr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_red(seq, to_red):\n",
    "    if(len(to_red) == 0):\n",
    "        return(''.join(seq))\n",
    "    else:\n",
    "        start = 0\n",
    "        final_seq = ''\n",
    "        for i in to_red:\n",
    "            final_seq += ''.join(seq[start:i]) + '\\x1b[31m' + str(seq[i]) + '\\x1b[0m' \n",
    "            start = i + 1\n",
    "        final_seq += ''.join(seq[start:len(seq)])\n",
    "        return(final_seq)\n",
    "    \n",
    "def add_green(seq, to_green):\n",
    "    if len(to_green) == 0:\n",
    "        return(''.join(seq))\n",
    "    else:\n",
    "        start = 0\n",
    "        final_seq = ''\n",
    "        for i in to_green:\n",
    "            final_seq += ''.join(seq[start:i]) + '\\x1b[32m' + str(seq[i]) + '\\x1b[0m' \n",
    "            start = i + 1\n",
    "        final_seq += ''.join(seq[start:len(seq)])\n",
    "        return(final_seq)\n",
    "    \n",
    "def add_blue(seq, to_blue):\n",
    "    if len(to_blue) == 0:\n",
    "        return(''.join(seq))\n",
    "    else:\n",
    "        start = 0\n",
    "        final_seq = ''\n",
    "        for i in to_blue:\n",
    "            final_seq += ''.join(seq[start:i]) + '\\x1b[34m' + str(seq[i]) + '\\x1b[0m' \n",
    "            start = i + 1\n",
    "        final_seq += ''.join(seq[start:len(seq)])\n",
    "        return(final_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red: Should have stayed the same but didnt (bad)[11]\n",
      "Green: Should have changed and did (good)[]\n",
      "Blue: Should have changed but didnt (ok)[]\n",
      "T ACGGAGGGTGC\u001b[31mG\u001b[0mAGCGTTGTCCGGAATCACTGGGCGTAAAGGGCGCGTAGGCGGTCTGCTAAGCGTGTGGTGAAAGCTCGGGGCTCAACCCCGAGTCTGCCATGCGAACTGGTGGACTAGAGCACTGTAGAGGCAGGTGGAATTCCGGGTGTAGCGGTGGAATGCGTAGAGATCCGGAAGAACACCAGTGGCGAAGGCGG\n",
      "I ACGGAGGGTGC\u001b[31mG\u001b[0mAGCGTTGTCCGGAATCACTGGGCGTAAAGGGCGCGTAGGCGGTCTGCTAAGCGTGTGGTGAAAGCTCGGGGCTCAACCCCGAGTCTGCCATGCGAACTGGTGGACTAGAGCACTGTAGAGGCAGGTGGAATTCCGGGTGTAGCGGTGGAATGCGTAGAGATCCGGAAGAACACCAGTGGCGAAGGCGG\n",
      "P ACGGAGGGTGC\u001b[31mA\u001b[0mAGCGTTGTCCGGAATCACTGGGCGTAAAGGGCGCGTAGGCGGTCTGCTAAGCGTGTGGTGAAAGCTCGGGGCTCAACCCCGAGTCTGCCATGCGAACTGGTGGACTAGAGCACTGTAGAGGCAGGTGGAATTCCGGGTGTAGCGGTGGAATGCGTAGAGATCCGGAAGAACACCAGTGGCGAAGGCGG\n",
      "\n",
      "\n",
      "Red: Should have stayed the same but didnt (bad)[]\n",
      "Green: Should have changed and did (good)[]\n",
      "Blue: Should have changed but didnt (ok)[ 71 155]\n",
      "T ACAGGGGTGGCAAGCGTTGTCCGGATTTACTGGGTGTAAAGGGTGCGCAGGCGGATTCATAAGTCGGGGGT\u001b[34mT\u001b[0mAAATCCATGTGCTTAACACATGCAAGGCTTCCGATACTGTGAGTCTAGAGTCTCGAAGAGGAAGATGGAATTTCCGGTGTAAC\u001b[34mG\u001b[0mGTGGAATGTGTAGATATCGGAAAGAACACCAGTGGCGAAGGCAG\n",
      "I ACAGGGGTGGCAAGCGTTGTCCGGATTTACTGGGTGTAAAGGGTGCGCAGGCGGATTCATAAGTCGGGGGT\u001b[34mC\u001b[0mAAATCCATGTGCTTAACACATGCAAGGCTTCCGATACTGTGAGTCTAGAGTCTCGAAGAGGAAGATGGAATTTCCGGTGTAAC\u001b[34mA\u001b[0mGTGGAATGTGTAGATATCGGAAAGAACACCAGTGGCGAAGGCAG\n",
      "P ACAGGGGTGGCAAGCGTTGTCCGGATTTACTGGGTGTAAAGGGTGCGCAGGCGGATTCATAAGTCGGGGGT\u001b[34mC\u001b[0mAAATCCATGTGCTTAACACATGCAAGGCTTCCGATACTGTGAGTCTAGAGTCTCGAAGAGGAAGATGGAATTTCCGGTGTAAC\u001b[34mA\u001b[0mGTGGAATGTGTAGATATCGGAAAGAACACCAGTGGCGAAGGCAG\n",
      "\n",
      "\n",
      "Red: Should have stayed the same but didnt (bad)[]\n",
      "Green: Should have changed and did (good)[ 11  54  56  57  79  91  96 101 112 116 119 122 136 138 171]\n",
      "Blue: Should have changed but didnt (ok)[]\n",
      "T ACGGAGGGTGC\u001b[32mA\u001b[0mAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTG\u001b[32mT\u001b[0mT\u001b[32mA\u001b[0m\u001b[32mT\u001b[0mGTAAGTCAGGGGTGAAAGCCC\u001b[32mA\u001b[0mCGGCTCAACCG\u001b[32mT\u001b[0mGGAA\u001b[32mC\u001b[0mTGCC\u001b[32mC\u001b[0mTTGATACTGC\u001b[32mA\u001b[0mCGA\u001b[32m-\u001b[0mCT\u001b[32mC\u001b[0mGA\u001b[32mA\u001b[0mTCCGGGAGAGGGT\u001b[32mG\u001b[0mG\u001b[32mC\u001b[0mGGAATTCCAGGTGTAGGAGTGAAATCCGTAGA\u001b[32mT\u001b[0mATCTGGAGGAACATCAGTGGCGAAGGCG\n",
      "I ACGGAGGGTGC\u001b[32mG\u001b[0mAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTG\u001b[32mC\u001b[0mT\u001b[32mT\u001b[0m\u001b[32mG\u001b[0mGTAAGTCAGGGGTGAAAGCCC\u001b[32mG\u001b[0mCGGCTCAACCG\u001b[32mC\u001b[0mGGAA\u001b[32mT\u001b[0mTGCC\u001b[32mT\u001b[0mTTGATACTGC\u001b[32m-\u001b[0mCGA\u001b[32mG\u001b[0mCT\u001b[32mA\u001b[0mGA\u001b[32mG\u001b[0mTCCGGGAGAGGGT\u001b[32mA\u001b[0mG\u001b[32mT\u001b[0mGGAATTCCAGGTGTAGGAGTGAAATCCGTAGA\u001b[32mG\u001b[0mATCTGGAGGAACATCAGTGGCGAAGGCG\n",
      "P ACGGAGGGTGC\u001b[32mA\u001b[0mAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTG\u001b[32mT\u001b[0mT\u001b[32mA\u001b[0m\u001b[32mT\u001b[0mGTAAGTCAGGGGTGAAAGCCC\u001b[32mA\u001b[0mCGGCTCAACCG\u001b[32mT\u001b[0mGGAA\u001b[32mC\u001b[0mTGCC\u001b[32mC\u001b[0mTTGATACTGC\u001b[32mA\u001b[0mCGA\u001b[32m-\u001b[0mCT\u001b[32mC\u001b[0mGA\u001b[32mA\u001b[0mTCCGGGAGAGGGT\u001b[32mG\u001b[0mG\u001b[32mC\u001b[0mGGAATTCCAGGTGTAGGAGTGAAATCCGTAGA\u001b[32mT\u001b[0mATCTGGAGGAACATCAGTGGCGAAGGCG\n",
      "\n",
      "\n",
      "Red: Should have stayed the same but didnt (bad)[]\n",
      "Green: Should have changed and did (good)[ 11  37  54  56  57  79  91  96 101 112 116 119 122 136 138 171]\n",
      "Blue: Should have changed but didnt (ok)[]\n",
      "T ACGGAGGGTGC\u001b[32mA\u001b[0mAGCGTTAATCGGAATCACTGGGCGT\u001b[32mA\u001b[0mAAGCGCACGTAGGCTG\u001b[32mT\u001b[0mT\u001b[32mA\u001b[0m\u001b[32mT\u001b[0mGTAAGTCAGGGGTGAAAGCCC\u001b[32mA\u001b[0mCGGCTCAACCG\u001b[32mT\u001b[0mGGAA\u001b[32mC\u001b[0mTGCC\u001b[32mC\u001b[0mTTGATACTGC\u001b[32mA\u001b[0mCGA\u001b[32m-\u001b[0mCT\u001b[32mC\u001b[0mGA\u001b[32mA\u001b[0mTCCGGGAGAGGGT\u001b[32mG\u001b[0mG\u001b[32mC\u001b[0mGGAATTCCAGGTGTAGGAGTGAAATCCGTAGA\u001b[32mT\u001b[0mATCTGGAGGAACATCAGTGGCGAAGGCG\n",
      "I ACGGAGGGTGC\u001b[32mG\u001b[0mAGCGTTAATCGGAATCACTGGGCGT\u001b[32m-\u001b[0mAAGCGCACGTAGGCTG\u001b[32mC\u001b[0mT\u001b[32mT\u001b[0m\u001b[32mG\u001b[0mGTAAGTCAGGGGTGAAAGCCC\u001b[32mG\u001b[0mCGGCTCAACCG\u001b[32mC\u001b[0mGGAA\u001b[32mT\u001b[0mTGCC\u001b[32mT\u001b[0mTTGATACTGC\u001b[32m-\u001b[0mCGA\u001b[32mG\u001b[0mCT\u001b[32mA\u001b[0mGA\u001b[32mG\u001b[0mTCCGGGAGAGGGT\u001b[32mA\u001b[0mG\u001b[32mT\u001b[0mGGAATTCCAGGTGTAGGAGTGAAATCCGTAGA\u001b[32mG\u001b[0mATCTGGAGGAACATCAGTGGCGAAGGCG\n",
      "P ACGGAGGGTGC\u001b[32mA\u001b[0mAGCGTTAATCGGAATCACTGGGCGT\u001b[32mA\u001b[0mAAGCGCACGTAGGCTG\u001b[32mT\u001b[0mT\u001b[32mA\u001b[0m\u001b[32mT\u001b[0mGTAAGTCAGGGGTGAAAGCCC\u001b[32mA\u001b[0mCGGCTCAACCG\u001b[32mT\u001b[0mGGAA\u001b[32mC\u001b[0mTGCC\u001b[32mC\u001b[0mTTGATACTGC\u001b[32mA\u001b[0mCGA\u001b[32m-\u001b[0mCT\u001b[32mC\u001b[0mGA\u001b[32mA\u001b[0mTCCGGGAGAGGGT\u001b[32mG\u001b[0mG\u001b[32mC\u001b[0mGGAATTCCAGGTGTAGGAGTGAAATCCGTAGA\u001b[32mT\u001b[0mATCTGGAGGAACATCAGTGGCGAAGGCG\n",
      "\n",
      "\n",
      "Red: Should have stayed the same but didnt (bad)[]\n",
      "Green: Should have changed and did (good)[]\n",
      "Blue: Should have changed but didnt (ok)[55]\n",
      "T ACGTAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGGGTGCGCAGGCGGT\u001b[34mC\u001b[0mTTGCAAGTCAGATGTGAAAGCCCCGGGCTTAACCTGGGAATTGCGTTTGAAACTACAAGGCTAGAGTGCAGCAGAGGGGAGTGGAATTCCATGTGTAGCAGTGAAATGCGTAGAGATGTGGAAGAACACCGATGGCGAAGGCAG\n",
      "I ACGTAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGGGTGCGCAGGCGGT\u001b[34mT\u001b[0mTTGCAAGTCAGATGTGAAAGCCCCGGGCTTAACCTGGGAATTGCGTTTGAAACTACAAGGCTAGAGTGCAGCAGAGGGGAGTGGAATTCCATGTGTAGCAGTGAAATGCGTAGAGATGTGGAAGAACACCGATGGCGAAGGCAG\n",
      "P ACGTAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGGGTGCGCAGGCGGT\u001b[34mT\u001b[0mTTGCAAGTCAGATGTGAAAGCCCCGGGCTTAACCTGGGAATTGCGTTTGAAACTACAAGGCTAGAGTGCAGCAGAGGGGAGTGGAATTCCATGTGTAGCAGTGAAATGCGTAGAGATGTGGAAGAACACCGATGGCGAAGGCAG\n",
      "\n",
      "\n",
      "Red: Should have stayed the same but didnt (bad)[]\n",
      "Green: Should have changed and did (good)[37]\n",
      "Blue: Should have changed but didnt (ok)[]\n",
      "T ACGTAGGGTGCAAGCGTTAATCGGAATTACTGGGCGT\u001b[32mA\u001b[0mAAGCGTGCGCAGGCGGTTCGGAAAGAAAGATGTGAAATCCCAGGGCTTAACCTTGGAACTGCATTTTTAACTACCGGGCTAGAGTGTGTCAGAGGGAGGTGGAATTCCGCGTGTAGCAGTGAAATGCGTAGATATGCGGAGGAACACCGATGGCGAAGGCAG\n",
      "I ACGTAGGGTGCAAGCGTTAATCGGAATTACTGGGCGT\u001b[32m-\u001b[0mAAGCGTGCGCAGGCGGTTCGGAAAGAAAGATGTGAAATCCCAGGGCTTAACCTTGGAACTGCATTTTTAACTACCGGGCTAGAGTGTGTCAGAGGGAGGTGGAATTCCGCGTGTAGCAGTGAAATGCGTAGATATGCGGAGGAACACCGATGGCGAAGGCAG\n",
      "P ACGTAGGGTGCAAGCGTTAATCGGAATTACTGGGCGT\u001b[32mA\u001b[0mAAGCGTGCGCAGGCGGTTCGGAAAGAAAGATGTGAAATCCCAGGGCTTAACCTTGGAACTGCATTTTTAACTACCGGGCTAGAGTGTGTCAGAGGGAGGTGGAATTCCGCGTGTAGCAGTGAAATGCGTAGATATGCGGAGGAACACCGATGGCGAAGGCAG\n",
      "\n",
      "\n",
      "Red: Should have stayed the same but didnt (bad)[]\n",
      "Green: Should have changed and did (good)[112 123]\n",
      "Blue: Should have changed but didnt (ok)[ 56  57  58  63  65  67  68  75  85  96 100 101 103 105 111 113 118 121\n",
      " 182]\n",
      "T ACGTAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTT\u001b[34mC\u001b[0m\u001b[34mG\u001b[0m\u001b[34mC\u001b[0mTAAG\u001b[34mA\u001b[0mC\u001b[34mA\u001b[0mG\u001b[34mA\u001b[0m\u001b[34mT\u001b[0mGTGAAA\u001b[34mT\u001b[0mCCCCGGGCT\u001b[34mT\u001b[0mAACCTGGGAA\u001b[34mC\u001b[0mTGC\u001b[34mA\u001b[0m\u001b[34mT\u001b[0mT\u001b[34mT\u001b[0mG\u001b[34mT\u001b[0mGACTG\u001b[34mG\u001b[0m\u001b\u001b[34m[\u001b[0m32mC\u001b[34m\u001b\u001b[0m[0\u001b[34mm\u001b[0mGGGCTAGAGT\u001b[32mA\u001b[0mTGGCAGAGGGGGGTAGAATTCCACGTGTAGCAGTGAAATG\u001b[34mC\u001b[0mGTAGAGATGTGGAGGAATACCGATGGCGAAGGCAG\n",
      "I ACGTAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTT\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mG\u001b[0mTAAG\u001b[34mT\u001b[0mC\u001b[34mT\u001b[0mG\u001b[34mT\u001b[0m\u001b[34mC\u001b[0mGTGAAA\u001b[34mG\u001b[0mCCCCGGGCT\u001b[34mC\u001b[0mAACCTGGGAA\u001b[34mT\u001b[0mTGC\u001b[34mG\u001b[0m\u001b[34mA\u001b[0mT\u001b[34mG\u001b[0mG\u001b[34mA\u001b[0mGACTG\u001b[34mC\u001b[0m\u001b\u001b[34m[\u001b[0m32mA\u001b[34m\u001b\u001b[0m[0\u001b[34mm\u001b[0mAGGCTTGAAT\u001b[32mC\u001b[0mTGGCAGAGGGGGGTAGAATTCCACGTGTAGCAGTGAAATG\u001b[34mC\u001b[0mGTAGAGATGTGGAGGAACACCGATGGCGAAGGCAG\n",
      "P ACGTAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTT\u001b[34mT\u001b[0m\u001b[34mT\u001b[0m\u001b[34mG\u001b[0mTAAG\u001b[34mT\u001b[0mC\u001b[34mT\u001b[0mG\u001b[34mT\u001b[0m\u001b[34mC\u001b[0mGTGAAA\u001b[34mG\u001b[0mCCCCGGGCT\u001b[34mC\u001b[0mAACCTGGGAA\u001b[34mT\u001b[0mTGC\u001b[34mG\u001b[0m\u001b[34mA\u001b[0mT\u001b[34mG\u001b[0mG\u001b[34mA\u001b[0mGACTG\u001b[34mT\u001b[0m\u001b\u001b[34m[\u001b[0m32mC\u001b[34m\u001b\u001b[0m[0\u001b[34mm\u001b[0mAGGCTCGAAT\u001b[32mA\u001b[0mTGGCAGAGGGGGGTAGAATTCCACGTGTAGCAGTGAAATG\u001b[34mC\u001b[0mGTAGAGATGTGGAGGAACACCGATGGCGAAGGCAG\n",
      "\n",
      "\n",
      "Red: Should have stayed the same but didnt (bad)[]\n",
      "Green: Should have changed and did (good)[ 11  54  56  57  79  91  96 101 112 116 119 122 136 138 171]\n",
      "Blue: Should have changed but didnt (ok)[]\n",
      "T ACGGAGGGTGC\u001b[32mA\u001b[0mAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTG\u001b[32mT\u001b[0mT\u001b[32mA\u001b[0m\u001b[32mT\u001b[0mGTAAGTCAGGGGTGAAAGCCC\u001b[32mA\u001b[0mCGGCTCAACCG\u001b[32mT\u001b[0mGGAA\u001b[32mC\u001b[0mTGCC\u001b[32mC\u001b[0mTTGATACTGC\u001b[32mA\u001b[0mCGA\u001b[32m-\u001b[0mCT\u001b[32mC\u001b[0mGA\u001b[32mA\u001b[0mTCCGGGAGAGGGT\u001b[32mG\u001b[0mG\u001b[32mC\u001b[0mGGAATTCCAGGTGTAGGAGTGAAATCCGTAGA\u001b[32mT\u001b[0mATCTGGAGGAACATCAGTGGCGAAGGCG\n",
      "I ACGGAGGGTGC\u001b[32mG\u001b[0mAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTG\u001b[32mC\u001b[0mT\u001b[32mT\u001b[0m\u001b[32mG\u001b[0mGTAAGTCAGGGGTGAAAGCCC\u001b[32mG\u001b[0mCGGCTCAACCG\u001b[32mC\u001b[0mGGAA\u001b[32mT\u001b[0mTGCC\u001b[32mT\u001b[0mTTGATACTGC\u001b[32m-\u001b[0mCGA\u001b[32mG\u001b[0mCT\u001b[32mA\u001b[0mGA\u001b[32mG\u001b[0mTCCGGGAGAGGGT\u001b[32mA\u001b[0mG\u001b[32mT\u001b[0mGGAATTCCAGGTGTAGGAGTGAAATCCGTAGA\u001b[32mG\u001b[0mATCTGGAGGAACATCAGTGGCGAAGGCG\n",
      "P ACGGAGGGTGC\u001b[32mA\u001b[0mAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTG\u001b[32mT\u001b[0mT\u001b[32mA\u001b[0m\u001b[32mT\u001b[0mGTAAGTCAGGGGTGAAAGCCC\u001b[32mA\u001b[0mCGGCTCAACCG\u001b[32mT\u001b[0mGGAA\u001b[32mC\u001b[0mTGCC\u001b[32mC\u001b[0mTTGATACTGC\u001b[32mA\u001b[0mCGA\u001b[32m-\u001b[0mCT\u001b[32mC\u001b[0mGA\u001b[32mA\u001b[0mTCCGGGAGAGGGT\u001b[32mG\u001b[0mG\u001b[32mC\u001b[0mGGAATTCCAGGTGTAGGAGTGAAATCCGTAGA\u001b[32mT\u001b[0mATCTGGAGGAACATCAGTGGCGAAGGCG\n",
      "\n",
      "\n",
      "Average good changes per sequence : 0.49\n",
      "Average bad changes per sequence: 0.01\n",
      "Average failure to change per sequence: 0.22\n"
     ]
    }
   ],
   "source": [
    "#PRINT COLOR_CODED SEQUENCES\n",
    "\n",
    "bad_changes = 0\n",
    "fail_to_change = 0\n",
    "good_changes = 0\n",
    "\n",
    "numSeqs = 100\n",
    "for i in range(numSeqs):\n",
    "    #each of these are character arrays\n",
    "    target_char = output_dict[np.argmax(Y_test[i,0:seq_lengths[i]], axis = -1)]\n",
    "    test_char = output_dict[np.argmax(X_test[i,0:seq_lengths[i]], axis = -1)]\n",
    "    pred_char = output_dict[np.argmax(hard_max[i,0:seq_lengths[i]], axis = -1)]\n",
    "    \n",
    "    #subset if desired for visual appeal\n",
    "    subset = 200\n",
    "    target_char = target_char[0:subset]\n",
    "    test_char = test_char[0:subset]\n",
    "    pred_char = pred_char[0:subset]\n",
    "    \n",
    "    \n",
    "    to_red = np.where((target_char != pred_char) & (target_char == test_char))[0]\n",
    "    to_green = np.where((target_char != test_char) & (target_char == pred_char))[0]\n",
    "    to_blue = np.where((target_char != test_char) & (target_char != pred_char))[0]\n",
    "    \n",
    "    bad_changes += len(to_red)\n",
    "    good_changes += len(to_green)\n",
    "    fail_to_change += len(to_blue)\n",
    "    \n",
    "    if(len(to_red) != 0 or len(to_green) != 0 or len(to_blue) != 0):\n",
    "        print('Red: Should have stayed the same but didnt (bad)' + str(to_red))\n",
    "        print('Green: Should have changed and did (good)' + str(to_green))\n",
    "        print('Blue: Should have changed but didnt (ok)' + str(to_blue))\n",
    "\n",
    "\n",
    "        print(\"T \" + add_blue(add_green(add_red(target_char, to_red), to_green), to_blue))\n",
    "        print(\"I \" + add_blue(add_green(add_red(test_char, to_red), to_green), to_blue))\n",
    "        print(\"P \" + add_blue(add_green(add_red(pred_char, to_red), to_green), to_blue))\n",
    "        print('\\n')\n",
    "    \n",
    "print(\"Average good changes per sequence : \" + str(good_changes / numSeqs))\n",
    "print(\"Average bad changes per sequence: \" + str(bad_changes / numSeqs))\n",
    "print(\"Average failure to change per sequence: \" + str(fail_to_change / numSeqs))\n",
    "\n",
    "\n",
    "#Summary: for the 10000 test examples, there were an average of 3.2 bases that needed to be changed per sequence (considered noise).\n",
    "#Of these, the algorithm replaced 2.23 of them correctly, did not touch .97 of them, and replaced .01 of them incorrectly\n",
    "\n",
    "#This means there is quite a bit of structure to learn!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.02300887, 0.0239837 , 0.02327125, 0.02394429, 0.02493877,\n",
       "       0.0237598 , 0.02428735, 0.02509262, 0.02671051, 0.03129609,\n",
       "       0.02728344, 0.02937831, 0.04113571, 0.02778651, 0.0237339 ,\n",
       "       0.02448991, 0.01956574, 0.01730471, 0.01789672, 0.01841125,\n",
       "       0.01919418, 0.02003692, 0.02114091, 0.02227397, 0.02377455,\n",
       "       0.02539153, 0.0318067 , 0.05073694, 0.01310796, 0.03973856])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in file to re-create training loss curve\n",
    "file = open(\"log_loss.txt\", 'r')\n",
    "lines =  np.delete(np.flip(np.squeeze(file.read().split(',')), axis = 0), 0).astype(np.float)\n",
    "plt.plot(range(len(lines)), lines, label=\"loss\")\n",
    "#plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "#plt.plot(self.x, self.val_acc, label = \"val_acc\")\n",
    "plt.legend()\n",
    "plt.show();\n",
    "        \n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical distance metrics - has problems with different length sequences!\n",
    "matches_per_seq = np.sum(np.multiply(hard_max, target_seqs), axis = -1)\n",
    "print(matches_per_seq.shape)\n",
    "error_per_seq = np.mean(matches_per_seq, axis = 1)\n",
    "print(error_per_seq.shape)\n",
    "error = np.mean(error_per_seq)\n",
    "print(error)\n",
    "#On average, per example, we have a mismatch of 0.7 nucleotides between predicted and target output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline matrix multiplication\n",
    "matches_per_seq = np.sum(np.multiply(test_input_seqs, target_seqs), axis = -1)\n",
    "error_per_seq = np.mean(matches_per_seq, axis = 1)\n",
    "error = np.mean(error_per_seq)\n",
    "print(error)\n",
    "\n",
    "\n",
    "#TODO\n",
    "#convert back to character sequences and compare by eye\n",
    "#Get actual sequence length per sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(np.sum(target != test))\n",
    "#print(np.sum(target != pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate and cal errors\n",
    "indices = np.argmax(probs, axis = -1)\n",
    "output_dict = np.array(['A', 'T', 'C', 'G', '-', '\\t', '\\n'])\n",
    "\n",
    "error_baseline_sum = 0\n",
    "error_output_sum = 0\n",
    "#input_seqs = diff_entries_input + same_entries_input[1:50000]\n",
    "#output_seqs = diff_entries_output + same_entries_output[1:50000]\n",
    "\n",
    "for i in range(indices.shape[0]):\n",
    "    pred = ''.join(output_dict[indices[i,]])\n",
    "    inp = input_seqs[i]\n",
    "    out = output_seqs[i]\n",
    "    print(\"predicted: \" + pred)\n",
    "    print(\"Input: \" + inp)\n",
    "    print(\"Output: \" + out)\n",
    "    print('\\n')\n",
    "    \n",
    "    error_baseline = np.sum([1  for j in range(len(inp)) if inp[j] != out[j] ])\n",
    "    error_output = np.sum([1  for j in range(len(inp)) if pred[j] != out[j] ])\n",
    "    error_baseline_sum += error_baseline\n",
    "    error_output_sum += error_output\n",
    "                          \n",
    "    #print(\"Pred - Input: \" +  str(np.sum([1  for i in range(len(inp)) if pred[i] != inp[i] ] )))\n",
    "    #print(\"Pred - Output (ie We care): \" +  str(error_output ))\n",
    "    #print(\"Input - Output (ie Baseline): \" +  str(error_baseline ))\n",
    "    \n",
    "print(\"Baseline error (average noisy nucleotide per sequence): \" + str(error_baseline_sum / indices.shape[0]))\n",
    "print(\"Output error (average noisy nucleotide per sequence): \" + str(error_output_sum / indices.shape[0]))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save history and continue training\n",
    "with open(\"seqWeights/LSTM-manytomany.txt\", 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "model.load_weights(\"seqWeights/LSTM-manytomany-17-0.87.hdf5\")\n",
    "adam = keras.optimizers.Adam(lr = .001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(encoder_input_data[0:numExamples, :, :],\n",
    "          decoder_input_data[0:numExamples, :, :],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2, verbose = 1,\n",
    "         callbacks = [history, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epochs = 50\n",
    "numExamples = 10000\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#model.load_weights(\"seqWeights/LSTM7layer-05-0.82.hdf5\")\n",
    "adam = keras.optimizers.Adam(lr = .01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Run training\n",
    "history = History()\n",
    "filepath=\"LSTM7layer-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data[0:numExamples, :, :], decoder_input_data[0:numExamples, :, :]], decoder_target_data[0:numExamples, :, :],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2, verbose = 1,\n",
    "          callbacks = [history, checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"seqWeights/LSTM-manytomany-17-0.87.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_inf = Model(encoder_inputs, [e_h1, e_c1, e_h2, e_c2, e_h3, e_c3, e_h4, e_c4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Inference on new sequence\n",
    "\n",
    "d_input_states = [d_input_h1, d_input_c1, d_input_h2, d_input_c2, d_input_h3, d_input_c3, d_input_h4, d_input_c4]\n",
    "d_input_h1 = Input(shape = (latent_dim, ))\n",
    "d_input_c1 = Input(shape = (latent_dim, ))\n",
    "d_input_h2 = Input(shape = (latent_dim, ))\n",
    "d_input_c2 = Input(shape = (latent_dim, ))\n",
    "d_input_h3 = Input(shape = (latent_dim, ))\n",
    "d_input_c3 = Input(shape = (latent_dim, ))\n",
    "d_input_h4 = Input(shape = (latent_dim, ))\n",
    "d_input_c4 = Input(shape = (latent_dim, ))\n",
    "\n",
    "\n",
    "d_out_1, d_h1, d_c1 = d_lstm_1(decoder_inputs, initial_state = [d_input_h1, d_input_c1])\n",
    "d_out_2, d_h2, d_c2 = d_lstm_2(d_out_1, initial_state = [d_input_h2, d_input_c2])\n",
    "d_out_3, d_h3, d_c3 = d_lstm_3(d_out_2, initial_state = [d_input_h3, d_input_c3])\n",
    "d_out_4, d_h4, d_c4 = d_lstm_4(d_out_3, initial_state = [d_input_h4, d_input_c4])\n",
    "\n",
    "d_output_states = [d_h1, d_c1, d_h2, d_c2, d_h3, d_c3, d_h4, d_c4 ]\n",
    "\n",
    "decoder_out = d_dense(d_out_4)\n",
    "decoder_model_inf = Model(inputs = [decoder_inputs] + [d_input_h1, d_input_c1, d_input_h2, d_input_c2, d_input_h3, d_input_c3, d_input_h4, d_input_c4],\n",
    "                         outputs = [decoder_out] + [d_h1, d_c1, d_h2, d_c2, d_h3, d_c3, d_h4, d_c4 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_nuc = ['A', 'T', 'C', 'G', '-', '\\t', '\\n']\n",
    "def decode_seq(inp_seq):\n",
    "    print(inp_seq.shape)\n",
    "    states = encoder_model_inf.predict(inp_seq)\n",
    "    print(len(states))\n",
    "    target_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "    target_seq[0,0, one_hot_input['\\t']] = 1\n",
    "    \n",
    "    translated_seq = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        decoder_out, d_h1, d_c1, d_h2, d_c2, d_h3, d_c3, d_h4, d_c4  = decoder_model_inf.predict(x = [target_seq] + states)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0, -1, :])\n",
    "        sampled_nucleotide = index_to_nuc[max_val_index]\n",
    "        translated_seq += sampled_nucleotide\n",
    "        \n",
    "        if (sampled_nucleotide == '\\n'):\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "        target_seq[0,0, max_val_index] = 1\n",
    "        \n",
    "        states = [d_h1, d_c1, d_h2, d_c2, d_h3, d_c3, d_h4, d_c4]\n",
    "    return(translated_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = diff_entries_input\n",
    "output_seqs = diff_entries_output\n",
    "\n",
    "for i in range(10):\n",
    "    input_seq = np.array(np.expand_dims(encoder_input_data[i, :, :], axis = 0))\n",
    "    trans_seq = decode_seq(input_seq)\n",
    "    print(\"Original Sequence: \" + input_seqs[i])\n",
    "    print(\"Predicted Sequence: \" + trans_seq)\n",
    "    print(\"Target Sequence: \" + output_seqs[i])\n",
    "    \n",
    "#Metric can be standard confusion matrix: of the bases that should have been substituted, how many were substituted correctly (true positive)\n",
    "#of the bases that should not have been substituted, how many were not substituted, true negative\n",
    "# of the bases that should not have been substituted, how many were: false positive\n",
    "# of the bases that should have been substituted, how many were not: false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.load(open( \"history_lr0.001\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "history = pickle.load(open('history_lr' + str(lr), \"rb\"))\n",
    "def plot_loss(history):\n",
    "    epochs = len(history[\"loss\"])\n",
    "    plt.plot(range(0, epochs), history[\"loss\"])\n",
    "    plt.plot(range(0, epochs), history[\"val_loss\"])\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_new = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_train_model(model, history, model_save, history_save, lr = .01, epochs = 10, batch_size = 50):\n",
    "    \n",
    "    history_new = History()\n",
    "    \n",
    "    adam = keras.optimizers.Adam(lr = lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "    model.fit([encoder_input_data[0:numExamples, :, :], \n",
    "               decoder_input_data[0:numExamples, :, :]],\n",
    "               decoder_target_data[0:numExamples, :, :],\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_split=0.2, \n",
    "                verbose = 1,\n",
    "                callbacks = [history_new])\n",
    "\n",
    "    #update history\n",
    "    for k in history.keys():\n",
    "        history[k] = history[k] + history_new.history[k]\n",
    "    with open(history_save, 'wb') as file_pi:\n",
    "        pickle.dump(history, file_pi)\n",
    "        \n",
    "    #Save model\n",
    "    model.save(model_save)\n",
    "    return(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and continue training as desired\n",
    "lr = 0.001\n",
    "model = load_model('s2s_lr.01_lr.001.h5')\n",
    "history = pickle.load(open('history_lr.01_ly.001.h5', \"rb\"))\n",
    "\n",
    "model, history = continue_train_model(model = model, history = history, model_save = ('s2s_lr.01_lr.001.h5'), history_save = ('history_lr.01_ly.001.h5'), \n",
    "                                      lr = .001, epochs = 10, batch_size = 50)\n",
    " \n",
    "    \n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_multilayer(num_encoder_tokens, num_decoder_tokens):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = num_encoder_tokens, output_dim=162))\n",
    "    model.add(LSTM(units = 100, return_sequences=True, input_shape = (None, 162))) #None refers to unknown sequence length\n",
    "    model.add(LSTM(100, return_sequences = True))\n",
    "    model.add(LSTM(50, return_sequences = True))\n",
    "    model.add(Dense(162, activation = \"softmax\"))\n",
    "    adam = keras.optimizers.Adam(lr = .01, beta_1=0.9, beta_2 = 0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer= adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return(model)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#With more latent dimensions\n",
    "numExamples = 5000\n",
    "batch_size = 50\n",
    "epochs = 2\n",
    "\n",
    "model = define_model_multilayer(num_encoder_tokens = 7, num_decoder_tokens = 7)\n",
    "history = History()\n",
    "\n",
    "print(len(diff_entries_input[0]))\n",
    "print(encoder_input_data[1, 1, :])\n",
    "print(encoder_input_data[0:numExamples, :, :].shape)\n",
    "#model.fit(encoder_input_data[0:numExamples, :, :],\n",
    "#          decoder_target_data[0:numExamples, :, :],\n",
    "#          batch_size=batch_size,\n",
    "#          epochs=epochs,\n",
    "#          validation_split=0.1, \n",
    "#          verbose = 1,\n",
    "#          callbacks = [history])\n",
    "\n",
    "model.fit(np.array(diff_entries_input)[0:numExamples, ],\n",
    "          np.array(diff_entries_output)[0:numExamples, ],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1, \n",
    "          verbose = 1,\n",
    "          callbacks = [history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict(encoder_input_data[0:5, :, :])\n",
    "print(len(probs))\n",
    "probs = np.array(probs)\n",
    "print(probs.shape)\n",
    "indices = np.argmax(probs, axis = -1)\n",
    "\n",
    "output_dict = np.array(['A', 'T', 'C', 'G', '-', '\\t', '\\n'])\n",
    "for i in range(indices.shape[0]):\n",
    "    print(output_dict[indices[i,]])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
