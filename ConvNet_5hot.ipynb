{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from load_5hot import load\n",
    "from onehot_tostring import convert_to_nucs, show_noise\n",
    "from keras import layers\n",
    "from keras import callbacks\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Conv1D, Add, Reshape\n",
    "from keras.layers import AveragePooling2D, UpSampling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from keras.activations import softmax\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "101886 sequences were uploaded\n",
      "\n",
      "Maximum sequence length in is 308\n",
      "Maximum sequence length out is 308\n",
      "\n",
      "Converting to one-hot...\n",
      "Done\n",
      "Converting to one-hot...\n",
      "Done\n",
      "Converting to one-hot...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "[X_train,Y_train,X_test,Y_test,X_val,Y_val] = load(\"blast_tab_1hit.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 77510 training examples\n",
      "There are 12269 validation examples\n",
      "There are 12107 testing examples\n",
      "There are 5 classes: A, C, G, T, -\n",
      "The longest sequence is 308 nucleotides long\n",
      "X_train shape is:\n",
      "(77510, 5, 308, 1)\n"
     ]
    }
   ],
   "source": [
    "m = X_train.shape[0]\n",
    "print(\"There are \" + str(m) + \" training examples\")\n",
    "print(\"There are \" + str(X_val.shape[0]) + \" validation examples\")\n",
    "print(\"There are \" + str(X_test.shape[0]) + \" testing examples\")\n",
    "print(\"There are \" + str(X_train.shape[1]) + \" classes: A, C, G, T, -\")\n",
    "max_length = max(X_train.shape[2],X_test.shape[2])\n",
    "print(\"The longest sequence is \" + str(max_length) + \" nucleotides long\")\n",
    "print(\"X_train shape is:\")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Examples:\n",
      "\n",
      "ACGGAGGGTGCGAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGCTTGGTAAGTCAGGGGTGAAAGCCCGCGGCTCAACCGCGGAATTGCCTTTGATACTGC-CGAGCTAGAGTCCGGGAGAGGGTAGTGGAATTCCAGGTGTAGGAGTGAAATCCGTAGAGATCTGGAGGAACATCAGTGGCGAAGGCGACTACCTGGACCGGTACTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG--------------------------------------------------------, ACGGAGGGTGCAAGCGTTAATCGGAATCACTGGGCGTAAAGCGCACGTAGGCTGTTATGTAAGTCAGGGGTGAAAGCCCACGGCTCAACCGTGGAACTGCCCTTGATACTGCACGA-CTCGAATCCGGGAGAGGGTGGCGGAATTCCAGGTGTAGGAGTGAAATCCGTAGATATCTGGAGGAACATCAGTGGCGAAGGCGGCCACCTGGACCGGTATTGACGCTGAGGTGCGAAAGCGTGGGGAGCAAACAG--------------------------------------------------------\n",
      "\n",
      "ACAGAGGGGGCAAGCGTTGTCCGGAGTTACTGGGCGTAAAGGGCGCGCAGGCGGTGGGCTGCGTCGGCGCTGAAAGCGCCCCGCTTAACGGGGCGAGGCGCGCCGATACGAGTCCACTCGAGGCAAGCAGAGGGTGGCGGAATTCCGGGTGGAGTGGTGAAATGCGTAGAGATCCGGAGGAACGCCGGTGGGGAAGCCGGCCACCTGGGCTTGACCTGACGCTGCGGCGCGACAGCGTGGGGAGCAAACCG---------------------------------------------------------, ACAGAGGGGGCAAGCGTTGTCCGGAGTTACTGGGCGTAAAGGGCGCGCAGGCGGTGGGCTGCGTCGGCGCTGAAAGCGCCCCGCTTAACGGGGCGAGGCGCGCCGATACGAGTCCACTCGAGGCAAGCAGAGGGTGGCGGAATTCCGGGTGGAGCGGTGAAATGCGTAGAGATCCGGAGGAACGCCGGTGGGGAAGCCGGCCACCTGGGCTTGACCTGACGCTGCGGCGCGACAGCGTGGGGAGCAAACCG---------------------------------------------------------\n",
      "\n",
      "ACGTAGGTCCCGAACGTTGCGCGAATTTACTGGGCGTAAAGGGTCCGTAGGCGGTTTAGCAAGTGGTTGGTGAAATTTCACGGCTCAACCGTGAAACTGCCTTCCAAACTGCTAAACTTGAGGCAGGGAGAGGTCGGCGGAATTCCCGGTGTAGCGGTGAAATGCGTAGATATCGGGAGGAACACCAGTGGCGAAGGCGGCCGACTGGAACTGTTCTGACGCTGAGGGACGAAAGCTAGGGGAGCAAACCG---------------------------------------------------------, ACGTAGGTCCCGAACGTTGCGCGAATTTACTGGGCGTAAAGGGTCCGTAGGCGGTTTAGCAAGTGGTTGGTGAAATTTCACGGCTCAACCGTGAAACTGCCTTCCAAACTGCTAAACTTGAGGCAGGGAGAGGTCGGCGGAATTCCCGGTGTAGCGGTGAAATGCGTAGATATCGGGAGGAACACCAGTGGCGAAGGCGGCCGACTGGAACTGTCCTGACGCTGAGGGACGAAAGCTAGGGGAGCAAACCG---------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Validation Examples:\n",
      "\n",
      "ACAGGGGTGGCAAGCGTTGTCCGGATTTACTGGGTGTAAAGGGTGCGCAGGCGGATCGATAAGTCGGGGGTTAAATCCATGTGCTTAACACATGCACGGCTTCCGATACTGTTGATCTAGAGTCTCGAAGAGGAAGGTGGAATTTCCGGTGTAACGGTGGAATGTGTAGATATCGGAAAGAACACCAGTGGCGAAGGCAGCCTTCTGGTCGAGTACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG---------------------------------------------------------, ACAGGGGTGGCAAGCGTTGTCCGGATTTACTGGGTGTAAAGGGTGCGCAGGCGGATCGATAAGTCGGGGGTTAAATCCATGTGCTTAACACATGCACGGCTTCCGATACTGTTGATCTAGAGTCTCGAAGAGGAAGGTGGAATTTCCGGTGTAACGGTGGAATGTGTAGATATCGGAAAGAACACCAGTGGCGAAGGCAGCCTTCTGGTCGAGTACTGACGCTCAGGCACGAAAGCGTGGGGAGCAAACAG---------------------------------------------------------\n",
      "\n",
      "ACGTACCGTCCAAACGTTATTCGGTATCACTGGGCTTAAAGCGTTCGTAGGCGGCCTAGAAGGTGAGATGTGAAAGCCCACGGCTCAACCGTGGAATTGCGTTTCAAACCACTAGGCTTGAGGAAGACAGGGGTGATGGGAACTTATGGTGGAGCGGTGAAATGCGTTGATATCATAGGGAACACCGGTGGCGAAAGCGCATCACTGGGTCTTTTCTGACGCTGAGGAACGAAAGCTAGGGTAGCGAACGG---------------------------------------------------------, ACGTACCGTCCAAACGTTATTCGGTATCACTGGGCTTAAAGCGTTCGTAGGCGGCCTAGAAGGTGAGATGTGAAAGCCCACGGCTCAACCGTGGAATTGCGTTTCAAACCACTAGGCTTGAGGAAGACAGGGGTGATGGGAACTTATGGTGGAGCGGTGAAATGCGTTGATATCATAGGGAACACCGGTGGCGAAAGCGCATCACTGGGTCTTTTCTGACGCTGAGGAACGAAAGCTAGGGTAGCGAACGG---------------------------------------------------------\n",
      "\n",
      "ACGTAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTGTGTAAGACAGATGTGAAATCCCCGGGCTCAACCTGGGAACTGCATTTGTGACTGCACAGCTAGAGTACGGTAGAGGGGGATGGAATTCCGCGTGTAGCAGTGAAATGCGTAGATATGCGGAGGAACACCGATGGCGAAGGCAATCCCCTGGACCTGTACTGACGCTCATGCACGAAAGCGTGGGGAGCAAACAG---------------------------------------------------------, ACGTAGGGTGCAAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTTGTGTAAGACAGATGTGAAATCCCCGGGCTCAACCTGGGAACTGCATTTGTGACTGCACAGCTAGAGTACGGTAGAGGGGGATGGAATTCCGCGTGTAGCAGTGAAATGCGTAGATATGCGGAGGAACACCGATGGCGAAGGCAATCCCCTGGACCTGTACTGACGCTCATGCACGAAAGCGTGGGGAGCAAACAG---------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Testing Examples:\n",
      "\n",
      "ACGGAGGGCGCGAGCGTTACCCGGATTCACTGGGCGTAAAGGGCGTGTAGGCGGCCTGGGGCGTCCCATGTGAAAGACCACGGCTCAACCGTGGGGGAGCGTGGGATACGCTCAGGCTAGACGGTGGGAGAGGGTGGTGGAATTCCCGGAGTAGCGGTGAAATGCGCAGATACCGGGAGGAACGCCGATGGCGAAGGCAGCCACCTGGTCCACCCGTGACGCTGAGGCGCGAAAGCGTGGGGAGCAAACCG---------------------------------------------------------, ACGGAGGGCGCGAGCGTTACCCGGATTCACTGGGCGTAAAGGGCGTGTAGGCGGCCTGGGGCGTCCCATGTGAAAGACCACGGCTCAACCGTGGGGGAGCGTGGGATACGCTCAGGCTAGACGGTGGGAGAGGGTGGTGGAATTCCCGGAGTAGCGGTGAAATGCGCAGATACCGGGAGGAACGCCGATGGCGAAGGCAGCCACCTGGTCCACCCGTGACGCTGAGGCGCGAAAGCGTGGGGAGCAAACCG---------------------------------------------------------\n",
      "\n",
      "ACGTAGGTCCCGAACGTTGCGCGAAATTACTGGGCGTAAAGGGTCCGTAGGCGGTCTGGTAAGTGGAAGGTGAAAGCCCACGGCTCAACCGTGGAATTGCCTTCCAAACTGCTGGACTTGAGGGCGGAAGAGGTCGGCGGAATTCCCGGTGTAGCGGTGAAATGCGTAGATATCGGGAGGAACACCGGTGGCGAAGGCGGCCGACTGGGACGCACCTGACGCTGAGGGACGAAAGCCAGGGGAGCGAACCG---------------------------------------------------------, ACGTAGGTCCCGAACGTTGCGCGAAATTACTGGGCGTAAAGGGTCCGTAGGCGGTCTGGTAAGTGGAAGGTGAAAGCCCACGGCTCAACCGTGGAATTGCCTTCCAAACTGCTGGACTTGAGGGCGGAAGAGGTCGGCGGAATTCCCGGTGTAGCGGTGAAATGCGTAGATATCGGGAGGAACACCGGTGGCGAAGGCGGCCGACTGGGACGCACCTGACGCTGAGGGACGAAAGCCAGGGGAGCGAACCG---------------------------------------------------------\n",
      "\n",
      "ACGTAAGGGGCGAGCGTTGTTCGGAATTATTGGGCGTAAAGGGCATGTAGGCGGTTATGTAAGCCTGGTGTGAAATCCTGGGGCTTAACCCTAGAATAGCATTGGGTACTGTATAACTTGAATTACGGAAGGGAAACTGGAATTCCAAGTGTAGGGGTGGAATCTGTAGATATTTGGAAGAACACCGGTGGCGAAGGCGGGTTTCTGGCCGATAATTGACGCTGAGATGCGAAAGTGTGGGGATCGAACAG---------------------------------------------------------, ACGTAAGGGGCGAGCGTTGTTCGGAATTATTGGGCGTAAAGGGCATGTAGGCGGTTATGTAAGCCTGGTGTGAAATCCTGGGGCTTAACCCTAGAATAGCATTGGGTACTGTATAACTTGAATTACGGAAGGGAAACTGGAATTCCAAGTGTAGGGGTGGAATCTGTAGATATTTGGAAGAACACCGGTGGCGAAGGCGGGTTTCTGGCCGATAATTGACGCTGAGATGCGAAAGTGTGGGGATCGAACAG---------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize data sets to ensure they appear as anticipated\n",
    "def display_example_sequences(X,Y,n):\n",
    "    for sample in range(n):\n",
    "        samp_n_X = convert_to_nucs(X[sample,:,:,0])\n",
    "        samp_n_Y = convert_to_nucs(Y[sample,:,:,0])\n",
    "        print(samp_n_X + ', ' + samp_n_Y + '\\n')\n",
    "\n",
    "    print('')\n",
    "    \n",
    "n_examples = 3\n",
    "print('\\nTraining Examples:\\n')\n",
    "display_example_sequences(X_train,Y_train,n_examples)\n",
    "print('\\nValidation Examples:\\n')\n",
    "display_example_sequences(X_val,Y_val,n_examples)\n",
    "print('\\nTesting Examples:\\n')\n",
    "display_example_sequences(X_test,Y_test,n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permuting training samples...\n",
      "finished X\n",
      "finished Y\n"
     ]
    }
   ],
   "source": [
    "print('Permuting training samples...')\n",
    "np.random.seed(0)\n",
    "rand_perm = np.random.rand(m).argsort()\n",
    "np.take(X_train,rand_perm,axis=0,out=X_train)\n",
    "print(\"finished X\")\n",
    "np.take(Y_train,rand_perm,axis=0,out=Y_train)\n",
    "print(\"finished Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf = 16\n",
    "input_nc = 1\n",
    "output_nc = 1\n",
    "n_blocks_gen = 9\n",
    "\n",
    "def softMaxAxis1(x):\n",
    "    return softmax(x,axis=1)\n",
    "\n",
    "# started from: https://blog.sicara.com/keras-generative-adversarial-networks-image-deblurring-45e3ab6977b5\n",
    "def Model_1(input_shape):\n",
    "    \"\"\"Build generator architecture.\"\"\"\n",
    "    # Current version : ResNet block\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    # X = ZeroPadding2D((0, 3))(X_input) # allows 'valid' in conv0 to keep length and collapse one-hot\n",
    "\n",
    "    X = Conv2D(128, (4, 7), strides = (1, 1), padding = 'same', name = 'conv0')(X_input)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(64, (1, 7), strides = (1, 1), padding = 'same', name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(64, (1, 7), strides = (1, 1), padding = 'same', name = 'conv2')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn2')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Dropout(0.3, name = 'dropout')(X)\n",
    "   \n",
    "    X = Conv2D(32, (1, 7), strides = (1, 1), padding = 'same', name = 'conv3')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn3')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(1, (1, 1), strides = (1, 1), padding = 'same', name = 'conv4')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn4')(X)\n",
    "    X = Dense(1, activation = softMaxAxis1)(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X, name='Model_2')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 5, 308, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 5, 308, 128)       3712      \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 5, 308, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5, 308, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 5, 308, 64)        57408     \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 5, 308, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 5, 308, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 5, 308, 64)        28736     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 5, 308, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 5, 308, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5, 308, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 5, 308, 32)        14368     \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (None, 5, 308, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5, 308, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 5, 308, 1)         33        \n",
      "_________________________________________________________________\n",
      "bn4 (BatchNormalization)     (None, 5, 308, 1)         4         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5, 308, 1)         2         \n",
      "=================================================================\n",
      "Total params: 105,415\n",
      "Trainable params: 104,837\n",
      "Non-trainable params: 578\n",
      "_________________________________________________________________\n",
      "None\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "myModel = Model_1((5,max_length,1))\n",
    "print(myModel.summary())\n",
    "print('Done!')\n",
    "logging = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "myModel.compile(optimizer=\"Adam\", loss=\"mean_squared_error\", metrics = [\"accuracy\"])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy of Train if predicting output = input is 0.994588750173836\n",
      "Baseline accuracy of Val if predicting output = input is 0.999137833394904\n",
      "Baseline accuracy of Test if predicting output = input is 0.9990534616123119\n"
     ]
    }
   ],
   "source": [
    "# Understand baseline accuracy of if model output its own input\n",
    "diffs = np.absolute(X_train-Y_train);\n",
    "err = np.sum(diffs)/np.ma.size(X_train);\n",
    "print(\"Baseline accuracy of Train if predicting output = input is \" + str(1-err))\n",
    "\n",
    "diffs = np.absolute(X_val-Y_val);\n",
    "err = np.sum(diffs)/np.ma.size(X_val);\n",
    "print(\"Baseline accuracy of Val if predicting output = input is \" + str(1-err))\n",
    "\n",
    "diffs = np.absolute(X_test-Y_test);\n",
    "err = np.sum(diffs)/np.ma.size(X_test);\n",
    "print(\"Baseline accuracy of Test if predicting output = input is \" + str(1-err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77510 samples, validate on 12269 samples\n",
      "Epoch 1/8\n",
      " 2000/77510 [..............................] - ETA: 1:06:04 - loss: 0.0372 - acc: 0.9884\n",
      "step 100: loss = 0.029664497822523117, acc = 0.9922402501106262\n",
      " 4000/77510 [>.............................] - ETA: 1:02:55 - loss: 0.0302 - acc: 0.9922\n",
      "step 200: loss = 0.019517946988344193, acc = 0.9968830943107605\n",
      " 6000/77510 [=>............................] - ETA: 59:34 - loss: 0.0258 - acc: 0.9939\n",
      "step 300: loss = 0.01615731418132782, acc = 0.9961038827896118\n",
      " 8000/77510 [==>...........................] - ETA: 57:02 - loss: 0.0227 - acc: 0.9949\n",
      "step 400: loss = 0.012312302365899086, acc = 0.9983766078948975\n",
      "10000/77510 [==>...........................] - ETA: 54:58 - loss: 0.0206 - acc: 0.9955\n",
      "step 500: loss = 0.012428302317857742, acc = 0.9968830943107605\n",
      "12000/77510 [===>..........................] - ETA: 53:02 - loss: 0.0190 - acc: 0.9959\n",
      "step 600: loss = 0.011275513097643852, acc = 0.9975324869155884\n",
      "14000/77510 [====>.........................] - ETA: 51:11 - loss: 0.0178 - acc: 0.9962\n",
      "step 700: loss = 0.010557237081229687, acc = 0.9977272748947144\n",
      "16000/77510 [=====>........................] - ETA: 49:26 - loss: 0.0169 - acc: 0.9964\n",
      "step 800: loss = 0.011845425702631474, acc = 0.9962337613105774\n",
      "18000/77510 [=====>........................] - ETA: 47:41 - loss: 0.0161 - acc: 0.9966\n",
      "step 900: loss = 0.009638044983148575, acc = 0.9984415769577026\n",
      "20000/77510 [======>.......................] - ETA: 45:57 - loss: 0.0155 - acc: 0.9968\n",
      "step 1000: loss = 0.009717423468828201, acc = 0.9981818199157715\n",
      "22000/77510 [=======>......................] - ETA: 44:16 - loss: 0.0150 - acc: 0.9969\n",
      "step 1100: loss = 0.00872180424630642, acc = 0.9991558194160461\n",
      "24000/77510 [========>.....................] - ETA: 42:37 - loss: 0.0145 - acc: 0.9970\n",
      "step 1200: loss = 0.009544292464852333, acc = 0.9981493353843689\n",
      "26000/77510 [=========>....................] - ETA: 41:08 - loss: 0.0141 - acc: 0.9971\n",
      "step 1300: loss = 0.008758032694458961, acc = 0.9988636374473572\n",
      "28000/77510 [=========>....................] - ETA: 40:23 - loss: 0.0138 - acc: 0.9972\n",
      "step 1400: loss = 0.00883536133915186, acc = 0.9988311529159546\n",
      "30000/77510 [==========>...................] - ETA: 39:31 - loss: 0.0135 - acc: 0.9973\n",
      "step 1500: loss = 0.008675731718540192, acc = 0.9989935159683228\n",
      "32000/77510 [===========>..................] - ETA: 38:50 - loss: 0.0132 - acc: 0.9973\n",
      "step 1600: loss = 0.008694238029420376, acc = 0.9989610314369202\n",
      "34000/77510 [============>.................] - ETA: 38:04 - loss: 0.0130 - acc: 0.9974\n",
      "step 1700: loss = 0.011157575063407421, acc = 0.9962337613105774\n",
      "36000/77510 [============>.................] - ETA: 36:49 - loss: 0.0128 - acc: 0.9974\n",
      "step 1800: loss = 0.008319812826812267, acc = 0.9992856979370117\n",
      "38000/77510 [=============>................] - ETA: 35:18 - loss: 0.0126 - acc: 0.9975\n",
      "step 1900: loss = 0.008554425090551376, acc = 0.9990260004997253\n",
      "40000/77510 [==============>...............] - ETA: 33:22 - loss: 0.0124 - acc: 0.9975\n",
      "step 2000: loss = 0.008916047401726246, acc = 0.9986363649368286\n",
      "42000/77510 [===============>..............] - ETA: 31:28 - loss: 0.0123 - acc: 0.9975\n",
      "step 2100: loss = 0.011921331286430359, acc = 0.9952921867370605\n",
      "44000/77510 [================>.............] - ETA: 29:36 - loss: 0.0121 - acc: 0.9976\n",
      "step 2200: loss = 0.01017255149781704, acc = 0.9972402453422546\n",
      "46000/77510 [================>.............] - ETA: 27:48 - loss: 0.0120 - acc: 0.9976\n",
      "step 2300: loss = 0.009303705766797066, acc = 0.9981818199157715\n",
      "48000/77510 [=================>............] - ETA: 25:56 - loss: 0.0119 - acc: 0.9977\n",
      "step 2400: loss = 0.008768846280872822, acc = 0.9990584254264832\n",
      "50000/77510 [==================>...........] - ETA: 24:05 - loss: 0.0118 - acc: 0.9977\n",
      "step 2500: loss = 0.009351108223199844, acc = 0.9980844259262085\n",
      "52000/77510 [===================>..........] - ETA: 22:15 - loss: 0.0117 - acc: 0.9977\n",
      "step 2600: loss = 0.00845047552138567, acc = 0.9990260004997253\n",
      "54000/77510 [===================>..........] - ETA: 20:27 - loss: 0.0116 - acc: 0.9977\n",
      "step 2700: loss = 0.009504863992333412, acc = 0.9979220628738403\n",
      "56000/77510 [====================>.........] - ETA: 18:39 - loss: 0.0115 - acc: 0.9978\n",
      "step 2800: loss = 0.009091119281947613, acc = 0.9984415769577026\n",
      "58000/77510 [=====================>........] - ETA: 16:52 - loss: 0.0114 - acc: 0.9978\n",
      "step 2900: loss = 0.009669800288975239, acc = 0.9977921843528748\n",
      "60000/77510 [======================>.......] - ETA: 15:06 - loss: 0.0113 - acc: 0.9978\n",
      "step 3000: loss = 0.00932985544204712, acc = 0.9980844259262085\n",
      "62000/77510 [======================>.......] - ETA: 13:20 - loss: 0.0112 - acc: 0.9978\n",
      "step 3100: loss = 0.007984459400177002, acc = 0.9995454549789429\n",
      "64000/77510 [=======================>......] - ETA: 11:35 - loss: 0.0111 - acc: 0.9979\n",
      "step 3200: loss = 0.00939914770424366, acc = 0.9979870319366455\n",
      "66000/77510 [========================>.....] - ETA: 9:51 - loss: 0.0111 - acc: 0.9979\n",
      "step 3300: loss = 0.009256607852876186, acc = 0.9979545474052429\n",
      "68000/77510 [=========================>....] - ETA: 8:07 - loss: 0.0110 - acc: 0.9979\n",
      "step 3400: loss = 0.01001360360532999, acc = 0.9973052144050598\n",
      "70000/77510 [==========================>...] - ETA: 6:24 - loss: 0.0110 - acc: 0.9979\n",
      "step 3500: loss = 0.009081062860786915, acc = 0.9983116984367371\n",
      "72000/77510 [==========================>...] - ETA: 4:41 - loss: 0.0109 - acc: 0.9979\n",
      "step 3600: loss = 0.009425211697816849, acc = 0.9979220628738403\n",
      "74000/77510 [===========================>..] - ETA: 2:59 - loss: 0.0109 - acc: 0.9979\n",
      "step 3700: loss = 0.01001537125557661, acc = 0.9973701238632202\n",
      "76000/77510 [============================>.] - ETA: 1:16 - loss: 0.0108 - acc: 0.9979\n",
      "step 3800: loss = 0.009883378632366657, acc = 0.9974675178527832\n",
      "77510/77510 [==============================] - 4111s 53ms/step - loss: 0.0108 - acc: 0.9980 - val_loss: 0.0077 - val_acc: 0.9997\n",
      "Epoch 2/8\n",
      "  480/77510 [..............................] - ETA: 1:01:15 - loss: 0.0088 - acc: 0.9985\n",
      "step 3900: loss = 0.008842399343848228, acc = 0.9985714554786682\n",
      " 2480/77510 [..............................] - ETA: 1:00:12 - loss: 0.0089 - acc: 0.9985\n",
      "step 4000: loss = 0.008531810715794563, acc = 0.9988311529159546\n",
      " 4480/77510 [>.............................] - ETA: 58:32 - loss: 0.0090 - acc: 0.9984\n",
      "step 4100: loss = 0.008739698678255081, acc = 0.998701274394989\n",
      " 6480/77510 [=>............................] - ETA: 56:53 - loss: 0.0090 - acc: 0.9984\n",
      "step 4200: loss = 0.009247243404388428, acc = 0.9981818199157715\n",
      " 8480/77510 [==>...........................] - ETA: 55:16 - loss: 0.0089 - acc: 0.9984\n",
      "step 4300: loss = 0.008884675800800323, acc = 0.9985389709472656\n",
      "10480/77510 [===>..........................] - ETA: 53:40 - loss: 0.0089 - acc: 0.9985\n",
      "step 4400: loss = 0.008204866200685501, acc = 0.9992856979370117\n",
      "12480/77510 [===>..........................] - ETA: 52:07 - loss: 0.0089 - acc: 0.9985\n",
      "step 4500: loss = 0.007777747698128223, acc = 0.9996103644371033\n",
      "14480/77510 [====>.........................] - ETA: 50:31 - loss: 0.0089 - acc: 0.9985\n",
      "step 4600: loss = 0.01054537482559681, acc = 0.9970454573631287\n",
      "16480/77510 [=====>........................] - ETA: 48:55 - loss: 0.0089 - acc: 0.9985\n",
      "step 4700: loss = 0.008323105052113533, acc = 0.9990584254264832\n",
      "18480/77510 [======>.......................] - ETA: 47:18 - loss: 0.0089 - acc: 0.9985\n",
      "step 4800: loss = 0.009680824354290962, acc = 0.9977272748947144\n",
      "20480/77510 [======>.......................] - ETA: 45:43 - loss: 0.0089 - acc: 0.9985\n",
      "step 4900: loss = 0.00906308926641941, acc = 0.9983766078948975\n",
      "22480/77510 [=======>......................] - ETA: 44:08 - loss: 0.0089 - acc: 0.9985\n",
      "step 5000: loss = 0.0100645050406456, acc = 0.9973376393318176\n",
      "24480/77510 [========>.....................] - ETA: 42:39 - loss: 0.0089 - acc: 0.9985\n",
      "step 5100: loss = 0.00932464748620987, acc = 0.9979220628738403\n",
      "26480/77510 [=========>....................] - ETA: 41:07 - loss: 0.0089 - acc: 0.9985\n",
      "step 5200: loss = 0.008683182299137115, acc = 0.998701274394989\n",
      "28480/77510 [==========>...................] - ETA: 39:35 - loss: 0.0089 - acc: 0.9985\n",
      "step 5300: loss = 0.008347118273377419, acc = 0.9991233944892883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30480/77510 [==========>...................] - ETA: 38:17 - loss: 0.0089 - acc: 0.9985\n",
      "step 5400: loss = 0.008534017018973827, acc = 0.9988636374473572\n",
      "32480/77510 [===========>..................] - ETA: 36:38 - loss: 0.0089 - acc: 0.9985\n",
      "step 5500: loss = 0.008176573552191257, acc = 0.9991233944892883\n",
      "34480/77510 [============>.................] - ETA: 35:12 - loss: 0.0089 - acc: 0.9985\n",
      "step 5600: loss = 0.009493514895439148, acc = 0.9977921843528748\n",
      "36480/77510 [=============>................] - ETA: 33:46 - loss: 0.0089 - acc: 0.9985\n",
      "step 5700: loss = 0.009841037914156914, acc = 0.9975000023841858\n",
      "38480/77510 [=============>................] - ETA: 32:15 - loss: 0.0088 - acc: 0.9985\n",
      "step 5800: loss = 0.008399231359362602, acc = 0.9990260004997253\n",
      "40480/77510 [==============>...............] - ETA: 30:39 - loss: 0.0088 - acc: 0.9985\n",
      "step 5900: loss = 0.00918654352426529, acc = 0.9981493353843689\n",
      "42480/77510 [===============>..............] - ETA: 29:03 - loss: 0.0088 - acc: 0.9985\n",
      "step 6000: loss = 0.008380820043385029, acc = 0.9990260004997253\n",
      "44480/77510 [================>.............] - ETA: 27:26 - loss: 0.0088 - acc: 0.9985\n",
      "step 6100: loss = 0.01001489907503128, acc = 0.9972078204154968\n",
      "46480/77510 [================>.............] - ETA: 25:49 - loss: 0.0088 - acc: 0.9986\n",
      "step 6200: loss = 0.00783422589302063, acc = 0.9995454549789429\n",
      "48480/77510 [=================>............] - ETA: 24:11 - loss: 0.0088 - acc: 0.9986\n",
      "step 6300: loss = 0.0081062326207757, acc = 0.9993181824684143\n",
      "50480/77510 [==================>...........] - ETA: 22:31 - loss: 0.0088 - acc: 0.9986\n",
      "step 6400: loss = 0.007874557748436928, acc = 0.9994805455207825\n",
      "52480/77510 [===================>..........] - ETA: 20:51 - loss: 0.0088 - acc: 0.9986\n",
      "step 6500: loss = 0.008649002760648727, acc = 0.9987662434577942\n",
      "54480/77510 [====================>.........] - ETA: 19:11 - loss: 0.0088 - acc: 0.9986\n",
      "step 6600: loss = 0.009536029770970345, acc = 0.997662365436554\n",
      "56480/77510 [====================>.........] - ETA: 17:31 - loss: 0.0088 - acc: 0.9986\n",
      "step 6700: loss = 0.008457244373857975, acc = 0.9990260004997253\n",
      "58480/77510 [=====================>........] - ETA: 15:51 - loss: 0.0088 - acc: 0.9986\n",
      "step 6800: loss = 0.008068745955824852, acc = 0.9992856979370117\n",
      "60480/77510 [======================>.......] - ETA: 14:12 - loss: 0.0088 - acc: 0.9986\n",
      "step 6900: loss = 0.012293418869376183, acc = 0.994870126247406\n",
      "62480/77510 [=======================>......] - ETA: 12:32 - loss: 0.0088 - acc: 0.9986\n",
      "step 7000: loss = 0.0091431038454175, acc = 0.9982792139053345\n",
      "64480/77510 [=======================>......] - ETA: 10:52 - loss: 0.0088 - acc: 0.9986\n",
      "step 7100: loss = 0.008397744037210941, acc = 0.9989935159683228\n",
      "66480/77510 [========================>.....] - ETA: 9:11 - loss: 0.0088 - acc: 0.9986\n",
      "step 7200: loss = 0.008164720609784126, acc = 0.9991883039474487\n",
      "68480/77510 [=========================>....] - ETA: 7:30 - loss: 0.0088 - acc: 0.9986\n",
      "step 7300: loss = 0.009029555134475231, acc = 0.998701274394989\n",
      "70480/77510 [==========================>...] - ETA: 5:50 - loss: 0.0088 - acc: 0.9986\n",
      "step 7400: loss = 0.008122625760734081, acc = 0.9992856979370117\n",
      "72480/77510 [===========================>..] - ETA: 4:10 - loss: 0.0088 - acc: 0.9986\n",
      "step 7500: loss = 0.009187363088130951, acc = 0.9981818199157715\n",
      "74480/77510 [===========================>..] - ETA: 2:30 - loss: 0.0088 - acc: 0.9986\n",
      "step 7600: loss = 0.008610627613961697, acc = 0.9987337589263916\n",
      "76480/77510 [============================>.] - ETA: 51s - loss: 0.0088 - acc: 0.9986\n",
      "step 7700: loss = 0.009902437217533588, acc = 0.9974675178527832\n",
      "77510/77510 [==============================] - 4033s 52ms/step - loss: 0.0088 - acc: 0.9986 - val_loss: 0.0077 - val_acc: 0.9998\n",
      "Epoch 3/8\n",
      "  960/77510 [..............................] - ETA: 1:01:39 - loss: 0.0087 - acc: 0.9986\n",
      "step 7800: loss = 0.007998782210052013, acc = 0.9993181824684143\n",
      " 2960/77510 [>.............................] - ETA: 1:02:37 - loss: 0.0086 - acc: 0.9987\n",
      "step 7900: loss = 0.009115113876760006, acc = 0.9982467293739319\n",
      " 4960/77510 [>.............................] - ETA: 1:00:23 - loss: 0.0087 - acc: 0.9987\n",
      "step 8000: loss = 0.007909799925982952, acc = 0.9994155764579773\n",
      " 6960/77510 [=>............................] - ETA: 59:48 - loss: 0.0087 - acc: 0.9987\n",
      "step 8100: loss = 0.00871180184185505, acc = 0.9986688494682312\n",
      " 8960/77510 [==>...........................] - ETA: 58:44 - loss: 0.0087 - acc: 0.9986\n",
      "step 8200: loss = 0.00815434567630291, acc = 0.9992856979370117\n",
      "10960/77510 [===>..........................] - ETA: 57:26 - loss: 0.0087 - acc: 0.9987\n",
      "step 8300: loss = 0.007760306354612112, acc = 0.9996753334999084\n",
      "12960/77510 [====>.........................] - ETA: 55:32 - loss: 0.0087 - acc: 0.9987\n",
      "step 8400: loss = 0.009839500300586224, acc = 0.9972727298736572\n",
      "14960/77510 [====>.........................] - ETA: 53:30 - loss: 0.0087 - acc: 0.9987\n",
      "step 8500: loss = 0.008527477271854877, acc = 0.9991233944892883\n",
      "16960/77510 [=====>........................] - ETA: 51:34 - loss: 0.0087 - acc: 0.9987\n",
      "step 8600: loss = 0.00819244422018528, acc = 0.9991233944892883\n",
      "18960/77510 [======>.......................] - ETA: 49:41 - loss: 0.0087 - acc: 0.9987\n",
      "step 8700: loss = 0.008433787152171135, acc = 0.9990909099578857\n",
      "20960/77510 [=======>......................] - ETA: 47:49 - loss: 0.0087 - acc: 0.9987\n",
      "step 8800: loss = 0.007946645841002464, acc = 0.9995454549789429\n",
      "22960/77510 [=======>......................] - ETA: 46:01 - loss: 0.0087 - acc: 0.9987\n",
      "step 8900: loss = 0.009239442646503448, acc = 0.9980844259262085\n",
      "24960/77510 [========>.....................] - ETA: 44:15 - loss: 0.0087 - acc: 0.9987\n",
      "step 9000: loss = 0.008195905946195126, acc = 0.9991883039474487\n",
      "26960/77510 [=========>....................] - ETA: 42:30 - loss: 0.0087 - acc: 0.9987\n",
      "step 9100: loss = 0.00896492600440979, acc = 0.9984740018844604\n",
      "28960/77510 [==========>...................] - ETA: 40:45 - loss: 0.0087 - acc: 0.9987\n",
      "step 9200: loss = 0.009042998775839806, acc = 0.9983766078948975\n",
      "30960/77510 [==========>...................] - ETA: 39:02 - loss: 0.0087 - acc: 0.9987\n",
      "step 9300: loss = 0.009120429866015911, acc = 0.9982792139053345\n",
      "32960/77510 [===========>..................] - ETA: 37:18 - loss: 0.0087 - acc: 0.9987\n",
      "step 9400: loss = 0.007871059700846672, acc = 0.9995779395103455\n",
      "34960/77510 [============>.................] - ETA: 35:36 - loss: 0.0087 - acc: 0.9987\n",
      "step 9500: loss = 0.007878381758928299, acc = 0.9994805455207825\n",
      "36960/77510 [=============>................] - ETA: 33:54 - loss: 0.0087 - acc: 0.9987\n",
      "step 9600: loss = 0.0082753486931324, acc = 0.9991558194160461\n",
      "38960/77510 [==============>...............] - ETA: 32:12 - loss: 0.0087 - acc: 0.9987\n",
      "step 9700: loss = 0.007918763905763626, acc = 0.9994480609893799\n",
      "40960/77510 [==============>...............] - ETA: 30:29 - loss: 0.0087 - acc: 0.9987\n",
      "step 9800: loss = 0.009137796238064766, acc = 0.9981493353843689\n",
      "42960/77510 [===============>..............] - ETA: 28:48 - loss: 0.0087 - acc: 0.9987\n",
      "step 9900: loss = 0.008037248626351357, acc = 0.9993181824684143\n",
      "44960/77510 [================>.............] - ETA: 27:07 - loss: 0.0087 - acc: 0.9987\n",
      "step 10000: loss = 0.007851792499423027, acc = 0.9996103644371033\n",
      "46960/77510 [=================>............] - ETA: 25:26 - loss: 0.0087 - acc: 0.9987\n",
      "step 10100: loss = 0.007695521228015423, acc = 0.9995779395103455\n",
      "48960/77510 [=================>............] - ETA: 23:45 - loss: 0.0087 - acc: 0.9987\n",
      "step 10200: loss = 0.010156193748116493, acc = 0.9972078204154968\n",
      "50960/77510 [==================>...........] - ETA: 22:04 - loss: 0.0087 - acc: 0.9987\n",
      "step 10300: loss = 0.00835022795945406, acc = 0.9990260004997253\n",
      "52960/77510 [===================>..........] - ETA: 20:24 - loss: 0.0087 - acc: 0.9987\n",
      "step 10400: loss = 0.007937213405966759, acc = 0.9994155764579773\n",
      "54960/77510 [====================>.........] - ETA: 18:44 - loss: 0.0087 - acc: 0.9987\n",
      "step 10500: loss = 0.008438889868557453, acc = 0.9988311529159546\n",
      "56960/77510 [=====================>........] - ETA: 17:03 - loss: 0.0087 - acc: 0.9987\n",
      "step 10600: loss = 0.007958623580634594, acc = 0.9993506669998169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58960/77510 [=====================>........] - ETA: 15:23 - loss: 0.0087 - acc: 0.9987\n",
      "step 10700: loss = 0.008849562145769596, acc = 0.9985714554786682\n",
      "60960/77510 [======================>.......] - ETA: 13:43 - loss: 0.0087 - acc: 0.9987\n",
      "step 10800: loss = 0.007829698733985424, acc = 0.9995454549789429\n",
      "62960/77510 [=======================>......] - ETA: 12:04 - loss: 0.0087 - acc: 0.9987\n",
      "step 10900: loss = 0.007760000415146351, acc = 0.9996103644371033\n",
      "64960/77510 [========================>.....] - ETA: 10:24 - loss: 0.0087 - acc: 0.9987\n",
      "step 11000: loss = 0.008059415966272354, acc = 0.9993181824684143\n",
      "66960/77510 [========================>.....] - ETA: 8:44 - loss: 0.0087 - acc: 0.9987\n",
      "step 11100: loss = 0.007867627777159214, acc = 0.9994480609893799\n",
      "68960/77510 [=========================>....] - ETA: 7:05 - loss: 0.0087 - acc: 0.9987\n",
      "step 11200: loss = 0.007998963817954063, acc = 0.9993506669998169\n",
      "70960/77510 [==========================>...] - ETA: 5:25 - loss: 0.0087 - acc: 0.9987\n",
      "step 11300: loss = 0.00909996498376131, acc = 0.9982143044471741\n",
      "72960/77510 [===========================>..] - ETA: 3:46 - loss: 0.0087 - acc: 0.9987\n",
      "step 11400: loss = 0.009497396647930145, acc = 0.9978896379470825\n",
      "74960/77510 [============================>.] - ETA: 2:06 - loss: 0.0087 - acc: 0.9987\n",
      "step 11500: loss = 0.009684925898909569, acc = 0.9975649118423462\n",
      "76960/77510 [============================>.] - ETA: 27s - loss: 0.0087 - acc: 0.9987\n",
      "step 11600: loss = 0.008251560851931572, acc = 0.9991558194160461\n",
      "77510/77510 [==============================] - 4027s 52ms/step - loss: 0.0087 - acc: 0.9987 - val_loss: 0.0076 - val_acc: 0.9998\n",
      "Epoch 4/8\n",
      " 1440/77510 [..............................] - ETA: 1:02:55 - loss: 0.0085 - acc: 0.9988\n",
      "step 11700: loss = 0.009161504916846752, acc = 0.9981818199157715\n",
      " 3440/77510 [>.............................] - ETA: 1:01:10 - loss: 0.0086 - acc: 0.9988\n",
      "step 11800: loss = 0.007741410285234451, acc = 0.9995454549789429\n",
      " 5440/77510 [=>............................] - ETA: 59:29 - loss: 0.0086 - acc: 0.9987\n",
      "step 11900: loss = 0.009322911500930786, acc = 0.9979870319366455\n",
      " 7440/77510 [=>............................] - ETA: 57:49 - loss: 0.0086 - acc: 0.9987\n",
      "step 12000: loss = 0.008821169845759869, acc = 0.9984415769577026\n",
      " 9440/77510 [==>...........................] - ETA: 56:13 - loss: 0.0086 - acc: 0.9988\n",
      "step 12100: loss = 0.007643165532499552, acc = 0.9996753334999084\n",
      "11440/77510 [===>..........................] - ETA: 54:32 - loss: 0.0086 - acc: 0.9988\n",
      "step 12200: loss = 0.00870212260633707, acc = 0.9986363649368286\n",
      "13440/77510 [====>.........................] - ETA: 52:53 - loss: 0.0086 - acc: 0.9988\n",
      "step 12300: loss = 0.0077143944799900055, acc = 0.9996428489685059\n",
      "15440/77510 [====>.........................] - ETA: 51:13 - loss: 0.0086 - acc: 0.9988\n",
      "step 12400: loss = 0.008188370615243912, acc = 0.9991233944892883\n",
      "17440/77510 [=====>........................] - ETA: 49:34 - loss: 0.0086 - acc: 0.9988\n",
      "step 12500: loss = 0.010019557550549507, acc = 0.9973376393318176\n",
      "19440/77510 [======>.......................] - ETA: 47:55 - loss: 0.0086 - acc: 0.9988\n",
      "step 12600: loss = 0.008571764454245567, acc = 0.9988636374473572\n",
      "21440/77510 [=======>......................] - ETA: 46:17 - loss: 0.0086 - acc: 0.9987\n",
      "step 12700: loss = 0.008130975067615509, acc = 0.9992532730102539\n",
      "23440/77510 [========>.....................] - ETA: 44:38 - loss: 0.0086 - acc: 0.9987\n",
      "step 12800: loss = 0.008709154091775417, acc = 0.9986688494682312\n",
      "25440/77510 [========>.....................] - ETA: 42:59 - loss: 0.0086 - acc: 0.9987\n",
      "step 12900: loss = 0.008914267644286156, acc = 0.9985389709472656\n",
      "27440/77510 [=========>....................] - ETA: 41:19 - loss: 0.0086 - acc: 0.9987\n",
      "step 13000: loss = 0.008003721944987774, acc = 0.9994155764579773\n",
      "29440/77510 [==========>...................] - ETA: 39:40 - loss: 0.0086 - acc: 0.9987\n",
      "step 13100: loss = 0.008761500008404255, acc = 0.998603880405426\n",
      "31440/77510 [===========>..................] - ETA: 38:01 - loss: 0.0086 - acc: 0.9987\n",
      "step 13200: loss = 0.008900576271116734, acc = 0.9984740018844604\n",
      "33440/77510 [===========>..................] - ETA: 36:23 - loss: 0.0086 - acc: 0.9987\n",
      "step 13300: loss = 0.00862960983067751, acc = 0.9987662434577942\n",
      "35440/77510 [============>.................] - ETA: 34:44 - loss: 0.0086 - acc: 0.9987\n",
      "step 13400: loss = 0.007844922132790089, acc = 0.9996103644371033\n",
      "37440/77510 [=============>................] - ETA: 33:04 - loss: 0.0086 - acc: 0.9987\n",
      "step 13500: loss = 0.008364440873265266, acc = 0.9989610314369202\n",
      "39440/77510 [==============>...............] - ETA: 31:25 - loss: 0.0086 - acc: 0.9987\n",
      "step 13600: loss = 0.009039370343089104, acc = 0.9983116984367371\n",
      "41440/77510 [===============>..............] - ETA: 29:46 - loss: 0.0086 - acc: 0.9987\n",
      "step 13700: loss = 0.008816244080662727, acc = 0.9988636374473572\n",
      "43440/77510 [===============>..............] - ETA: 28:07 - loss: 0.0086 - acc: 0.9988\n",
      "step 13800: loss = 0.008637943305075169, acc = 0.9987662434577942\n",
      "45440/77510 [================>.............] - ETA: 26:28 - loss: 0.0086 - acc: 0.9988\n",
      "step 13900: loss = 0.008372657932341099, acc = 0.9990260004997253\n",
      "47440/77510 [=================>............] - ETA: 24:46 - loss: 0.0086 - acc: 0.9988\n",
      "step 14000: loss = 0.01073766965419054, acc = 0.996558427810669\n",
      "49440/77510 [==================>...........] - ETA: 23:05 - loss: 0.0086 - acc: 0.9987\n",
      "step 14100: loss = 0.008772583678364754, acc = 0.998603880405426\n",
      "51440/77510 [==================>...........] - ETA: 21:25 - loss: 0.0086 - acc: 0.9988\n",
      "step 14200: loss = 0.009443862363696098, acc = 0.9978571534156799\n",
      "53440/77510 [===================>..........] - ETA: 19:44 - loss: 0.0086 - acc: 0.9988\n",
      "step 14300: loss = 0.009043457917869091, acc = 0.9982143044471741\n",
      "55440/77510 [====================>.........] - ETA: 18:05 - loss: 0.0086 - acc: 0.9988\n",
      "step 14400: loss = 0.008399822749197483, acc = 0.9989935159683228\n",
      "57440/77510 [=====================>........] - ETA: 16:26 - loss: 0.0086 - acc: 0.9988\n",
      "step 14500: loss = 0.0076908692717552185, acc = 0.999707818031311\n",
      "59440/77510 [======================>.......] - ETA: 14:48 - loss: 0.0086 - acc: 0.9988\n",
      "step 14600: loss = 0.007882608100771904, acc = 0.9995779395103455\n",
      "61440/77510 [======================>.......] - ETA: 13:10 - loss: 0.0086 - acc: 0.9988\n",
      "step 14700: loss = 0.007615165319293737, acc = 0.9997402429580688\n",
      "63440/77510 [=======================>......] - ETA: 11:31 - loss: 0.0086 - acc: 0.9988\n",
      "step 14800: loss = 0.007525213062763214, acc = 0.9998701214790344\n",
      "65440/77510 [========================>.....] - ETA: 9:53 - loss: 0.0086 - acc: 0.9988\n",
      "step 14900: loss = 0.008663447573781013, acc = 0.9986363649368286\n",
      "67440/77510 [=========================>....] - ETA: 8:15 - loss: 0.0086 - acc: 0.9988\n",
      "step 15000: loss = 0.009163334965705872, acc = 0.9981818199157715\n",
      "69440/77510 [=========================>....] - ETA: 6:36 - loss: 0.0086 - acc: 0.9988\n",
      "step 15100: loss = 0.008827613666653633, acc = 0.998506486415863\n",
      "71440/77510 [==========================>...] - ETA: 4:58 - loss: 0.0086 - acc: 0.9988\n",
      "step 15200: loss = 0.009426476433873177, acc = 0.9979545474052429\n",
      "73440/77510 [===========================>..] - ETA: 3:20 - loss: 0.0086 - acc: 0.9988\n",
      "step 15300: loss = 0.008021937683224678, acc = 0.9992207884788513\n",
      "75440/77510 [============================>.] - ETA: 1:41 - loss: 0.0086 - acc: 0.9988\n",
      "step 15400: loss = 0.007561204023659229, acc = 0.9997402429580688\n",
      "77440/77510 [============================>.] - ETA: 3s - loss: 0.0086 - acc: 0.9988\n",
      "step 15500: loss = 0.007485962472856045, acc = 0.9999350905418396\n",
      "77510/77510 [==============================] - 3988s 51ms/step - loss: 0.0086 - acc: 0.9988 - val_loss: 0.0076 - val_acc: 0.9998\n",
      "Epoch 5/8\n",
      " 1920/77510 [..............................] - ETA: 1:02:05 - loss: 0.0086 - acc: 0.9988\n",
      "step 15600: loss = 0.008712132461369038, acc = 0.998701274394989\n",
      " 3920/77510 [>.............................] - ETA: 1:00:26 - loss: 0.0086 - acc: 0.9988\n",
      "step 15700: loss = 0.009335918352007866, acc = 0.9980519413948059\n",
      " 5920/77510 [=>............................] - ETA: 58:49 - loss: 0.0085 - acc: 0.9988\n",
      "step 15800: loss = 0.009677214547991753, acc = 0.9975973963737488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7920/77510 [==>...........................] - ETA: 57:08 - loss: 0.0086 - acc: 0.9988\n",
      "step 15900: loss = 0.008132373914122581, acc = 0.9992856979370117\n",
      " 9920/77510 [==>...........................] - ETA: 55:28 - loss: 0.0086 - acc: 0.9988\n",
      "step 16000: loss = 0.008172688074409962, acc = 0.9992207884788513\n",
      "11920/77510 [===>..........................] - ETA: 53:51 - loss: 0.0086 - acc: 0.9988\n",
      "step 16100: loss = 0.011481653898954391, acc = 0.9958117008209229\n",
      "13920/77510 [====>.........................] - ETA: 52:20 - loss: 0.0086 - acc: 0.9988\n",
      "step 16200: loss = 0.009094232693314552, acc = 0.9982792139053345\n",
      "15920/77510 [=====>........................] - ETA: 50:43 - loss: 0.0086 - acc: 0.9988\n",
      "step 16300: loss = 0.009992801584303379, acc = 0.9974026083946228\n",
      "17920/77510 [=====>........................] - ETA: 48:53 - loss: 0.0086 - acc: 0.9988\n",
      "step 16400: loss = 0.007783564273267984, acc = 0.9995454549789429\n",
      "19920/77510 [======>.......................] - ETA: 47:06 - loss: 0.0086 - acc: 0.9988\n",
      "step 16500: loss = 0.00968707911670208, acc = 0.9977272748947144\n",
      "21920/77510 [=======>......................] - ETA: 45:21 - loss: 0.0086 - acc: 0.9988\n",
      "step 16600: loss = 0.008485795930027962, acc = 0.9988311529159546\n",
      "23920/77510 [========>.....................] - ETA: 43:37 - loss: 0.0086 - acc: 0.9988\n",
      "step 16700: loss = 0.009272006340324879, acc = 0.9980519413948059\n",
      "25920/77510 [=========>....................] - ETA: 41:56 - loss: 0.0086 - acc: 0.9988\n",
      "step 16800: loss = 0.008706104941666126, acc = 0.9985389709472656\n",
      "27920/77510 [=========>....................] - ETA: 40:14 - loss: 0.0086 - acc: 0.9988\n",
      "step 16900: loss = 0.008793385699391365, acc = 0.998506486415863\n",
      "29920/77510 [==========>...................] - ETA: 38:33 - loss: 0.0086 - acc: 0.9988\n",
      "step 17000: loss = 0.00807318463921547, acc = 0.9990584254264832\n",
      "31920/77510 [===========>..................] - ETA: 36:52 - loss: 0.0086 - acc: 0.9988\n",
      "step 17100: loss = 0.007614216301590204, acc = 0.999805212020874\n",
      "33920/77510 [============>.................] - ETA: 35:12 - loss: 0.0086 - acc: 0.9988\n",
      "step 17200: loss = 0.009728786535561085, acc = 0.997662365436554\n",
      "35920/77510 [============>.................] - ETA: 33:32 - loss: 0.0086 - acc: 0.9988\n",
      "step 17300: loss = 0.010070165619254112, acc = 0.9972727298736572\n",
      "37920/77510 [=============>................] - ETA: 31:53 - loss: 0.0086 - acc: 0.9988\n",
      "step 17400: loss = 0.010031623765826225, acc = 0.9973052144050598\n",
      "39920/77510 [==============>...............] - ETA: 30:14 - loss: 0.0086 - acc: 0.9988\n",
      "step 17500: loss = 0.008994740433990955, acc = 0.9983766078948975\n",
      "41920/77510 [===============>..............] - ETA: 28:36 - loss: 0.0086 - acc: 0.9988\n",
      "step 17600: loss = 0.007852817885577679, acc = 0.9994805455207825\n",
      "43920/77510 [===============>..............] - ETA: 26:58 - loss: 0.0086 - acc: 0.9988\n",
      "step 17700: loss = 0.008221222087740898, acc = 0.9990909099578857\n",
      "45920/77510 [================>.............] - ETA: 25:20 - loss: 0.0086 - acc: 0.9988\n",
      "step 17800: loss = 0.008634666912257671, acc = 0.9987662434577942\n",
      "47920/77510 [=================>............] - ETA: 23:43 - loss: 0.0086 - acc: 0.9988\n",
      "step 17900: loss = 0.008862058632075787, acc = 0.998506486415863\n",
      "49920/77510 [==================>...........] - ETA: 22:06 - loss: 0.0086 - acc: 0.9988\n",
      "step 18000: loss = 0.00808620359748602, acc = 0.9994155764579773\n",
      "51920/77510 [===================>..........] - ETA: 20:29 - loss: 0.0086 - acc: 0.9988\n",
      "step 18100: loss = 0.007537655532360077, acc = 0.9998376369476318\n",
      "53920/77510 [===================>..........] - ETA: 18:52 - loss: 0.0086 - acc: 0.9988\n",
      "step 18200: loss = 0.007921973243355751, acc = 0.9994480609893799\n",
      "55920/77510 [====================>.........] - ETA: 17:15 - loss: 0.0086 - acc: 0.9988\n",
      "step 18300: loss = 0.00957298744469881, acc = 0.9978246688842773\n",
      "57920/77510 [=====================>........] - ETA: 15:39 - loss: 0.0086 - acc: 0.9988\n",
      "step 18400: loss = 0.007464590482413769, acc = 0.9999350905418396\n",
      "59920/77510 [======================>.......] - ETA: 14:02 - loss: 0.0086 - acc: 0.9988\n",
      "step 18500: loss = 0.007793896831572056, acc = 0.9995779395103455\n",
      "61920/77510 [======================>.......] - ETA: 12:26 - loss: 0.0086 - acc: 0.9988\n",
      "step 18600: loss = 0.009593415074050426, acc = 0.997662365436554\n",
      "63920/77510 [=======================>......] - ETA: 10:50 - loss: 0.0086 - acc: 0.9988\n",
      "step 18700: loss = 0.008726952597498894, acc = 0.998701274394989\n",
      "65920/77510 [========================>.....] - ETA: 9:14 - loss: 0.0086 - acc: 0.9988\n",
      "step 18800: loss = 0.009885504841804504, acc = 0.9975000023841858\n",
      "67920/77510 [=========================>....] - ETA: 7:38 - loss: 0.0086 - acc: 0.9988\n",
      "step 18900: loss = 0.008640393614768982, acc = 0.9987987279891968\n",
      "69920/77510 [==========================>...] - ETA: 6:02 - loss: 0.0086 - acc: 0.9988\n",
      "step 19000: loss = 0.00795523077249527, acc = 0.9993506669998169\n",
      "71920/77510 [==========================>...] - ETA: 4:27 - loss: 0.0086 - acc: 0.9988\n",
      "step 19100: loss = 0.007811243180185556, acc = 0.9996103644371033\n",
      "73920/77510 [===========================>..] - ETA: 2:51 - loss: 0.0086 - acc: 0.9988\n",
      "step 19200: loss = 0.0077760787680745125, acc = 0.9996753334999084\n",
      "75920/77510 [============================>.] - ETA: 1:15 - loss: 0.0086 - acc: 0.9988\n",
      "step 19300: loss = 0.008395284414291382, acc = 0.9990584254264832\n",
      "77510/77510 [==============================] - 3871s 50ms/step - loss: 0.0086 - acc: 0.9988 - val_loss: 0.0076 - val_acc: 0.9998\n",
      "Epoch 6/8\n",
      "  400/77510 [..............................] - ETA: 1:01:18 - loss: 0.0085 - acc: 0.9989\n",
      "step 19400: loss = 0.008451437577605247, acc = 0.9987662434577942\n",
      " 2400/77510 [..............................] - ETA: 59:12 - loss: 0.0085 - acc: 0.9989\n",
      "step 19500: loss = 0.007641238626092672, acc = 0.9997402429580688\n",
      " 4400/77510 [>.............................] - ETA: 57:33 - loss: 0.0085 - acc: 0.9988\n",
      "step 19600: loss = 0.008023766800761223, acc = 0.9993506669998169\n",
      " 6400/77510 [=>............................] - ETA: 55:58 - loss: 0.0086 - acc: 0.9988\n",
      "step 19700: loss = 0.007435116916894913, acc = 0.9999350905418396\n",
      " 8400/77510 [==>...........................] - ETA: 54:25 - loss: 0.0086 - acc: 0.9988\n",
      "step 19800: loss = 0.008880747482180595, acc = 0.9985714554786682\n",
      "10400/77510 [===>..........................] - ETA: 52:51 - loss: 0.0086 - acc: 0.9988\n",
      "step 19900: loss = 0.008150117471814156, acc = 0.9992856979370117\n",
      "12400/77510 [===>..........................] - ETA: 51:15 - loss: 0.0086 - acc: 0.9988\n",
      "step 20000: loss = 0.009085371159017086, acc = 0.9982143044471741\n",
      "14400/77510 [====>.........................] - ETA: 49:42 - loss: 0.0086 - acc: 0.9988\n",
      "step 20100: loss = 0.007531304843723774, acc = 0.9998376369476318\n",
      "16400/77510 [=====>........................] - ETA: 48:07 - loss: 0.0086 - acc: 0.9988\n",
      "step 20200: loss = 0.008591912686824799, acc = 0.9987662434577942\n",
      "18400/77510 [======>.......................] - ETA: 46:33 - loss: 0.0086 - acc: 0.9988\n",
      "step 20300: loss = 0.00996212475001812, acc = 0.9973376393318176\n",
      "20400/77510 [======>.......................] - ETA: 44:58 - loss: 0.0086 - acc: 0.9988\n",
      "step 20400: loss = 0.009783459827303886, acc = 0.9974675178527832\n",
      "22400/77510 [=======>......................] - ETA: 43:23 - loss: 0.0085 - acc: 0.9988\n",
      "step 20500: loss = 0.008093146607279778, acc = 0.9992207884788513\n",
      "24400/77510 [========>.....................] - ETA: 41:49 - loss: 0.0085 - acc: 0.9988\n",
      "step 20600: loss = 0.007607074920088053, acc = 0.999805212020874\n",
      "26400/77510 [=========>....................] - ETA: 40:15 - loss: 0.0086 - acc: 0.9988\n",
      "step 20700: loss = 0.008529344573616982, acc = 0.9987987279891968\n",
      "28400/77510 [=========>....................] - ETA: 38:40 - loss: 0.0086 - acc: 0.9988\n",
      "step 20800: loss = 0.008161360397934914, acc = 0.9992207884788513\n",
      "30400/77510 [==========>...................] - ETA: 37:06 - loss: 0.0085 - acc: 0.9988\n",
      "step 20900: loss = 0.008027459494769573, acc = 0.9993506669998169\n",
      "32400/77510 [===========>..................] - ETA: 35:31 - loss: 0.0086 - acc: 0.9988\n",
      "step 21000: loss = 0.007592545356601477, acc = 0.999805212020874\n",
      "34400/77510 [============>.................] - ETA: 33:57 - loss: 0.0085 - acc: 0.9988\n",
      "step 21100: loss = 0.0076310536824166775, acc = 0.999805212020874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36400/77510 [=============>................] - ETA: 32:22 - loss: 0.0085 - acc: 0.9988\n",
      "step 21200: loss = 0.007702133152633905, acc = 0.9996753334999084\n",
      "38400/77510 [=============>................] - ETA: 30:48 - loss: 0.0085 - acc: 0.9988\n",
      "step 21300: loss = 0.008695442229509354, acc = 0.9986363649368286\n",
      "40400/77510 [==============>...............] - ETA: 29:13 - loss: 0.0085 - acc: 0.9988\n",
      "step 21400: loss = 0.010515005327761173, acc = 0.9968181848526001\n",
      "42400/77510 [===============>..............] - ETA: 27:39 - loss: 0.0085 - acc: 0.9988\n",
      "step 21500: loss = 0.007803717162460089, acc = 0.9996103644371033\n",
      "44400/77510 [================>.............] - ETA: 26:05 - loss: 0.0085 - acc: 0.9988\n",
      "step 21600: loss = 0.009633000008761883, acc = 0.9977272748947144\n",
      "46400/77510 [================>.............] - ETA: 24:30 - loss: 0.0085 - acc: 0.9988\n",
      "step 21700: loss = 0.008992713876068592, acc = 0.9983441829681396\n",
      "48400/77510 [=================>............] - ETA: 22:56 - loss: 0.0085 - acc: 0.9988\n",
      "step 21800: loss = 0.008759892545640469, acc = 0.9986363649368286\n",
      "50400/77510 [==================>...........] - ETA: 21:21 - loss: 0.0085 - acc: 0.9988\n",
      "step 21900: loss = 0.007957804016768932, acc = 0.9994155764579773\n",
      "52400/77510 [===================>..........] - ETA: 19:46 - loss: 0.0085 - acc: 0.9988\n",
      "step 22000: loss = 0.007890300825238228, acc = 0.9994805455207825\n",
      "54400/77510 [====================>.........] - ETA: 18:12 - loss: 0.0085 - acc: 0.9989\n",
      "step 22100: loss = 0.008717240765690804, acc = 0.998701274394989\n",
      "56400/77510 [====================>.........] - ETA: 16:37 - loss: 0.0085 - acc: 0.9989\n",
      "step 22200: loss = 0.008206391707062721, acc = 0.9991883039474487\n",
      "58400/77510 [=====================>........] - ETA: 15:03 - loss: 0.0085 - acc: 0.9989\n",
      "step 22300: loss = 0.008487669751048088, acc = 0.9992856979370117\n",
      "60400/77510 [======================>.......] - ETA: 13:28 - loss: 0.0085 - acc: 0.9989\n",
      "step 22400: loss = 0.007953290827572346, acc = 0.9994155764579773\n",
      "62400/77510 [=======================>......] - ETA: 11:54 - loss: 0.0085 - acc: 0.9989\n",
      "step 22500: loss = 0.0103681655600667, acc = 0.9969480633735657\n",
      "64400/77510 [=======================>......] - ETA: 10:19 - loss: 0.0085 - acc: 0.9989\n",
      "step 22600: loss = 0.008425245992839336, acc = 0.9989610314369202\n",
      "66400/77510 [========================>.....] - ETA: 8:45 - loss: 0.0085 - acc: 0.9989\n",
      "step 22700: loss = 0.008019199594855309, acc = 0.9993506669998169\n",
      "68400/77510 [=========================>....] - ETA: 7:10 - loss: 0.0085 - acc: 0.9989\n",
      "step 22800: loss = 0.008661783300340176, acc = 0.9986363649368286\n",
      "70400/77510 [==========================>...] - ETA: 5:36 - loss: 0.0085 - acc: 0.9989\n",
      "step 22900: loss = 0.008989701978862286, acc = 0.9983766078948975\n",
      "72400/77510 [===========================>..] - ETA: 4:01 - loss: 0.0085 - acc: 0.9989\n",
      "step 23000: loss = 0.008382393047213554, acc = 0.9990584254264832\n",
      "74400/77510 [===========================>..] - ETA: 2:27 - loss: 0.0085 - acc: 0.9989\n",
      "step 23100: loss = 0.007542179431766272, acc = 0.9998376369476318\n",
      "76400/77510 [============================>.] - ETA: 52s - loss: 0.0085 - acc: 0.9989\n",
      "step 23200: loss = 0.007714671082794666, acc = 0.9996428489685059\n",
      "77510/77510 [==============================] - 3832s 49ms/step - loss: 0.0085 - acc: 0.9989 - val_loss: 0.0076 - val_acc: 0.9998\n",
      "Epoch 7/8\n",
      "  880/77510 [..............................] - ETA: 1:00:12 - loss: 0.0084 - acc: 0.9989\n",
      "step 23300: loss = 0.007873051799833775, acc = 0.9995454549789429\n",
      " 2880/77510 [>.............................] - ETA: 59:04 - loss: 0.0084 - acc: 0.9990\n",
      "step 23400: loss = 0.008286568336188793, acc = 0.9991558194160461\n",
      " 4880/77510 [>.............................] - ETA: 57:20 - loss: 0.0085 - acc: 0.9989\n",
      "step 23500: loss = 0.009507967159152031, acc = 0.9977597594261169\n",
      " 6880/77510 [=>............................] - ETA: 55:46 - loss: 0.0085 - acc: 0.9989\n",
      "step 23600: loss = 0.00830455869436264, acc = 0.9990909099578857\n",
      " 8880/77510 [==>...........................] - ETA: 54:10 - loss: 0.0085 - acc: 0.9989\n",
      "step 23700: loss = 0.008468122221529484, acc = 0.9989285469055176\n",
      "10880/77510 [===>..........................] - ETA: 52:36 - loss: 0.0085 - acc: 0.9989\n",
      "step 23800: loss = 0.00812063179910183, acc = 0.9992207884788513\n",
      "12880/77510 [===>..........................] - ETA: 51:00 - loss: 0.0085 - acc: 0.9989\n",
      "step 23900: loss = 0.007565164472907782, acc = 0.9998701214790344\n",
      "14880/77510 [====>.........................] - ETA: 49:25 - loss: 0.0085 - acc: 0.9989\n",
      "step 24000: loss = 0.009735241532325745, acc = 0.9975973963737488\n",
      "16880/77510 [=====>........................] - ETA: 47:49 - loss: 0.0085 - acc: 0.9989\n",
      "step 24100: loss = 0.009049968793988228, acc = 0.9983116984367371\n",
      "18880/77510 [======>.......................] - ETA: 46:15 - loss: 0.0085 - acc: 0.9989\n",
      "step 24200: loss = 0.007732500787824392, acc = 0.9996428489685059\n",
      "20880/77510 [=======>......................] - ETA: 44:40 - loss: 0.0085 - acc: 0.9989\n",
      "step 24300: loss = 0.007473691366612911, acc = 0.9999350905418396\n",
      "22880/77510 [=======>......................] - ETA: 43:05 - loss: 0.0085 - acc: 0.9989\n",
      "step 24400: loss = 0.00839156098663807, acc = 0.9989610314369202\n",
      "24880/77510 [========>.....................] - ETA: 41:29 - loss: 0.0085 - acc: 0.9989\n",
      "step 24500: loss = 0.008360322564840317, acc = 0.9990584254264832\n",
      "26880/77510 [=========>....................] - ETA: 39:54 - loss: 0.0085 - acc: 0.9989\n",
      "step 24600: loss = 0.008438071236014366, acc = 0.9990260004997253\n",
      "28880/77510 [==========>...................] - ETA: 38:20 - loss: 0.0085 - acc: 0.9989\n",
      "step 24700: loss = 0.008789101615548134, acc = 0.9985714554786682\n",
      "30880/77510 [==========>...................] - ETA: 36:46 - loss: 0.0085 - acc: 0.9989\n",
      "step 24800: loss = 0.009087304584681988, acc = 0.9982467293739319\n",
      "32880/77510 [===========>..................] - ETA: 35:11 - loss: 0.0085 - acc: 0.9989\n",
      "step 24900: loss = 0.008991141803562641, acc = 0.9983766078948975\n",
      "34880/77510 [============>.................] - ETA: 33:37 - loss: 0.0085 - acc: 0.9989\n",
      "step 25000: loss = 0.008752535097301006, acc = 0.9985714554786682\n",
      "36880/77510 [=============>................] - ETA: 32:02 - loss: 0.0085 - acc: 0.9989\n",
      "step 25100: loss = 0.007551276590675116, acc = 0.9998376369476318\n",
      "38880/77510 [==============>...............] - ETA: 30:27 - loss: 0.0085 - acc: 0.9989\n",
      "step 25200: loss = 0.008022193796932697, acc = 0.9993181824684143\n",
      "40880/77510 [==============>...............] - ETA: 28:52 - loss: 0.0085 - acc: 0.9989\n",
      "step 25300: loss = 0.008710705675184727, acc = 0.9986363649368286\n",
      "42880/77510 [===============>..............] - ETA: 27:17 - loss: 0.0085 - acc: 0.9989\n",
      "step 25400: loss = 0.008372729644179344, acc = 0.9988961219787598\n",
      "44880/77510 [================>.............] - ETA: 25:42 - loss: 0.0085 - acc: 0.9989\n",
      "step 25500: loss = 0.008311247453093529, acc = 0.9992207884788513\n",
      "46880/77510 [=================>............] - ETA: 24:08 - loss: 0.0085 - acc: 0.9989\n",
      "step 25600: loss = 0.008247192949056625, acc = 0.9992856979370117\n",
      "48880/77510 [=================>............] - ETA: 22:33 - loss: 0.0085 - acc: 0.9989\n",
      "step 25700: loss = 0.008856724016368389, acc = 0.9986363649368286\n",
      "50880/77510 [==================>...........] - ETA: 20:58 - loss: 0.0085 - acc: 0.9989\n",
      "step 25800: loss = 0.008223630487918854, acc = 0.9991558194160461\n",
      "52880/77510 [===================>..........] - ETA: 19:24 - loss: 0.0085 - acc: 0.9989\n",
      "step 25900: loss = 0.009123209863901138, acc = 0.9981169104576111\n",
      "54880/77510 [====================>.........] - ETA: 17:49 - loss: 0.0085 - acc: 0.9989\n",
      "step 26000: loss = 0.008813661523163319, acc = 0.9985714554786682\n",
      "56880/77510 [=====================>........] - ETA: 16:15 - loss: 0.0085 - acc: 0.9989\n",
      "step 26100: loss = 0.007938556373119354, acc = 0.9994155764579773\n",
      "58880/77510 [=====================>........] - ETA: 14:40 - loss: 0.0085 - acc: 0.9989\n",
      "step 26200: loss = 0.010754354298114777, acc = 0.99626624584198\n",
      "60880/77510 [======================>.......] - ETA: 13:06 - loss: 0.0085 - acc: 0.9989\n",
      "step 26300: loss = 0.008819655515253544, acc = 0.998506486415863\n",
      "62880/77510 [=======================>......] - ETA: 11:31 - loss: 0.0085 - acc: 0.9989\n",
      "step 26400: loss = 0.007439993321895599, acc = 0.9998701214790344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64880/77510 [========================>.....] - ETA: 9:56 - loss: 0.0085 - acc: 0.9989\n",
      "step 26500: loss = 0.0076713040471076965, acc = 0.9996753334999084\n",
      "66880/77510 [========================>.....] - ETA: 8:22 - loss: 0.0085 - acc: 0.9989\n",
      "step 26600: loss = 0.010222632437944412, acc = 0.9970129728317261\n",
      "68880/77510 [=========================>....] - ETA: 6:47 - loss: 0.0085 - acc: 0.9989\n",
      "step 26700: loss = 0.00790601409971714, acc = 0.9994805455207825\n",
      "70880/77510 [==========================>...] - ETA: 5:13 - loss: 0.0085 - acc: 0.9989\n",
      "step 26800: loss = 0.007961941882967949, acc = 0.9994155764579773\n",
      "72880/77510 [===========================>..] - ETA: 3:38 - loss: 0.0085 - acc: 0.9989\n",
      "step 26900: loss = 0.008495154790580273, acc = 0.9988961219787598\n",
      "74880/77510 [===========================>..] - ETA: 2:04 - loss: 0.0085 - acc: 0.9989\n",
      "step 27000: loss = 0.009760592132806778, acc = 0.9975649118423462\n",
      "76880/77510 [============================>.] - ETA: 29s - loss: 0.0085 - acc: 0.9989\n",
      "step 27100: loss = 0.009441918693482876, acc = 0.9977597594261169\n",
      "77510/77510 [==============================] - 3831s 49ms/step - loss: 0.0085 - acc: 0.9989 - val_loss: 0.0076 - val_acc: 0.9998\n",
      "Epoch 8/8\n",
      " 1360/77510 [..............................] - ETA: 1:00:12 - loss: 0.0086 - acc: 0.9988\n",
      "step 27200: loss = 0.008674376644194126, acc = 0.9987662434577942\n",
      " 3360/77510 [>.............................] - ETA: 58:26 - loss: 0.0085 - acc: 0.9989\n",
      "step 27300: loss = 0.00888863205909729, acc = 0.9984090924263\n",
      " 5360/77510 [=>............................] - ETA: 56:48 - loss: 0.0085 - acc: 0.9989\n",
      "step 27400: loss = 0.008309000171720982, acc = 0.9990260004997253\n",
      " 7360/77510 [=>............................] - ETA: 55:11 - loss: 0.0085 - acc: 0.9989\n",
      "step 27500: loss = 0.00845644436776638, acc = 0.9988961219787598\n",
      " 9360/77510 [==>...........................] - ETA: 53:36 - loss: 0.0085 - acc: 0.9989\n",
      "step 27600: loss = 0.007657934911549091, acc = 0.9997402429580688\n",
      "11360/77510 [===>..........................] - ETA: 52:02 - loss: 0.0085 - acc: 0.9989\n",
      "step 27700: loss = 0.009396408684551716, acc = 0.9978571534156799\n",
      "13360/77510 [====>.........................] - ETA: 50:27 - loss: 0.0085 - acc: 0.9989\n",
      "step 27800: loss = 0.007805169560015202, acc = 0.9996753334999084\n",
      "15360/77510 [====>.........................] - ETA: 48:52 - loss: 0.0085 - acc: 0.9989\n",
      "step 27900: loss = 0.008429145440459251, acc = 0.9989610314369202\n",
      "17360/77510 [=====>........................] - ETA: 47:18 - loss: 0.0085 - acc: 0.9989\n",
      "step 28000: loss = 0.008106927387416363, acc = 0.9993181824684143\n",
      "19360/77510 [======>.......................] - ETA: 45:44 - loss: 0.0085 - acc: 0.9989\n",
      "step 28100: loss = 0.008989967405796051, acc = 0.9983766078948975\n",
      "21360/77510 [=======>......................] - ETA: 44:10 - loss: 0.0085 - acc: 0.9989\n",
      "step 28200: loss = 0.009236780926585197, acc = 0.9981169104576111\n",
      "23360/77510 [========>.....................] - ETA: 42:37 - loss: 0.0085 - acc: 0.9989\n",
      "step 28300: loss = 0.007592593319714069, acc = 0.999805212020874\n",
      "25360/77510 [========>.....................] - ETA: 41:01 - loss: 0.0085 - acc: 0.9989\n",
      "step 28400: loss = 0.00769733265042305, acc = 0.9996753334999084\n",
      "27360/77510 [=========>....................] - ETA: 39:27 - loss: 0.0085 - acc: 0.9989\n",
      "step 28500: loss = 0.00758095970377326, acc = 0.999805212020874\n",
      "29360/77510 [==========>...................] - ETA: 37:53 - loss: 0.0085 - acc: 0.9989\n",
      "step 28600: loss = 0.008883582428097725, acc = 0.998506486415863\n",
      "31360/77510 [===========>..................] - ETA: 36:18 - loss: 0.0085 - acc: 0.9989\n",
      "step 28700: loss = 0.0075660040602087975, acc = 0.9998701214790344\n",
      "33360/77510 [===========>..................] - ETA: 34:43 - loss: 0.0085 - acc: 0.9989\n",
      "step 28800: loss = 0.008249612525105476, acc = 0.9991558194160461\n",
      "35360/77510 [============>.................] - ETA: 33:09 - loss: 0.0085 - acc: 0.9989\n",
      "step 28900: loss = 0.008968660607933998, acc = 0.9984090924263\n",
      "37360/77510 [=============>................] - ETA: 31:35 - loss: 0.0085 - acc: 0.9989\n",
      "step 29000: loss = 0.008948927745223045, acc = 0.9983766078948975\n",
      "39360/77510 [==============>...............] - ETA: 30:01 - loss: 0.0085 - acc: 0.9989\n",
      "step 29100: loss = 0.0078487703576684, acc = 0.9994805455207825\n",
      "41360/77510 [===============>..............] - ETA: 28:26 - loss: 0.0085 - acc: 0.9989\n",
      "step 29200: loss = 0.007699052337557077, acc = 0.9996753334999084\n",
      "43360/77510 [===============>..............] - ETA: 26:52 - loss: 0.0085 - acc: 0.9989\n",
      "step 29300: loss = 0.008855882100760937, acc = 0.9985714554786682\n",
      "45360/77510 [================>.............] - ETA: 25:17 - loss: 0.0085 - acc: 0.9989\n",
      "step 29400: loss = 0.008599375374615192, acc = 0.9988311529159546\n",
      "47360/77510 [=================>............] - ETA: 23:43 - loss: 0.0085 - acc: 0.9989\n",
      "step 29500: loss = 0.007672038860619068, acc = 0.9997402429580688\n",
      "49360/77510 [==================>...........] - ETA: 22:09 - loss: 0.0085 - acc: 0.9989\n",
      "step 29600: loss = 0.007781380321830511, acc = 0.9996103644371033\n",
      "51360/77510 [==================>...........] - ETA: 20:34 - loss: 0.0085 - acc: 0.9989\n",
      "step 29700: loss = 0.010052082128822803, acc = 0.9974026083946228\n",
      "53360/77510 [===================>..........] - ETA: 19:00 - loss: 0.0085 - acc: 0.9989\n",
      "step 29800: loss = 0.008331924676895142, acc = 0.9990260004997253\n",
      "55360/77510 [====================>.........] - ETA: 17:26 - loss: 0.0085 - acc: 0.9989\n",
      "step 29900: loss = 0.008447395637631416, acc = 0.9989610314369202\n",
      "57360/77510 [=====================>........] - ETA: 15:51 - loss: 0.0085 - acc: 0.9989\n",
      "step 30000: loss = 0.008901426568627357, acc = 0.9984090924263\n",
      "59360/77510 [=====================>........] - ETA: 14:17 - loss: 0.0085 - acc: 0.9989\n",
      "step 30100: loss = 0.00959104672074318, acc = 0.9977597594261169\n",
      "61360/77510 [======================>.......] - ETA: 12:42 - loss: 0.0085 - acc: 0.9989\n",
      "step 30200: loss = 0.007579681929200888, acc = 0.999805212020874\n",
      "63360/77510 [=======================>......] - ETA: 11:08 - loss: 0.0085 - acc: 0.9989\n",
      "step 30300: loss = 0.007547421846538782, acc = 0.9997727274894714\n",
      "65360/77510 [========================>.....] - ETA: 9:33 - loss: 0.0085 - acc: 0.9989\n",
      "step 30400: loss = 0.008546477183699608, acc = 0.9988961219787598\n",
      "67360/77510 [=========================>....] - ETA: 7:59 - loss: 0.0085 - acc: 0.9989\n",
      "step 30500: loss = 0.008150184527039528, acc = 0.9991558194160461\n",
      "69360/77510 [=========================>....] - ETA: 6:24 - loss: 0.0085 - acc: 0.9989\n",
      "step 30600: loss = 0.007748404052108526, acc = 0.9995454549789429\n",
      "71360/77510 [==========================>...] - ETA: 4:50 - loss: 0.0085 - acc: 0.9989\n",
      "step 30700: loss = 0.008194460533559322, acc = 0.9992207884788513\n",
      "73360/77510 [===========================>..] - ETA: 3:16 - loss: 0.0085 - acc: 0.9989\n",
      "step 30800: loss = 0.007997015491127968, acc = 0.9993181824684143\n",
      "75360/77510 [============================>.] - ETA: 1:41 - loss: 0.0085 - acc: 0.9989\n",
      "step 30900: loss = 0.007969358935952187, acc = 0.9994155764579773\n",
      "77360/77510 [============================>.] - ETA: 7s - loss: 0.0085 - acc: 0.9989\n",
      "step 31000: loss = 0.008175721392035484, acc = 0.9992207884788513\n",
      "77510/77510 [==============================] - 3829s 49ms/step - loss: 0.0085 - acc: 0.9989 - val_loss: 0.0076 - val_acc: 0.9998\n",
      "12107/12107 [==============================] - 170s 14ms/step\n",
      "[0.007607596895698327, 0.9998061633123831]\n",
      "./convWeights/0603-0412_5hot.hdf5\n"
     ]
    }
   ],
   "source": [
    "class NBatchLogger(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A Logger that log average performance per `display` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, display):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.step += 1\n",
    "        if self.step % self.display == 0:\n",
    "            loss = logs.get('loss')\n",
    "            acc = logs.get('acc')\n",
    "            self.losses.append(loss)\n",
    "            self.accs.append(acc)\n",
    "            print('\\nstep {}: loss = {}, acc = {}'.format(self.step, loss, acc))\n",
    "\n",
    "            \n",
    "out_batch = NBatchLogger(display=100)\n",
    "history = myModel.fit(x = X_train, y = Y_train, epochs = 8, validation_data = (X_val, Y_val), batch_size = 20, callbacks = [out_batch], verbose = 1)\n",
    "\n",
    "loss_and_acc = myModel.evaluate(X_test, Y_test)\n",
    "print(loss_and_acc)\n",
    "\n",
    "currtime = datetime.datetime.now()\n",
    "fname = \"./convWeights/\" + currtime.strftime(\"%m%d-%H%M\") + \"_5hot.hdf5\"\n",
    "print(fname)\n",
    "myModel.save_weights(fname)\n",
    "logging.append(out_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "histfname = \"./trainHistoryDict_\" + currtime.strftime(\"%m%d-%H%M\") + \"_5hot\"\n",
    "with open(histfname, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "histfname = \"./trainHistoryDict_\" + \"0603-0412\" + \"_5hot\"\n",
    "history = pickle.load(open(histfname, \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(favorite_color['acc'])\n",
    "plt.plot(favorite_color['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(favorite_color['loss'])\n",
    "plt.plot(favorite_color['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = myModel.predict(x = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample number 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-cf323b91006c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample number \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msamp_n_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msamp_n_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msamp_n_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# Show whole sequence\n",
    "for n in range(100):\n",
    "    print(\"Sample number \" + str(n+1))\n",
    "    samp_n_pred = preds[n,:,:,0]\n",
    "    samp_n_in = X_test[n,:,:,0]\n",
    "    samp_n_true = Y_test[n,:,:,0]\n",
    "\n",
    "    samp_n_pred = convert_to_nucs(samp_n_pred)\n",
    "    samp_n_true = convert_to_nucs(samp_n_true)\n",
    "    samp_n_in = convert_to_nucs(samp_n_in)\n",
    "    noisy = show_noise(samp_n_pred, samp_n_true, samp_n_in)\n",
    "\n",
    "    print('Predicted:')\n",
    "    print('Denoised:')\n",
    "    print('Noisy:')\n",
    "    print('Noise locations:\\n')\n",
    "\n",
    "    stt = 0\n",
    "    stp = 100\n",
    "    while(stp < len(noisy)):\n",
    "        print(samp_n_pred[stt:stp])\n",
    "        print(samp_n_true[stt:stp])\n",
    "        print(samp_n_in[stt:stp])\n",
    "        print(noisy[stt:stp])\n",
    "        print('')\n",
    "        stt = stp\n",
    "        stp = stt+100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.load_weights(\"./convWeights/0531-2012_5hot.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
